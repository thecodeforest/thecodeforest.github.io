<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.30.2" />
  <meta name="author" content="Mark LeBoeuf">
  <meta name="description" content="Data Scientist">

  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/monokai.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  
  


  

  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-101855339-2', 'auto');
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="The Code Forest">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="The Code Forest">
  

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="/post/time_series_forecasting_with_neural_networks.html">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="The Code Forest">
  <meta property="og:url" content="/post/time_series_forecasting_with_neural_networks.html">
  <meta property="og:title" content="Time Series Forecasting with Neural Networks | The Code Forest">
  <meta property="og:description" content="">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-01-04T21:13:14-05:00">
  
  <meta property="article:modified_time" content="2018-01-04T21:13:14-05:00">
  

  

  <title>Time Series Forecasting with Neural Networks | The Code Forest</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">The Code Forest</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#about">
            
            <span>About</span>
          </a>
        </li>

        
        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <div class="article-inner">
      <h1 itemprop="name">Time Series Forecasting with Neural Networks</h1>

      

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2018-01-04 21:13:14 -0500 -0500" itemprop="datePublished">
      Jan 4, 2018
    </time>
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    13 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="/post/time_series_forecasting_with_neural_networks.html#disqus_thread"></a>
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="/categories/r">R</a
    >, 
    
    <a href="/categories/neural-networks">Neural Networks</a
    >, 
    
    <a href="/categories/forecasting">Forecasting</a
    >
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Time%20Series%20Forecasting%20with%20Neural%20Networks&amp;url=%2fpost%2ftime_series_forecasting_with_neural_networks.html"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2ftime_series_forecasting_with_neural_networks.html"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2ftime_series_forecasting_with_neural_networks.html&amp;title=Time%20Series%20Forecasting%20with%20Neural%20Networks"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2ftime_series_forecasting_with_neural_networks.html&amp;title=Time%20Series%20Forecasting%20with%20Neural%20Networks"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Time%20Series%20Forecasting%20with%20Neural%20Networks&amp;body=%2fpost%2ftime_series_forecasting_with_neural_networks.html">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


      <div class="article-style" itemprop="articleBody">
        <p><img src="images/city_lights.jpg" width="800px" height="800px" /></p>
<div id="overview" class="section level3">
<h3>Overview</h3>
<p>The Internet of Things (IOT) has enabled data collection on a moment-by-moment basis. Some examples include smart-thermostats that store power readings from millions of homes every minute or sensors within a machine that capture part vibration readings every second. The granular nature of these data streams provides opportunites to leverage more advanced forecasting methodologies that can better model non-linearities and higher-order interactions. Accordingly, the goal of this post is to provide an end-to-end overview of how to generate a forecast using neural nets with a classic IOT dataset – hourly electricity consumption from commercial buildings.</p>
</div>
<div id="forecasting-power-consumption" class="section level3">
<h3>Forecasting Power Consumption</h3>
<p>Nothing gets me more <em>charged up</em> than forecasting electricity consumption, so the data we’ll use here is a time series of consumption for an anonymized commercial building from 2012. Measurements were recorded for a single year at five-minute intervals, so each hour has 12 readings, and each day has 288 readings. Our goal is to train a model on several months of consumption data and then produce a 24-hour ahead forecast of consumption. Let’s get started by downloading the data and examining the first few rows.</p>
<pre class="r"><code>libs = c(&#39;data.table&#39;,&#39;h2o&#39;,&#39;forecast&#39;,
         &#39;lubridate&#39;,&#39;forcats&#39;,
          &#39;ggforce&#39;, &#39;ggplot2&#39;,
          &#39;reshape&#39;, &#39;knitr&#39;,
         &#39;kableExtra&#39;, &#39;tidyquant&#39;,
         &#39;stringr&#39;, &#39;dplyr&#39;,
         &#39;doParallel&#39;, &#39;foreach&#39;,
         &#39;janitor&#39;, &#39;timetk&#39;,
         &#39;tidyquant&#39;, &#39;stringr&#39;
         )
lapply(libs, require, character.only = TRUE)
data_source = &#39;https://open-enernoc-data.s3.amazonaws.com/anon/csv/401.csv&#39;
df_elec = fread(data_source,
                data.table = FALSE)</code></pre>
<div style="border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:400px; overflow-x: scroll; width:720px; ">
<table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:center;">
timestamp
</th>
<th style="text-align:center;">
dttm_utc
</th>
<th style="text-align:center;">
value
</th>
<th style="text-align:center;">
estimated
</th>
<th style="text-align:center;">
anomaly
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
1325376300
</td>
<td style="text-align:center;">
2012-01-01 00:05:00
</td>
<td style="text-align:center;">
10.6253
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:center;">
1325376600
</td>
<td style="text-align:center;">
2012-01-01 00:10:00
</td>
<td style="text-align:center;">
10.1194
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:center;">
1325376900
</td>
<td style="text-align:center;">
2012-01-01 00:15:00
</td>
<td style="text-align:center;">
10.6253
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:center;">
1325377200
</td>
<td style="text-align:center;">
2012-01-01 00:20:00
</td>
<td style="text-align:center;">
9.6134
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:center;">
1325377500
</td>
<td style="text-align:center;">
2012-01-01 00:25:00
</td>
<td style="text-align:center;">
10.6253
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:center;">
1325377800
</td>
<td style="text-align:center;">
2012-01-01 00:30:00
</td>
<td style="text-align:center;">
9.6134
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:center;">
1325378100
</td>
<td style="text-align:center;">
2012-01-01 00:35:00
</td>
<td style="text-align:center;">
10.6253
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:center;">
1325378400
</td>
<td style="text-align:center;">
2012-01-01 00:40:00
</td>
<td style="text-align:center;">
9.6134
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:center;">
1325378700
</td>
<td style="text-align:center;">
2012-01-01 00:45:00
</td>
<td style="text-align:center;">
11.6373
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:center;">
1325379000
</td>
<td style="text-align:center;">
2012-01-01 00:50:00
</td>
<td style="text-align:center;">
12.1433
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:center;">
1325379300
</td>
<td style="text-align:center;">
2012-01-01 00:55:00
</td>
<td style="text-align:center;">
13.6612
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:center;">
1325379600
</td>
<td style="text-align:center;">
2012-01-01 01:00:00
</td>
<td style="text-align:center;">
14.1671
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:center;">
1325379900
</td>
<td style="text-align:center;">
2012-01-01 01:05:00
</td>
<td style="text-align:center;">
12.1433
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:center;">
1325380200
</td>
<td style="text-align:center;">
2012-01-01 01:10:00
</td>
<td style="text-align:center;">
13.1552
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:center;">
1325380500
</td>
<td style="text-align:center;">
2012-01-01 01:15:00
</td>
<td style="text-align:center;">
12.6492
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
</td>
</tr>
</tbody>
</table>
</div>
<p>We only need the date-time and consumption values, so we’ll filter out several variables and extract the date information from the timestamps.</p>
<pre class="r"><code>df_elec = df_elec %&gt;% 
  dplyr::select(-anomaly, -timestamp) %&gt;%
  dplyr::rename(date_time = dttm_utc) %&gt;%
         mutate(date_only = as.Date(date_time)) %&gt;% 
         mutate(month = lubridate::month(date_time, label = TRUE),
                week = as.factor(lubridate::week(date_time)),
                hour = lubridate::hour(date_time),
                day = lubridate::day(date_time))</code></pre>
<p>To keep things simple, we’ll model the average reading per hour instead of every five minutes, meaning there will be 24 readings per day instead of 288.</p>
<pre class="r"><code>df_hourly = df_elec %&gt;% 
         group_by(date_only, month, hour) %&gt;% 
         summarise(value = mean(value)) %&gt;% 
         ungroup() %&gt;% 
         mutate(hour = ifelse(nchar(as.character(hour)) == 1, 
                              paste0(&quot;0&quot;, as.character(hour)),
                              hour)) %&gt;% 
         mutate(hour = paste(hour, &quot;00&quot;, &quot;00&quot;, sep = &quot;:&quot;)) %&gt;% 
         mutate(date_time = lubridate::ymd_hms(paste(date_only, hour))) %&gt;% 
         dplyr::select(date_time, month, value) %&gt;% 
         mutate(week = as.factor(lubridate::week(date_time)),
                day = lubridate::wday(date_time, label = TRUE),
                hour = lubridate::hour(date_time)
                ) </code></pre>
<p>Next, we’ll filter the timeframe to only a few months to speed up the eventual training time and break our dataset into <em>training</em>, <em>validation</em>, and <em>testing</em> segments. The 2nd to last week is held out for validation and the last week for testing. The validation and testing segments contain a total of 168 observations (24 per day).</p>
<pre class="r"><code>df_hourly = df_hourly %&gt;% 
            filter(month %in% c(&#39;Feb&#39;,&#39;Mar&#39;, &#39;Apr&#39;, &#39;May&#39;, &#39;Jun&#39;)) %&gt;% 
            dplyr::select(-month)
# daily period (24 hours)
daily_p = 24
# weekly period (7 days)
weekly_p = 7
# period length
period_length = daily_p * weekly_p
# create train, validation, and testing
train_df = head(df_hourly, (nrow(df_hourly) - period_length * 2))
validation_df = head(tail(df_hourly, period_length * 2), period_length)
test_df = tail(df_hourly, period_length)</code></pre>
<p>Let’s start by creating a high level view of the entire time series and load up a custom plotting theme.</p>
<pre class="r"><code>my_plot_theme = function(){
    font_family = &quot;Helvetica&quot;
    font_face = &quot;bold&quot;
    return(theme(
    axis.text.x = element_text(size = 14, face = font_face, family = font_family),
    axis.text.y = element_text(size = 14, face = font_face, family = font_family),
    axis.title.x = element_text(size = 14, face = font_face, family = font_family),
    axis.title.y = element_text(size = 14, face = font_face, family = font_family),
    strip.text.y = element_text(size = 14, face = font_face, family = font_family),
    plot.title = element_text(size = 18, face = font_face, family = font_family),
    legend.position = &quot;top&quot;,
    legend.title = element_text(size = 16,
    face = font_face,
    family = font_family),
    legend.text = element_text(size = 14,
    face = font_face,
    family = font_family)
))
}

bind_rows(train_df %&gt;% 
            mutate(part = &quot;train&quot;),
          validation_df %&gt;% 
            mutate(part = &quot;validation&quot;), 
          test_df %&gt;% 
            mutate(part = &quot;test&quot;)) %&gt;% 
  dplyr::select(date_time, part, value) %&gt;% 
  ggplot(aes(x = date_time, y = value, color = part)) + 
  geom_line() +   
  facet_zoom(x = date_time %in% c(validation_df$date_time, 
                                  test_df$date_time)) + 
  theme_bw() + 
  my_plot_theme() + 
  xlab(&quot;Date-Time&quot;) + 
  ylab(&quot;Value&quot;) + 
  theme(legend.title=element_blank())</code></pre>
<p><img src="/post/time_series_forecasting_with_neural_networks_files/figure-html/unnamed-chunk-8-1.png" width="960" /></p>
<p>There is clear seasonality in the data – and that’s what we’ll focus on next. The measurements are at the hourly level, so a periodicity (or seasonality) of 24 is likely. However, we have no idea what kind of business we’re working with (since all the data is anonymized) and can’t make any assumptions about the nature of consumption throughout the week. Rather than assuming these patterns exist, it helps to sample some days and plot them out. If there is a pattern amongst the sampled values, then there’s a signal we can leverage to make a forecast. Let’s examine how consumption varies throughout the day and week for a random sample of 100 days.</p>
<pre class="r"><code>set.seed(123)
sample_size = 100
sample_days = train_df %&gt;% 
  dplyr::select(week, day) %&gt;% 
  distinct() %&gt;% 
  sample_n(sample_size) %&gt;% 
  inner_join(train_df) %&gt;% 
  mutate(day_of_week = lubridate::wday(date_time, 
                                       label = TRUE))</code></pre>
<pre class="r"><code>ggplot(sample_days, aes(x = hour, y = value, color = day_of_week)) + 
  geom_point() + 
  geom_jitter()  + 
  stat_smooth(se = FALSE, size = 2) + 
  theme_bw() + 
  my_plot_theme() + 
  xlab(&quot;Hour&quot;) + 
  ylab(&quot;Value&quot;) + 
  facet_grid(day_of_week ~ .) + 
  theme(legend.position = &quot;none&quot;, 
  axis.text.y = element_text(size = 13))</code></pre>
<p><img src="/post/time_series_forecasting_with_neural_networks_files/figure-html/unnamed-chunk-10-1.png" width="960" /></p>
<p>Consumption follows a similar pattern for each day of the week, peaking around midnight and reaching a minimum around noon. Thus, the pattern of consumption is stable across weekday but varies by hour within each day, indicating an hourly periodicity. We can confirm this by examining the lagged correlation values. Note much of the code in this section is adapted from <a href="http://www.business-science.io/timeseries-analysis/2017/08/30/tidy-timeseries-analysis-pt-4.html">here</a>.</p>
<pre class="r"><code>k = 1:period_length
col_names = paste0(&quot;lag_&quot;, k)
consumption_lags = train_df %&gt;%
  tq_mutate(
    select = value,
    mutate_fun = lag.xts,
    k = 1:period_length,
    col_rename = col_names
  )

consumption_auto_cor = consumption_lags %&gt;% 
  gather(key = &quot;lag&quot;, value = &quot;lag_value&quot;, -c(date_time, value, week, day, hour)) %&gt;% 
  mutate(lag = str_sub(lag, start = 5) %&gt;% as.numeric) %&gt;% 
  group_by(lag) %&gt;% 
  summarize(
    abs_cor = abs(cor(x = value, y = lag_value, use = &quot;pairwise.complete.obs&quot;))
  ) %&gt;% 
  mutate(lag = factor(lag)) %&gt;% 
  arrange(desc(abs_cor)) %&gt;% 
  head(25)
# examine top 25 correlations
ggplot(consumption_auto_cor, aes(x = fct_reorder(lag, abs_cor, .desc = TRUE) , y = abs_cor)) +
geom_point(size = 3) +
  geom_segment(aes(x=lag, xend=lag, y=0, yend=abs_cor)) + 
  theme_bw() +
  my_plot_theme() + 
  xlab(&quot;Lags&quot;) + 
  ylab(&quot;Absolute Correlation&quot;)</code></pre>
<p><img src="/post/time_series_forecasting_with_neural_networks_files/figure-html/unnamed-chunk-11-1.png" width="960" /> The plot above shows a strong autocorrelation at multiples of 24, confirming our initial hunch of an hourly periodicity. We’ll use this frequency when modeling our time series.</p>
</div>
<div id="feature-engineering-parameter-tuning-forecasting" class="section level3">
<h3>Feature Engineering, Parameter Tuning, &amp; Forecasting</h3>
<p>We’ll implement the forecast with the <code>nnetar</code> function in the <code>forecast</code> package. Neural networks, while incredibly powerful, have a tendency to overfit, so it is imperative to understand how different parameter configurations affect performance on unseen data. Accordingly, a total of seven, 24-hour forecasts are created from our validation set. Each model will have different parameter settings and vary along a complexity spectrum. For example, the size parameter specifies how many nodes are in a single hidden layer within the network. More nodes are capable of handling increasingly complex interactions amongst the inputs and target variable but are also more prone to overfitting.</p>
<p>The tuning process is parallelized with the <code>doParallel</code> and <code>foreach</code> packages. The output of each model is independent of the others, so we can run all candidate models in parallel. Finally, performance is based on two metrics: <em>Mean Absolute Percentage Error</em> (MAPE), and <em>Coverage</em> of the prediction intervals.</p>
<p>Note that we would typically scale our inputs. However, the <code>nnetar</code> automatically takes care of this prior to modeling, which eliminates this step from preprocessing.</p>
<pre class="r"><code># parameter grid
tune_grid = expand.grid(n_seasonal_lags = c(2, 3),
                        size = c(2, 8, 16)
                        )

# init computing cluster
registerDoParallel(cores = detectCores())

# bind training and validation datasets together
train_val_df = bind_rows(train_df,
                         validation_df)</code></pre>
<pre class="r"><code># create moving window across time when validating model
end_index_train = nrow(train_df)
start_index_validation = nrow(train_df) + 1
end_index_validation = start_index_validation + 23
prediction_results = NULL
n_days = 1:7

for(n in n_days){
  temp_train = train_val_df[1:end_index_train,]
  temp_validation = train_val_df[start_index_validation:end_index_validation,]
  temp_train_ts = ts(temp_train$value, frequency = 24)
  # test different parameter settings in parallel
  temp_result = foreach(i = 1:nrow(tune_grid), .combine = rbind) %dopar% {
    nn_fit = forecast::nnetar(temp_train_ts,
                              P = tune_grid[i,]$n_seasonal_lags,
                              size = tune_grid[i,]$size,
                              repeats = 25
                              )
    nn_fcast = forecast(nn_fit, 
                       PI = TRUE,
                       npaths = 50,
                       h = daily_p) %&gt;% 
               data.frame() %&gt;% 
               clean_names()
    
    data.frame(predicted = nn_fcast$point_forecast,
               lwr = nn_fcast$lo_95,
               upr = nn_fcast$hi_95,
               actual = temp_validation$value,
               n_seasonal_lags = tune_grid[i,]$n_seasonal_lags,
               size = tune_grid[i,]$size
               )
  }
  end_index_train = end_index_train + 23
  start_index_validation = end_index_validation
  end_index_validation = end_index_validation + 23
  prediction_results = bind_rows(prediction_results,
                                 temp_result %&gt;% 
                                 mutate(day = n,
                                        hour = rep(0:23, nrow(tune_grid))
                                        ))
}</code></pre>
<p>We have the forecasts for each parameter combination across the seven validation days. We’ll select the model with the best balance between a high <em>Coverage Rate</em> and a low <em>MAPE</em>.</p>
<pre class="r"><code>validation_perf = prediction_results %&gt;% 
            mutate(residual = actual - predicted,
                   coverage = ifelse(actual &gt;= lwr &amp; actual &lt;= upr, 1, 0)) %&gt;% 
            group_by(n_seasonal_lags, size) %&gt;% 
            summarise(MAPE = round(mean(abs(residual)/actual * 100), 2),
                      coverage_95 = round(sum(coverage)/n() * 100, 2)
                      ) %&gt;% 
            data.frame() %&gt;% 
            arrange(desc(coverage_95), MAPE)</code></pre>
<div style="border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:200px; overflow-x: scroll; width:720px; ">
<table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:center;">
n_seasonal_lags
</th>
<th style="text-align:center;">
size
</th>
<th style="text-align:center;">
MAPE
</th>
<th style="text-align:center;">
coverage_95
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
3.87
</td>
<td style="text-align:center;">
85.71
</td>
</tr>
<tr>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
3.88
</td>
<td style="text-align:center;">
85.71
</td>
</tr>
<tr>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
8
</td>
<td style="text-align:center;">
3.84
</td>
<td style="text-align:center;">
83.33
</td>
</tr>
<tr>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
8
</td>
<td style="text-align:center;">
3.81
</td>
<td style="text-align:center;">
82.14
</td>
</tr>
<tr>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
16
</td>
<td style="text-align:center;">
3.74
</td>
<td style="text-align:center;">
77.98
</td>
</tr>
<tr>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
16
</td>
<td style="text-align:center;">
3.82
</td>
<td style="text-align:center;">
75.60
</td>
</tr>
</tbody>
</table>
</div>
<p>According to performance on our validation set, the best model has 3 seasonal lags and 2 nodes in the hidden layer. The coverage rate of 85.71 is reasonably close to 95%, although the intervals are bit too narrow. The MAPEs are similiar amongst all of the models, so the coverage rate is the differentiating factor. However, before moving on, it’s important to dig a bit deeper into the output by examining the residuals. First, we want to consider the autocorrelation between errors. If there is autocorrelation, then there remains signal that we could leverage to improve the model. Second, we want to consider if the forecast is biased upwards or downwards. If there is a bias, we can add a constant (i.e., the magnitude of the bias) to subsequent forecasts. Let’s investigate both issues by visualizing the validation residuals.</p>
<pre class="r"><code>week_results = prediction_results %&gt;% 
               filter(n_seasonal_lags == validation_perf$n_seasonal_lags[1] &amp;
                      size == validation_perf$size[1]
                        ) %&gt;% 
               mutate(residuals = actual - predicted)

week_results %&gt;% 
    dplyr::select(day, hour, predicted, actual) %&gt;% 
    melt(c(&quot;day&quot;, &quot;hour&quot;)) %&gt;% 
    ggplot(aes(x = hour, y = value, color = variable)) + 
    geom_point() + geom_line() + 
    facet_grid(day ~ .) + 
    theme_bw() + 
    my_plot_theme() + 
    theme(plot.title = element_blank())</code></pre>
<p><img src="/post/time_series_forecasting_with_neural_networks_files/figure-html/unnamed-chunk-16-1.png" width="960" /></p>
<p>Based on the plot above, there are continuous “runs” of prediction errors that are either positive or negative, suggesting the presence of autocorrelation. We can provide some statistical rigor to this insight by calculating the correlation coefficients for different lags amongst the residuals.</p>
<pre class="r"><code># all correlation coefficients above abs(0.4) are significant
acf_threshold = 0.4
acf_df = week_results %&gt;% 
         group_by(day) %&gt;% 
         do(day_acf = Acf(.$residuals, plot = FALSE)) %&gt;% 
         data.frame()
acf_df = sapply(acf_df$day_acf, function(x) x[[1]]) %&gt;% 
         data.frame()
names(acf_df) = gsub(&quot;X&quot;, &quot;&quot;, names(acf_df))
acf_df$lag = 0:(nrow(acf_df) - 1)

acf_df %&gt;% 
    filter(lag != 0) %&gt;% 
    melt(&#39;lag&#39;) %&gt;% 
    dplyr::rename(day = variable,
           ACF = value
           ) %&gt;% 
    mutate(sig_acf = as.factor(ifelse(ACF &gt; acf_threshold | ACF &lt; -acf_threshold, 1, 0))) %&gt;% 
    ggplot(aes(x = lag, y = ACF, color = sig_acf)) + 
    geom_point() + 
    geom_segment(aes(x=lag, xend=lag, y=0, yend=ACF)) + 
    facet_grid(day ~ .) + 
    theme_bw() + 
    my_plot_theme() + 
    geom_hline(yintercept = c(acf_threshold, 0, -acf_threshold), linetype = 2) + 
    theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="/post/time_series_forecasting_with_neural_networks_files/figure-html/unnamed-chunk-17-1.png" width="960" /></p>
<p>Autocorrelation is present on several days at lags one and two. There are some steps to remedy the issue, and I’ve found that identifying an omitted variable is a great place to start. For example, this data set is based on power consumption of a commercial building. Perhaps there are events that occur on certain days that lead to prolonged increases or decreases in power consumption outside of what is normal. Such events could be included as an external regressor in the model with the goal of reducing or eliminating the presence of autocorrelation. However, in order to keep things simple, we’ll stick with the model specification suggested by the validation set.</p>
<p>Next, we’ll consider if the forecasts are biased upward or downward. Overall, the mean of our residuals is near zero at 0.18, indicating that our model is not biased.</p>
<p>Finally, let’s examine the distribution of our residuals. One key assumption of the approach used to calculate the prediction intervals is that our residuals follow a normal distribution. If we find that they deviate significantly from a theoretical normal distribution, then our prediction intervals will might not be accurate.</p>
<pre class="r"><code>set.seed(123)
validation_residuals = week_results %&gt;% 
                       mutate(normal_residuals = rnorm(n = n(),
                                                      mean = mean(residuals),
                                                      sd = sd(residuals)
                                                      )) %&gt;% 
                       dplyr::select(residuals, normal_residuals)
validation_residuals %&gt;% 
  melt() %&gt;% 
  dplyr::rename(Residuals = value) %&gt;% 
  ggplot(aes(x = Residuals, fill = variable, color = variable)) + 
  geom_density(alpha = 0.1) + 
  theme_bw() + 
  my_plot_theme() + 
  ylab(&quot;Density&quot;) + 
  theme(legend.title = element_blank())</code></pre>
<p><img src="/post/time_series_forecasting_with_neural_networks_files/figure-html/unnamed-chunk-18-1.png" width="960" /></p>
<p>There is a fair amount of overlap between the distributions, which implies that our errors do in fact follow a normal distribution. We’ll run a KS-test just to be sure, which compares our actual residuals with those generated from our theoretical normal distribution.</p>
<pre class="r"><code>ks.test(validation_residuals$residuals,
        validation_residuals$normal_residuals)</code></pre>
<pre><code>## 
##  Two-sample Kolmogorov-Smirnov test
## 
## data:  validation_residuals$residuals and validation_residuals$normal_residuals
## D = 0.095238, p-value = 0.4313
## alternative hypothesis: two-sided</code></pre>
<p>A non-significant <em>p-value</em> indicates that both samples are drawn from the same distribution, so the assumption of normality is valid. This means we can have greater confidence in the accuracy of our prediction intervals and their ability to quantify forecasting uncertainty.</p>
<p>Now that we have our parameter settings and have investigated potential limitations of the model, let’s get a true test of our performance by making seven 24-hour ahead forecasts on our test set. We’ll also run several benchmark models to compare our performance against. Indeed, while neural networks are very powerful, they take a lot of computing resources and time to run. They also lack the interpretability of other forecasting approaches, such as seasonal decomposition. Can we achieve similar performance with a simpler model? To answer this question, we’ll use the <code>HoltWinters</code> and <code>stl</code> functions – two forecasting approaches that have quick run times and can produce solid forecasting results.</p>
<pre class="r"><code>train_test_df = bind_rows(train_val_df,
                          test_df
                          )
end_index_train = nrow(train_val_df)
start_index_test = nrow(train_val_df) + 1
end_index_test = start_index_test + 23
prediction_results = NULL

for(n in n_days){
  temp_train = train_test_df[1:end_index_train,]
  temp_test = train_test_df[start_index_test:end_index_test,]
  temp_train_ts = ts(temp_train$value, frequency = 24)
  
  nn_fit = forecast::nnetar(temp_train_ts,
                              P = validation_perf[1,]$n_seasonal_lags,
                              size = validation_perf[1,]$size,
                              repeats = 50)
  
  nn_fcast = forecast(nn_fit, 
                      PI = TRUE,
                      npaths = 50,
                      h = daily_p) %&gt;% 
             data.frame() %&gt;% 
             clean_names() %&gt;% 
             setNames(paste0(&#39;nn_&#39;, names(.)))
  
  stl_fcast = forecast(stl(temp_train_ts, 
                           s.window = &quot;periodic&quot;, 
                           robust = TRUE),
                       h = daily_p) %&gt;% 
              data.frame() %&gt;% 
              clean_names() %&gt;% 
              setNames(paste0(&#39;stl_&#39;, names(.)))

  hw_fcast = forecast(HoltWinters(temp_train_ts),
                        h = daily_p) %&gt;% 
             data.frame() %&gt;% 
             clean_names() %&gt;% 
             setNames(paste0(&#39;hw_&#39;, names(.)))
  end_index_train = end_index_train + 23
  start_index_test = end_index_test
  end_index_test = end_index_test + 23
  
  prediction_results = bind_rows(prediction_results,
                                 bind_cols(nn_fcast,
                                           stl_fcast,
                                           hw_fcast) %&gt;% 
                                   dplyr::select(matches(&#39;point_forecast|hi_95|lo_95&#39;)) %&gt;% 
                                   mutate(day = n,
                                          hour = 0:23,
                                          actual = temp_test$value
                                          )
                                 )

}</code></pre>
<p>We’ll compare the forecasting accuracy and coverage between the three methods below.</p>
<pre class="r"><code># combine all forecasts here
mape_calc = function(actual, predicted){
  return(abs(actual - predicted)/actual * 100)
}

coverage_calc = function(actual, upr, lwr){
  return(ifelse(actual &lt; upr &amp; actual &gt; lwr, 1, 0))
}

test_results = prediction_results %&gt;% 
  mutate(nn = mape_calc(actual, nn_point_forecast),
         stl = mape_calc(actual, stl_point_forecast),
         hw= mape_calc(actual, hw_point_forecast),
         nn_coverage = coverage_calc(actual, nn_hi_95, nn_lo_95),
         stl_coverage = coverage_calc(actual, stl_hi_95, stl_lo_95),
         hw_coverage = coverage_calc(actual, hw_hi_95, hw_lo_95)
        ) %&gt;% 
  dplyr::select(nn:hw_coverage)

test_mape = test_results %&gt;% 
            dplyr::select(nn:hw) %&gt;% 
            melt() %&gt;% 
  dplyr::rename(method = variable,
         MAPE = value) %&gt;% 
  group_by(method) %&gt;% 
  summarise(MAPE = round(mean(MAPE), 2)) %&gt;%
  data.frame()

test_coverage = test_results %&gt;% 
                dplyr::select(nn_coverage:hw_coverage) %&gt;% 
                melt() %&gt;% 
                dplyr::rename(method = variable) %&gt;% 
                group_by(method) %&gt;% 
                summarise(coverage = round(sum(value)/n() * 100, 1)) %&gt;% 
                data.frame() %&gt;% 
                mutate(method = gsub(&quot;_coverage&quot;, &quot;&quot;, method))

test_comparison_df = inner_join(test_mape,
                                test_coverage
                                ) %&gt;% 
                     dplyr::rename(coverage_95 = coverage) %&gt;% 
                     arrange(MAPE, coverage_95)</code></pre>
<div style="border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:200px; overflow-x: scroll; width:720px; ">
<table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:center;">
method
</th>
<th style="text-align:center;">
MAPE
</th>
<th style="text-align:center;">
coverage_95
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
nn
</td>
<td style="text-align:center;">
3.89
</td>
<td style="text-align:center;">
89.3
</td>
</tr>
<tr>
<td style="text-align:center;">
hw
</td>
<td style="text-align:center;">
6.35
</td>
<td style="text-align:center;">
86.9
</td>
</tr>
<tr>
<td style="text-align:center;">
stl
</td>
<td style="text-align:center;">
7.69
</td>
<td style="text-align:center;">
86.3
</td>
</tr>
</tbody>
</table>
</div>
<p>While the coverage of our prediction intervals is comparable between the three methods, the prediction accuracy is notably better with the NN. This suggests that the additional training time and lack of interpretability are justified given the low forecasting error. While NNs don’t always outperform the “classic” time series forecasting methods, the results observed here fit with my experiences when working with highly granular time series.</p>
</div>
<div id="final-remarks" class="section level3">
<h3>Final Remarks</h3>
<p>Hopefully this post clarified one the biggest hurdles faced by data scientists and analysts: Getting your data into the right format and checking your assumptions to make the best forecast possible. There are a few more pre-processing steps to go through, but in many cases, the extra effort can result in more accurate predictions. Happy Forecasting!</p>
</div>

      </div>

      


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/r">R</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/neural-networks">Neural Networks</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/forecasting">Forecasting</a>
  
</div>



    </div>
  </div>

</article>



<div class="article-container article-widget">
  <div class="hr-light"></div>
  <h3>Related</h3>
  <ul>
    
    <li><a href="/post/forecasting_with_tom_brady.html">Forecasting with Tom Brady</a></li>
    
    <li><a href="/post/counterfactual_prediction.html">Establishing Causality with Counterfactual Prediction</a></li>
    
    <li><a href="/post/time_series_outlier_detection.html">Time Series Outlier Detection</a></li>
    
    <li><a href="/post/early_trend_detection.html">Early Trend Detection</a></li>
    
    <li><a href="/post/choosing_fantasy_qb.html">Choosing a Fantasy Football Quarterback</a></li>
    
  </ul>
</div>




<div class="article-container">
  
<section id="comments">
  <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "thecodeforest" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017 Mark LeBoeuf &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script id="dsq-count-scr" src="//thecodeforest.disqus.com/count.js" async></script>
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    

  </body>
</html>

