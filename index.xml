<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>The Code Forest on The Code Forest</title>
    <link>/</link>
    <description>Recent content in The Code Forest on The Code Forest</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Mark LeBoeuf</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0700</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Time Series Forecasting with Gradient Boosting</title>
      <link>/post/time_series_forecasting_with_gradient_boosting.html</link>
      <pubDate>Tue, 14 Nov 2017 21:13:14 -0500</pubDate>
      
      <guid>/post/time_series_forecasting_with_gradient_boosting.html</guid>
      <description>&lt;p&gt;&lt;img src=&#34;images/city_lights.jpg&#34; width=&#34;800px&#34; height=&#34;800px&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;When considering classic prediction problems, some of the following examples come to mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Predicting the content of an image&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Classifying Fraudulent Transactions&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Product recommendations&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Illness diagnostics&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A common assumption underlies all of these prediction scenarios: That the relationship between the inputs and output remain constant over time. For example, the appearance of dogs or cats hasn’t changed in recent history and will likely remain the same for many years to come. Thus, in an image prediction task, a model trained with images of dogs and cats from the year 2016 could perform just as well when fed images from the year 2020 because the appearance of dogs and cats will not change much from year to year.&lt;/p&gt;
&lt;p&gt;Now let’s think about classic time series prediction problems, with a few examples listed below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Daily temperature for the next 10 days&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Monthly unemployment rates for 2018 in the United States&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Daily demand for flights between Portland and New York City in December&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The difference between time series prediction problems and those listed initially is that the relationship between our input and outputs are likely changing over time. That is, we have to account for and anticipate where the thing we’re trying to predict might go in the future. For instance, take the last example – predicting daily bookings for flights from Portland to NYC. Has the number of people flying this route increased or decreased over the past few years? Has there been a change in the overall number of people flying in general? Has the introduction of price-comparison websites shifted the time periods during which people book flights? These forces may change consumption patterns over time, so a portion of our data may no longer reflect the relationship between the inputs and output. While this presents a more challenging prediction scenario, all of these forces can be accounted for and modeled. And once they are, a time series prediction problem is no different than any other prediction problem. Let’s make these concepts more concrete by working through an example.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forecasting-power-consumption&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Forecasting Power Consumption&lt;/h3&gt;
&lt;p&gt;Nothing gets me more &lt;em&gt;charged up&lt;/em&gt; than forecasting electricity consumption, so the data we’ll use here is a time series of consumption for an anonymized commercial building from 2012. Measurements were recorded for a single year at five-minute intervals, so each hour has 12 readings, and each day has 288 readings. Our goal is to train a model on several months of consumption data and then produce a one-week-ahead forecast for anticipated consumption. Let’s get started by downloading the data and examining the first few rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs = c(&amp;#39;data.table&amp;#39;,&amp;#39;h2o&amp;#39;,&amp;#39;forecast&amp;#39;,
         &amp;#39;lubridate&amp;#39;,&amp;#39;forcats&amp;#39;,
          &amp;#39;ggforce&amp;#39;, &amp;#39;ggplot2&amp;#39;,
          &amp;#39;reshape&amp;#39;, &amp;#39;knitr&amp;#39;,
         &amp;#39;kableExtra&amp;#39;,
         &amp;#39;dplyr&amp;#39;)
lapply(libs, require, character.only = TRUE)
data_source = &amp;#39;https://open-enernoc-data.s3.amazonaws.com/anon/csv/832.csv&amp;#39;
df_elec = fread(data_source,
                data.table = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:400px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
timestamp
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
dttm_utc
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
value
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
estimated
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
anomaly
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325376300
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 00:05:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.1388
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325376600
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 00:10:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.1388
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325376900
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 00:15:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.1388
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325377200
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 00:20:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
24.1851
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325377500
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 00:25:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.1388
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325377800
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 00:30:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
12.0925
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325378100
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 00:35:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.1388
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325378400
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 00:40:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.1388
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325378700
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 00:45:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.1388
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325379000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 00:50:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.1388
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325379300
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 00:55:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.1388
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325379600
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 01:00:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.1388
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325379900
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 01:05:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.1388
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325380200
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 01:10:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.1388
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1325380500
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012-01-01 01:15:00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.1388
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We only need the date-time and electricity values, so we’ll filter several variables and extract the date information from the timestamps.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_elec = df_elec %&amp;gt;% 
  dplyr::select(-anomaly, -timestamp) %&amp;gt;%
  dplyr::rename(date_time = dttm_utc) %&amp;gt;%
         mutate(date_only = as.Date(date_time)) %&amp;gt;% 
         mutate(month = lubridate::month(date_time, label = TRUE),
                week = as.factor(lubridate::week(date_time)),
                hour = lubridate::hour(date_time),
                day = lubridate::day(date_time))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To keep things really simple, we’ll model the average reading per hour instead of every five minutes, meaning there will be 24 readings per day instead of 288.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_hourly = df_elec %&amp;gt;% 
         group_by(date_only, month, hour) %&amp;gt;% 
         summarise(value = mean(value)) %&amp;gt;% 
         ungroup() %&amp;gt;% 
         mutate(hour = ifelse(nchar(as.character(hour)) == 1, 
                              paste0(&amp;quot;0&amp;quot;, as.character(hour)),
                              hour)) %&amp;gt;% 
         mutate(hour = paste(hour, &amp;quot;00&amp;quot;, &amp;quot;00&amp;quot;, sep = &amp;quot;:&amp;quot;)) %&amp;gt;% 
         mutate(date_time = lubridate::ymd_hms(paste(date_only, hour))) %&amp;gt;% 
         dplyr::select(date_time, month, value) %&amp;gt;% 
         mutate(week = as.factor(lubridate::week(date_time)),
                day = lubridate::day(date_time),
                hour = lubridate::hour(date_time)
                ) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we’ll filter the timeframe to only a few months to speed up the eventual training time and break our dataset into &lt;em&gt;training&lt;/em&gt; and &lt;em&gt;testing&lt;/em&gt; segments. We’ll hold out the last week for testing, which contains a total of 168 observations (24 per day).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_hourly = df_hourly %&amp;gt;% 
            filter(month %in% c(&amp;#39;Feb&amp;#39;,&amp;#39;Mar&amp;#39;, &amp;#39;Apr&amp;#39;, &amp;#39;May&amp;#39;, &amp;#39;Jun&amp;#39;)) %&amp;gt;% 
            dplyr::select(-month)
# daily period (24 hours)
daily_p = 24
# weekly period (7 days)
weekly_p = 7
# hold out last week of time series for testing
train_df = df_hourly[1:(nrow(df_hourly) - daily_p * weekly_p),]
test_df = tail(df_hourly, daily_p * weekly_p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s start by creating a high level view of the entire time series and loading up a custom plotting theme.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_plot_theme = function(){
    font_family = &amp;quot;Helvetica&amp;quot;
    font_face = &amp;quot;bold&amp;quot;
    return(theme(
    axis.text.x = element_text(size = 18, face = font_face, family = font_family),
    axis.text.y = element_text(size = 18, face = font_face, family = font_family),
    axis.title.x = element_text(size = 20, face = font_face, family = font_family),
    axis.title.y = element_text(size = 20, face = font_face, family = font_family),
    strip.text.y = element_text(size = 18, face = font_face, family = font_family),
    plot.title = element_text(size = 18, face = font_face, family = font_family),
    legend.position = &amp;quot;top&amp;quot;,
    legend.title = element_text(size = 16,
    face = font_face,
    family = font_family),
    legend.text = element_text(size = 14,
    face = font_face,
    family = font_family)
))
}

bind_rows(train_df %&amp;gt;% 
            mutate(part = &amp;quot;train&amp;quot;),
          test_df %&amp;gt;% 
            mutate(part = &amp;quot;test&amp;quot;)) %&amp;gt;% 
  dplyr::select(date_time, part, value) %&amp;gt;% 
  ggplot(aes(x = date_time, y = value, color = part)) + 
  geom_line() +   
  facet_zoom(x = date_time %in% c(test_df$date_time)) + 
  theme_bw() + 
  my_plot_theme() + 
  xlab(&amp;quot;Date-Time&amp;quot;) + 
  ylab(&amp;quot;Value&amp;quot;) + 
  theme(legend.title=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/time_series_forecasting_with_gradient_boosting_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is clear seasonality in the data – and that’s what we’ll focus on next. The measurements are at the hourly level, so a periodicity (or seasonality) of 24 is likely. This makes sense: consumption increases during business hours and decreases after-hours. It’s also possible there is a daily component, in that consumption is higher on weekdays relative to weekends. However, we have no idea what kind of business we’re working with here (since all the data is anonymized) and can’t make any assumptions about the nature of consumption throughout the week. Rather than assuming these patterns exist, I find it helps to sample some days and plot them out. If there is a pattern amongst the sampled values, then there’s a signal we can leverage to make a forecast. Let’s examine how consumption varies throughout the day and week for a random sample of 100 days.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
sample_size = 100
sample_days = train_df %&amp;gt;% 
  dplyr::select(week, day) %&amp;gt;% 
  distinct() %&amp;gt;% 
  sample_n(sample_size) %&amp;gt;% 
  inner_join(train_df) %&amp;gt;% 
  mutate(day_of_week = lubridate::wday(date_time, 
                                       label = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(sample_days, aes(x = hour, y = value, color = day_of_week)) + 
  geom_point() + 
  geom_jitter()  + 
  stat_smooth(se = FALSE, size = 2) + 
  theme_bw() + 
  my_plot_theme() + 
  xlab(&amp;quot;Hour&amp;quot;) + 
  ylab(&amp;quot;Value&amp;quot;) + 
  facet_grid(day_of_week ~ .) + 
  theme(legend.position = &amp;quot;none&amp;quot;, 
  axis.text.y = element_text(size = 13))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/time_series_forecasting_with_gradient_boosting_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is minimal consumption on Saturday and Sunday, while Monday through Friday has higher levels. Monday through Thursday follow a similar “S” shaped pattern throughout the day. Friday starts out similar to the other weekdays but trails off in the later hours of the day. Accordingly, there appears to be an hourly and weekly seasonality component that we can model. In the following section, we’ll go over how to translate these components into features.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-engineering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Feature Engineering&lt;/h3&gt;
&lt;p&gt;Based on the initial exploratory analysis, we’ve identified two seasonal components. We’ll capture this via &lt;code&gt;msts&lt;/code&gt;, which is intended for time series data with multiple seasonal periods.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_ts = msts(train_df$value, seasonal.periods = c(daily_p,
                                                    daily_p * weekly_p))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll take our &lt;code&gt;msts&lt;/code&gt; object and extract the seasonal features via a &lt;strong&gt;Fourier Transformation&lt;/strong&gt;. Fourier transformations are pretty incredible, and they have lots of real-world applications. In a nutshell, applying a Fourier transformation to a time-series (or any signal) is a way of “deconstructing” the series into different parts, each with a unique frequency. If all the parts are added back together, we get the original series. Our goal is to take the fewest number of parts that capture the greatest amount of variation when we add them back together. Let’s visualize what this looks like by examining the forecasted seasonal component for the test week.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;K = 2
fourier_train = data.frame(fourier(data_ts, K = c(K, K)))
fourier_test_fcast &amp;lt;- data.frame(fourier(data_ts, 
                                         K = c(K, K), 
                                         h = daily_p * weekly_p))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cbind(fourier_test_fcast,
      data.frame(date_time = test_df$date_time)) %&amp;gt;%
melt(id.vars = &amp;quot;date_time&amp;quot;) %&amp;gt;%
ggplot(aes(x = date_time, y = value, color = variable)) +
  geom_line() +
  facet_grid(variable ~ .) +
  theme_bw() +
  my_plot_theme() + 
  xlab(&amp;quot;Date&amp;quot;) + 
  ylab(&amp;quot;Value&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;,
        axis.text.y = element_text(size = 13),
        strip.text.y = element_text(size = 13))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/time_series_forecasting_with_gradient_boosting_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note how all the features with &lt;em&gt;24&lt;/em&gt; capture the hourly component, while those with a &lt;em&gt;168&lt;/em&gt; capture the daily component. Note also the continuous nature of the waves. Indeed, one clear advantage of using this approach for modeling seasonality is that it produces &lt;strong&gt;smooth&lt;/strong&gt; forecasts. If we look back at the plot of the daily consumption across the week, the hour-to-hour, day-to-day, and week-to-weekend changes are gradual. If we modeled the hour and day features as dummy variables, simply taking the average for each day or hour, then our seasonality would be stepwise and &lt;strong&gt;not smooth&lt;/strong&gt;, which would likely produce sub-optimal forecasts (also &lt;strong&gt;not smooth&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;Next, we’ll extract the &lt;em&gt;trend&lt;/em&gt; component from the time series. We want to remove this part, and the reasoning is simple: Our model can’t account for changes that occur over time. Let’s extract the trend, generate a trend-only forecast, and later on, we’ll add the trend-only forecast back in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ts_input = ts(data_ts, freq = daily_p * weekly_p)

# deconstruct series into seasonal, trend, and error components
decomp_ts = stl(ts_input, s.window = &amp;quot;periodic&amp;quot;, robust = TRUE)

# add together the seasonal and error component
seasonal_error_part = apply(data.frame(decomp_ts$time.series)[,c(1, 3)], 1, sum)

# extract the seasonal component
seasonal_part = as.vector(decomp_ts$time.series[,1])

# extract the trend component
trend_part = ts(decomp_ts$time.series[,2])

# generate a forecast for the trend 
trend_forecast_test = as.vector(forecast(auto.arima(trend_part), 
                          h = nrow(test_df))$mean)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’ll work with the &lt;code&gt;seasonal_error_part&lt;/code&gt;, which is the sum of our seasonal component and error component. In this section we’ll create a &lt;em&gt;seasonal lag&lt;/em&gt;, which is a simple (but effective) way of summarizing recent seasonal variation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seasonal_train = seasonal_part[1:(nrow(train_df) - (weekly_p * daily_p))]
seasonal_test = tail(seasonal_part, weekly_p * daily_p)
# build the training set
train_input = data.frame(value = head(seasonal_error_part, length(seasonal_train)),
                         fourier_train[1:length(seasonal_train),],
                         seasonal_lag = seasonal_train
                         )
# build the testing set
test_input = data.frame(value = test_df$value,
                        fourier_test_fcast,
                        seasonal_lag = seasonal_test
                        )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our time-series has successfully been translated into the format required for forecasting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forecasting-prediction-intervals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Forecasting &amp;amp; Prediction Intervals&lt;/h3&gt;
&lt;p&gt;We’ll use the &lt;code&gt;h2o&lt;/code&gt; package to generate forecasts and prediction intervals. A forecast is our best guess for what we believe the future holds. But that’s only one part of the process. The other part – a prediction interval – captures our certainty surrounding that guess. Prediction intervals can be created with a technique called &lt;em&gt;quantile regression&lt;/em&gt;. Quantile regression works by differentially weighting errors during the model fitting process. It models the relationship between our inputs and a specific quantile (i.e., percentile) of our output. For example, if the goal is to model the relationship between X and Y at the 90th percentile of Y, 10% of the errors should be positive and 90% negative to minimize our loss function (e.g., MAE, MSE). In this case, we’ll generate forecasts at the following percentiles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;10%&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;50%&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;90%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The 50th percentile is our forecast, while the 10th and 90th percentile will serve as the lower and upper bound of our 80 percent prediction interval. If our prediction interval is functioning properly, then approximately 90% of the errors for the 10th percentile should be positive, while 10% of the errors for 90% percentile should be positive. We can verify this assumption by examining the residuals on the test set. I’ve created a function below to do this in &lt;code&gt;h2o&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function to generate quantile regression estimates
gbm_prediction_interval = function(x_var, y_var, train_input, test_input, quantiles, seed){
  pred_interval = list()
  for(q in quantiles){
  quantile_fit = h2o.gbm(x = x_var,
                           y = y_var,
                           training_frame = as.h2o(train_input),
                           distribution = &amp;quot;quantile&amp;quot;,
                           quantile_alpha = q,
                           stopping_rounds = 15,
                           ntrees = 5000,
                           seed = seed
                           )
    quantile_pred = as.vector(predict(quantile_fit, as.h2o(test_input)))
    pred_interval[[paste0(&amp;quot;q_&amp;quot;, q)]] = quantile_pred
  }
  return(pred_interval)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.init()
# specify 80% prediction interval 
quantiles = c(0.1,0.5,0.9)
# set seed for reproduceability
seed = 123
y_var = &amp;quot;value&amp;quot;
x_var = setdiff(names(train_input),y_var)
detrended_consumption = gbm_prediction_interval(x_var, 
                                                y_var, 
                                                train_input,
                                                test_input, 
                                                quantiles, 
                                                seed)
# add trend forecast back to predictions 
predicted_consumption = data.frame(lapply(detrended_consumption, 
                                          function(x) x + trend_forecast_test)) %&amp;gt;% 
                        mutate(value = test_df$value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have our final model and predictions. Before considering performance on the test set, I’ll reflect on something that I wondered the first time I went through all of the steps outlined above: Is it worth the time and trouble? What kind of performance would result if we avoided all the lags, transformations, and assorted data wizardry and did something like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;arima_fit = auto.arima(data_ts)
arima_fcast = forecast(arima_fit, h = nrow(test_df))$mean&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That was two lines. Boom. Done. The ARIMA forecast will serve as our performance benchmark. Keep in mind that ARIMA models the &lt;strong&gt;linear&lt;/strong&gt; relationship between inputs and outputs. There are many instances in which capturing higher-order interactions and non-linearities can greatly improve forecasting accuracy, and we’ll determine if this happens to be one of those instances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# combine all forecasts here
perf_df = test_df %&amp;gt;% 
          mutate(arima_fcast = arima_fcast,
                 gbm_fcast = predicted_consumption$q_0.5,
                 lwr = predicted_consumption$q_0.1,
                 upr = predicted_consumption$q_0.9
                 ) %&amp;gt;% 
          dplyr::select(-week, -day, -hour) %&amp;gt;% 
          dplyr::rename(actual = value) %&amp;gt;% 
          data.frame()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perf_df %&amp;gt;% 
  melt(&amp;#39;date_time&amp;#39;) %&amp;gt;% 
  filter(variable %in% c(&amp;quot;actual&amp;quot;, &amp;quot;arima_fcast&amp;quot;, &amp;quot;gbm_fcast&amp;quot;)) %&amp;gt;% 
  ggplot(aes(x = date_time, y = value, color = variable)) + 
  geom_line(size = 1) + 
  theme_bw() + 
  my_plot_theme() + 
    xlab(&amp;quot;Date-Time&amp;quot;) + 
    ylab(&amp;quot;Value&amp;quot;) + 
  theme(legend.title=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/time_series_forecasting_with_gradient_boosting_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, the difference between the two methods is remarkable: GBM substantially outperforms the ARIMA model. While the granular nature of the data are particularly suited for this forecasting approach, I’ve found you can &lt;em&gt;boost&lt;/em&gt; your accuracy substantially by modeling non-linearities with more advanced machine learning methods. It’s obviously more involved than running a one-liner, and scalability can become an issue when generating many forecasts, but it can pay huge dividends in terms of accuracy.&lt;/p&gt;
&lt;p&gt;So now that we have our initial forecast, let’s examine our prediction interval.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perf_df %&amp;gt;% 
  ggplot(aes(x = date_time, y = actual)) + 
  geom_ribbon(aes(ymin = lwr,
                  ymax = upr),
              fill = &amp;quot;#F8766D&amp;quot;,
              alpha = 0.7,
              linetype = 2,
              size = 2
              ) + 
  geom_line(size = 2
            ) + 
  theme_bw() + 
  my_plot_theme() + 
  xlab(&amp;quot;Date-Time&amp;quot;) + 
  ylab(&amp;quot;Value&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/time_series_forecasting_with_gradient_boosting_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The black line is the actual consumption values, while the shaded red area is our prediction interval. Overall the interval does a decent job of capturing uncertainty surrounding the forecasts, but let’s verify the distribution of our residuals. Are ~90% of errors above the 10th quantile forecast?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(round(sum(ifelse(perf_df$actual - perf_df$lwr &amp;gt;= 0, 1, 0))/nrow(perf_df) * 100, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 89.3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks good! Almost 90% exactly. What about the upper bound?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(round(sum(ifelse(perf_df$actual - perf_df$upr &amp;gt;= 0, 1, 0))/nrow(perf_df) * 100, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 4.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Still reasonably close to 10%. The upper bound is more conservative than it needs to be (i.e., it is too wide in some spots) but it’s not too far from the benchmark. Overall our prediction interval performed well.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-remarks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Final Remarks&lt;/h3&gt;
&lt;p&gt;Ideally, this post clarified one the biggest hurdles faced by data scientists and analysts: Getting your data into the right format to make the best forecast possible. If there is one thing to take away from this post, it’s that time series are no different than “regular” machine learning problems. There are a few more pre-processing steps to go through, but in many cases, the extra effort can result in more accurate predictions. Happy Forecasting!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Choosing a Fantasy Football Quarterback</title>
      <link>/post/choosing_fantasy_qb.html</link>
      <pubDate>Sun, 10 Sep 2017 21:13:14 -0500</pubDate>
      
      <guid>/post/choosing_fantasy_qb.html</guid>
      <description>&lt;p&gt;&lt;img src=&#34;images/mariota.jpg&#34; width=&#34;800px&#34; height=&#34;800px&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;Understanding a new concept is all about connecting it with something you already know. I don’t know much, but I do know Fantasy Football. Thus, when I come across new concepts, I often think to myself, “How can I use this information to beat my friend Steve in Fantasy Football”? This very question was the impetus for putting these words and figures together in a post, which will introduce the idea of using the Beta Distribution to determine your weekly starter. I’ll explain this approach in the context of my 2015 Fantasy Football season.&lt;/p&gt;
&lt;p&gt;At the outset of that season, I drafted two quarterbacks: Joe Flacco and Marcus Mariota (it was a rough draft). Flacco had been in the NFL for a few years, while Mariota was still a rookie yet to play a game. I was also considering a separate rookie, Jameis Winston, who was available to pick up anytime during the season off the waiver wire. Throughout the season, I was faced with the following questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Who do I make the starting QB?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;If one QB is performing poorly, when is the right time to make the switch (e.g., Flacco -&amp;gt; Mariota; Flacco -&amp;gt; Winston; Mariota -&amp;gt; Winston)?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This question is faced by NFL coaches and fantasy owners alike. If your QB has a few bad weeks, should you continue with them into the next week, replace them with the 2nd string QB, or sign a free agent to your team mid-season?&lt;/p&gt;
&lt;p&gt;Before getting into the technical details, let’s first define what “Success” looks like for a Fantasy Football QB. Success can be defined in one word: Consistency. A QB that throws three touchdowns (TDs) every game for the first six games of the season (18 total) is better than a QB who throws five TDs for the first three games and then one TD during the next three games, despite having thrown the same number of TDs. Simply put - you want consistent, reliable performance every week. It doesn’t matter if you win by one point or 50 points – a win is a win. Thus, I evaluate my QB’s performance on the following criteria: A “Successful” performance is defined as &lt;strong&gt;3 or more touchdowns AND/OR 300 or more yards&lt;/strong&gt; for a given week. Touchdowns and passing yards are the two primary sources of QB fantasy points, and a +3TD|300yard weekly statline should cement a QB amongst that week’s top performers. Failing to meet either of these criteria was defined as an “Unsuccessful” performance. Note that this label could also factor in interceptions, pass completions, and fumble, but we’ll keep it simple and just focus on passing yards and passing touchdowns.&lt;/p&gt;
&lt;p&gt;Having defined the evaluation criteria, the data generating process was modeled via the beta distribution. Recall that the beta distribution defines a distribution of probabilities, and we’re interested in the probability of our QB having a Successful week. There are several years of performance history on Joe Flacco, so we can provide a reasonably informed estimate of his weekly probabilty for achieving success (i.e., our prior). In contrast, there is no NFL game history on Mariota or Winston, so we’ll assign each a uniform or uninformative prior. Our estimate of the Success parameter for Winston and Mariota will change rapidly as we acquire in-season data because our posterior is determined entirely from the data. We could create a more informed-–and stronger-–prior by assigning Mariota and Winston the historic first-year league average for all rookie QBs entering the NFL but we’ll keep it simple. A uniform prior means that all probabilities from 0-1 are equally likely.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;collecting-qb-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Collecting QB Data&lt;/h3&gt;
&lt;p&gt;We’ll use the &lt;code&gt;nflgame&lt;/code&gt; python package to gather QB data. We’ll pull 2013-2014 weekly performance data for Joe Flacco to calculate our prior, as well as the 2015 data for all three players. During the season we’ll update our priors to determine which QB we should play for a given week. That is, as we acquire results over the season, updates will be made to obtain a better, more reliable estimate of the “success” parameter for each QB.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import nflgame
import pandas as pd

game_years = range(2013, 2016)
game_weeks = range(1, 17)
qbs = (&amp;quot;Joe Flacco&amp;quot;, 
       &amp;quot;Marcus Mariota&amp;quot;,
       &amp;quot;Jameis Winston&amp;quot;)
       
def get_passing_data(year, week, players, qbs):
    qb_list = list()
    for p in players.passing():
        player = &amp;quot; &amp;quot;.join(str(p.player).split(&amp;quot; &amp;quot;)[:2]) 
        if player in qbs:
            qb_list.append([year, week, player, p.passing_tds, p.passing_yds])
    return qb_list
    
quarterback_data = pd.DataFrame()
for year in game_years:
    print &amp;quot;Retrieving Player Data for {year}&amp;quot;.format(year = year)
    for week in game_weeks:
        games = nflgame.games(year, week)
        players = nflgame.combine_game_stats(games)
        temp_qb_stats = get_passing_data(year, week, players, qbs)
        quarterback_data = quarterback_data.append(pd.DataFrame(temp_qb_stats))
        
quarterback_data.columns = [&amp;quot;year&amp;quot;, &amp;quot;week&amp;quot;, &amp;quot;player&amp;quot;, &amp;quot;touchdowns&amp;quot;, &amp;quot;passing_yds&amp;quot;]
quarterback_data.to_csv(&amp;quot;quarterback_data.csv&amp;quot;, index = False)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next let’s read the &lt;code&gt;quarterback_data.csv&lt;/code&gt; into R, create a plotting function, and examine the first few rows of data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_plot_theme = function(){
    font_family = &amp;quot;Helvetica&amp;quot;
    font_face = &amp;quot;bold&amp;quot;
    return(theme(
    axis.text.x = element_text(size = 18, face = font_face, family = font_family),
    axis.text.y = element_text(size = 18, face = font_face, family = font_family),
    axis.title.x = element_text(size = 20, face = font_face, family = font_family),
    axis.title.y = element_text(size = 20, face = font_face, family = font_family),
    strip.text.y = element_text(size = 18, face = font_face, family = font_family),
    plot.title = element_text(size = 18, face = font_face, family = font_family),
    legend.position = &amp;quot;top&amp;quot;,
    legend.title = element_text(size = 16,
    face = font_face,
    family = font_family),
    legend.text = element_text(size = 14,
    face = font_face,
    family = font_family)
))
}
# custom plotting colors
my_color_theme = c(&amp;quot;#272822&amp;quot;,  &amp;quot;#F92672&amp;quot;,&amp;quot;#66D9EF&amp;quot;,&amp;quot;#A6E22E&amp;quot;, &amp;quot;#A6E22E&amp;quot;, &amp;quot;#F92672&amp;quot;)
libs = c(&amp;#39;dplyr&amp;#39;, &amp;#39;zeallot&amp;#39;, &amp;#39;janitor&amp;#39;, 
         &amp;#39;reshape&amp;#39;, &amp;#39;readr&amp;#39;,&amp;#39;artyfarty&amp;#39;,  
         &amp;#39;knitr&amp;#39;, &amp;#39;ggplot2&amp;#39;, &amp;#39;kableExtra&amp;#39;)
lapply(libs, require, character.only = TRUE)
working_dir = &amp;#39;your_wd_here&amp;#39;
player_data = &amp;#39;quarterback_data.csv&amp;#39;
# define success metrics
n_touchdowns = 3
n_passing_yds = 300
qb_df = read_csv(file.path(working_dir, player_data)) %&amp;gt;% 
        mutate(success = ifelse(touchdowns &amp;gt;= n_touchdowns | passing_yds &amp;gt;= n_passing_yds, 
                                1, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:410px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
year
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
week
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
player
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
touchdowns
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
passing_yds
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
success
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
362
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
211
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
171
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
347
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
269
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
342
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
215
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
250
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
140
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
162
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
273
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
251
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
245
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
222
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
260
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
345
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
166
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
217
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
327
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
235
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
306
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
258
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
195
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
303
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Flacco
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
169
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We now have all the data to get started.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-qb-performance-with-the-beta-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Modeling QB Performance with the Beta Distribution&lt;/h3&gt;
&lt;p&gt;The Beta Distribution is used to model processes involving successes/failure, purchase/non-purchase, clicks/non-clicks – basically any process where there are two potential outcomes, and we’re interested in the probability of one of those outcomes occurring. It is defined with two parameters – &lt;em&gt;alpha&lt;/em&gt; (the number of successes) and &lt;em&gt;beta&lt;/em&gt; (the number of failures). We’ll calculate Flacco’s prior based on the 2013-2014 season performance. He played a total of 30 games during this time, and he threw 3TDs|300yards in eight of those weeks. Accordingly, Flacco’s alpha and beta are 8 and 22, respectively. Mariota and Winston have no history, so their alpha and beta will be 1 and 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this will hold the data we&amp;#39;ll update
flacco_prior =  qb_df %&amp;gt;% 
                filter(player == &amp;#39;Joe Flacco&amp;#39; &amp;amp; year &amp;lt; 2015)

c(flacco_beta, flacco_alpha) %&amp;lt;-% unname(table(flacco_prior$success))
c(mariota_beta, mariota_alpha) %&amp;lt;-% c(1, 1)
c(winston_beta, winston_alpha) %&amp;lt;-% c(1, 1)
# define priors
players_list = list()

# players_list_reference stores posterior distribution after each week
players_list_reference = list()
players_list[[&amp;#39;Joe Flacco&amp;#39;]] = c(flacco_alpha, flacco_beta)
players_list[[&amp;#39;Marcus Mariota&amp;#39;]] = c(mariota_alpha, mariota_beta)
players_list[[&amp;#39;Jameis Winston&amp;#39;]] = c(mariota_alpha, mariota_beta)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s filter the data to the current season and iterate through each week. The alpha or beta parameter is incremented by one, depending on whether the quarterback achieved success for that week, via the &lt;code&gt;update_player_beta&lt;/code&gt; function below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;update_player_beta = function(players_list, current_week, stat_field){
    for(temp_player in names(players_list)){
        player_stats = current_week %&amp;gt;% 
            filter(player == temp_player) %&amp;gt;% 
            select(stat_field)
        if(nrow(player_stats) == 0){
            next
        }
        if(player_stats == 1){
            players_list[[temp_player]][1] = players_list[[temp_player]][1] + 1
        } else {
            players_list[[temp_player]][2] = players_list[[temp_player]][2] + 1
        } 
    }
    return(players_list)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll also calculate the density of each player’s distribution for every value from 0 - 1. This indicates the likelihood of each player achieving success for a given week. For example, a mean of 0.4 would indicate that there’s a 40 percent chance that, after accounting for historical performance, a QB will throw +3TDs|300yards in the following week. We aren’t considering the uncertainty surrounding the estimate of our mean yet, but we’ll tackle that issue in a second. The &lt;code&gt;format_posterior&lt;/code&gt; function below will help transform the data into a format amenable to plotting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;format_posterior = function(players_list){
    posterior = lapply(players_list, function(x) dbeta(seq(0, 
                                                           1, 
                                                           by = 0.01), 
                                                           x[1], 
                                                           x[2])) %&amp;gt;% 
        data.frame() %&amp;gt;% 
        mutate(x = seq(0, 1, by = 0.01)) %&amp;gt;% 
        select(x, 1:(length(players_list))) %&amp;gt;% 
        reshape::melt(id.vars = &amp;quot;x&amp;quot;) %&amp;gt;% 
        dplyr::rename(player = variable,
                      density = value) %&amp;gt;% 
        mutate(weeks_elapsed = i)
    return(posterior)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;current_season = qb_df %&amp;gt;% 
  filter(year == 2015) %&amp;gt;% 
  select(year, week) %&amp;gt;% 
  distinct()

beta_evolution_df = data.frame(NULL)
for(i in 1:nrow(current_season)){
    c(cur_year, cur_week) %&amp;lt;-% current_season[i,]
    current_week = qb_df %&amp;gt;% 
                   filter(year == cur_year, 
                          week == cur_week)
    
    players_list = update_player_beta(players_list, 
                                      current_week, 
                                      &amp;#39;success&amp;#39;)
    players_list_reference[[i]] = players_list
    
    posterior = format_posterior(players_list)
    beta_evolution_df = bind_rows(beta_evolution_df, 
                                  posterior)  
    
}

beta_evolution_df = beta_evolution_df %&amp;gt;% 
                    mutate(player = gsub(&amp;quot;\\.&amp;quot;, &amp;quot; &amp;quot;, player))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s examine how our estimates for each player change as the season unfolds. I’ve plotted this process by storing the Probability Density Functions (PDFs) following each week (a total of 16 games) during the regular season.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(beta_evolution_df, aes(x, density, color = player)) + 
  geom_line(size = 2) + 
  facet_wrap(~weeks_elapsed) + 
  theme_bw() + 
  my_plot_theme() + 
  scale_color_manual(values = c(my_color_theme[1:3])) + 
  theme(legend.title=element_blank(),
        axis.text.x = element_text(size = 10)
        ) + 
  ylab(&amp;quot;Density&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/choosing_fantasy_qb_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By the end of the season, it looks like Mariota would’ve been the correct choice, even though there is a fair amount of overlap between the distributions. However, knowing this at Week 16 isn’t helpful – the fantasy season is over. Let’s say we started the veteran QB Joe Flacco for Week 1, and Week 2 we decided to use some maths to help us choose between our three QBs. There are two related approaches we could use to address this question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Upper Confidence Bound&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Simulation&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll outline both in further detail below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;decisions-based-on-the-upper-confidence-bound&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Decisions based on the Upper Confidence Bound&lt;/h3&gt;
&lt;p&gt;The Upper Confidence Bound algorithm is one way to make decisions in the face of uncertainty, in this case, which quarterback to fire up in our lineup each week. The idea is to choose the option with the greatest potential for a favorable outcome, even if our estimate of the probability of that outcome is uncertain (see &lt;a href=&#34;http://banditalgs.com/2016/09/18/the-upper-confidence-bound-algorithm/&#34;&gt;here&lt;/a&gt; for more detail). We’ll calculate a 90% credible interval for our beta parameter each week. A credible interval is like a confidence interval, but the interpretation is a bit different and, in my opinion, easier to understand. In the current context, a 90% credible interval is the range of values in which we are 90% certain that the probability of weekly success falls based on the data we have thus far (see &lt;a href=&#34;http://www.john-uebersax.com/stat312/17%20Credible%20Intervals%20and%20Confidence%20Intervals.pdf&#34;&gt;here&lt;/a&gt; for an excellent explanation on the distinction between confidence and credible intervals).&lt;/p&gt;
&lt;p&gt;Taking these concepts together, we’ll choose the QB in Week 2 with the highest upper bound on their credible interval. I’ll also include an additional five weeks to show how the interval evolves as we obtain more information.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;credible_intervals = data.frame(NULL)
for(week in 2:length(players_list_reference)){
  temp_week = players_list_reference[[week]]
  for(player in names(temp_week)){
    temp_player = temp_week[[player]]
    lower_cred_int = qbeta(0.05,temp_player[1], temp_player[2])
    upper_cred_int = qbeta(0.95,temp_player[1], temp_player[2])
    alpha = temp_player[1]
    beta = temp_player[2]
    credible_intervals = bind_rows(credible_intervals,
                                   data.frame(player = player,
                                                week = week,
                                                lower = lower_cred_int,
                                                mean = alpha/(alpha + beta),
                                                upper = upper_cred_int))
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(credible_intervals %&amp;gt;% filter(week &amp;lt; 11), 
       aes(x = mean, y = player, color = player)) + 
  geom_point(size = 3) + 
  geom_errorbarh(aes(xmin = lower,
                    xmax = upper),
                 size = 2) + 
  xlab(&amp;quot;Chance of Achieving +300yds | +3TDs During Week&amp;quot;) + 
  theme_bw() + 
  my_plot_theme() + 
  scale_color_manual(values = c(my_color_theme[1:3])) +
  facet_grid(week ~.) + 
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        legend.position = &amp;quot;top&amp;quot;,
        legend.title = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/choosing_fantasy_qb_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Mariota has the largest upper bound for his credible interval, so we should start Mariota during Week 2 and continue starting him as a quarterback until another QB’s upper bound is greater than his. Note also that Flacco’s interval is both narrower and remains relatively unchanged over the six week period. This is because we have a lot more information on Flacco going into the season – that is, our prior is a lot stronger! A few successful or unsuccessful games during the season wont shift Flacco’s interval much. We already have two seasons worth of data indicating that Flacco’s probability of success hovers somewhere around 30% but is likely not higher than 40% or lower than 20%. In contrast, Marriota’s credible interval changes rapidly from one week to the next. By Week 7, Mariota’s interval drifts toward a more reasonable range (0.15 - 0.70), a process known formally as shrinkage.&lt;/p&gt;
&lt;p&gt;The Upper Bound approach is an easy way to choose a QB. But what if we wanted a specific estimate of the probability that one QB was “better” than the other? For that question, we’ll turn to simulation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;decisions-based-on-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Decisions Based on Simulation&lt;/h3&gt;
&lt;p&gt;Let’s keep this simple and compare Flacco vs. Mariota following Week 2. Our goal is to determine the probability that one QB, or, more formally, their beta distribution, is greater/better than the other, according to our criteria for success. We’ll simulate 1000 draws from the posterior distribution of each week via the &lt;code&gt;simulate_weeks&lt;/code&gt; function, then compare the number of instances in which Mariota’s sampled beta was greater than Flacco’s.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simulate_weeks = function(player_1, player_2, n_simulations, players_list_reference){
  simulated_outcome = data.frame(NULL)
  set.seed(123)
  for(reference_week in 1:length(players_list_reference)){
      player1_simulation = rbeta(n_simulations,
            players_list_reference[[reference_week]][player_1][[1]][1],
            players_list_reference[[reference_week]][player_1][[1]][2])

      player2_simulation = rbeta(n_simulations,
                                 players_list_reference[[reference_week]][player_2][[1]][1],
                                 players_list_reference[[reference_week]][player_2][[1]][2])

      player1_v_player2 = mean(player1_simulation &amp;gt; player2_simulation)
      simulated_outcome = bind_rows(simulated_outcome,
                data.frame(weeks_elapsed = reference_week,
                 player = c(player_1, player_2),
                 simulations_won = c(player1_v_player2,
                                     1 - player1_v_player2)
                ))
  }
  return(simulated_outcome)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;player_1 = &amp;quot;Marcus Mariota&amp;quot;
player_2 = &amp;quot;Joe Flacco&amp;quot;
n_simulations = 1000
simulated_outcome = simulate_weeks(player_1, 
                                   player_2, 
                                   n_simulations, 
                                   players_list_reference)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(simulated_outcome, aes(x = weeks_elapsed, y = simulations_won, 
                              color = player)) +
    scale_x_continuous(breaks = seq(0, 20, 5)) + 
    xlab(&amp;quot;N Weeks Elapsed&amp;quot;) + 
    ylab(&amp;quot;Simulations Won&amp;quot;) + 
    geom_point(size = 2) + geom_line(size = 2) + 
    theme_bw() + 
    my_plot_theme() + 
    scale_color_manual(values = c(my_color_theme[c(1, 3)])) +
    theme(legend.title=element_blank()) + 
    scale_x_continuous(breaks = seq(1, 16))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/choosing_fantasy_qb_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From Week 2 there is a ~80% chance that Mariota is the better choice relative to Flacco. By Week 7 there is ~75% chance, and by Week 12 there is a ~79% chance. If someone asked you at the end of the season who was the better quarterback, you could say with about 75% certainty that Mariota was better than Flacco, according to the criteria defined here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;key-takeaways&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Key Takeaways&lt;/h3&gt;
&lt;p&gt;As mentioned above, this system has worked well for me over the past few seasons. It does have some obvious shortcomings, namely that it doesn’t account for the strength of an opposing defense, health of receivers, recent performance, or the Over/Under for the game – all factors that should be considered when selecting a weekly QB. However, this approach provides an objective way to quantify the potential of a QB to deliver a solid weekly performance.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Combine Analysis</title>
      <link>/post/combine_analysis.html</link>
      <pubDate>Mon, 04 Sep 2017 21:13:14 -0500</pubDate>
      
      <guid>/post/combine_analysis.html</guid>
      <description>&lt;p&gt;&lt;img src=&#34;images/running_man.jpg&#34; width=&#34;800px&#34; height=&#34;800px&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;A good draft is a big part of doing well in Fantasy Football. Every year, Fantasy Football pundits write droves of articles on “sleeper” picks, or players that aren’t on most people’s draft sheets. Many of these players are untested rookies drafted straight from college. So how do you know differentiate which rookies will remain sleepers from those who will propel your fantasy team to victory? One place to start is the NFL combine.&lt;/p&gt;
&lt;p&gt;Indeed, I’m not the first person to investigate the relationship between combine stats/physical attributes and player performance. NFL teams do it every season, as well as fellow data nerds (see &lt;a href=&#34;http://harvardsportsanalysis.org/2015/04/predicting-the-nfl-draft-using-seven-numbers-mock-draft-using-the-combine/%5D&#34;&gt;here&lt;/a&gt;). However, prior analyses have focused on performance over a longer period of time, such as the first three years in the league. This makes sense from the perspective of an NFL team, as rookies are typically signed for longer than a one-year contract. In contrast, most fantasy football leagues operate like one-year contracts; you draft a new team at the beginning of each year. Thus, in the case of NFL rookies, I’m interested exclusively in first year NFL performance. Can we learn anything from the combine that isn’t already baked into a rookie’s draft position?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nfl-combine-performance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;NFL Combine Performance&lt;/h3&gt;
&lt;p&gt;The NFL combine is like the SAT or ACT except for football, with more lifting and less clothing. It’s a chance for NFL teams to see how fast and strong NFL hopefuls are in a controlled environment. During the combine players typically complete the following events:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;40 yard dash&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Number of reps of 225lbs on the bench press (my number is 0)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How high someone can jump (vertical leap)&lt;/strong&gt; &lt;strong&gt;How far someone can jump (broad jump)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Basically run back and forth between two lines as fast as possible (shuttle)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Again more running between cones (3cone)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to these drills, height and weight are also measured. This data gives NFL team’s a holistic perspective to evaluate players. Despite a number of findings indicating the limited utility of this information translating into actual performance in the NFL, the difference between a 4.4 second and 4.6 second 40 yard dash can mean big differences in draft position.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;draft-position&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Draft Position&lt;/h3&gt;
&lt;p&gt;Most rookies in the NFL enter a team through the draft. A player’s draft position is a reasonable proxy for their perceived value. If a rookie is believed to be a game-changer, then they’ll have a low draft position; if they have some skills but need a little more work, then they’ll have a higher draft position. Indeed, rookie pay is directly correlated with draft position, such that players drafted in earlier rounds are paid more money than those drafted later. Thus draft position should relate to first-year performance, otherwise teams would just choose players at random, hoping to land the next Tom Brady or Julio Jones based on the phases of the moon or their horoscopes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;first-year-performance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;First Year Performance&lt;/h3&gt;
&lt;p&gt;I adopted a common scoring system from most standard fantasy football leagues when quantifying first-year performance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Touchdowns: 5 points&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Every 10 receiving yards: 1 point&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Every 10 rushing yards: 1 point&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Every fumble: -2 points&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, let’s say you draft a running back and he scores five touchdowns, rushes for 500 yards, and fumbles the ball twice during his rookie season. The points for this player would be (5 * 5) + (500 * 0.1) + (2 * -2) = 71 points. Additionally, 50 rushing or receiving yards is considered equivalent to one touchdown.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;collecting-combine-and-draft-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Collecting Combine and Draft Data&lt;/h3&gt;
&lt;p&gt;The first thing we’ll do is pull combine results and draft position data for all rookies in the NFL from 2011-2016. The &lt;code&gt;rvest&lt;/code&gt; package will do the heavy lifting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs = c(&amp;#39;rvest&amp;#39;, &amp;#39;dplyr&amp;#39;, &amp;#39;janitor&amp;#39;,
         &amp;#39;GGally&amp;#39;, &amp;#39;zeallot&amp;#39;, &amp;#39;mgcv&amp;#39;,
          &amp;#39;knitr&amp;#39;, &amp;#39;kableExtra&amp;#39;, &amp;#39;readr&amp;#39;)
lapply(libs, require, character.only = TRUE)
years = seq(2011, 2016)
combine_data = data.frame(NULL)
draft_position_data = data.frame(NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(y in years){
  print(paste0(&amp;quot;COLLECTING DATA FROM &amp;quot;, y))
  combine_url = paste0(&amp;#39;http://nflcombineresults.com/nflcombinedata.php?year=&amp;#39;,
              y,
             &amp;#39;&amp;amp;pos=&amp;amp;college=&amp;#39;)

 # collect combine data
  yearly_combine_data = combine_url %&amp;gt;%
        read_html() %&amp;gt;%
        html_nodes(xpath = &amp;#39;//*[@id=&amp;quot;datatable&amp;quot;]/table&amp;#39;) %&amp;gt;%
        html_table(fill = TRUE, header = TRUE) %&amp;gt;%
        as.data.frame()

  combine_data = bind_rows(combine_data,
                           yearly_combine_data)

  draft_position_url = paste0(&amp;#39;http://www.drafthistory.com/index.php/years/&amp;#39;,
                              y)
  # collect draft position data
  yearly_draft_data = draft_position_url %&amp;gt;%
       read_html() %&amp;gt;%
       html_nodes(xpath = &amp;#39;//*[@id=&amp;quot;main&amp;quot;]/table[1]&amp;#39;) %&amp;gt;%
       html_table(fill = TRUE, header = TRUE) %&amp;gt;%
       as.data.frame()

  names(yearly_draft_data) = yearly_draft_data[1,]
  yearly_draft_data[2:nrow(yearly_draft_data),]
  yearly_draft_data$year = y
  draft_position_data = bind_rows(draft_position_data,
                                  yearly_draft_data)

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we’ll do a bit of data munging and then join the draft and combine data together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;draft_position_data_clean = draft_position_data %&amp;gt;%
                      filter(Player != &amp;#39;Player&amp;#39;) %&amp;gt;%
                      clean_names() %&amp;gt;%
                      select(-pick, -round, -college, -position) %&amp;gt;%
                      dplyr::rename(nfl_team = team,
                                    pick = player)

combine_data_clean = combine_data %&amp;gt;%
               clean_names() %&amp;gt;%
               select(-na, -na_1)

# filter only players that are Running Backs &amp;amp; Wide Receivers
combine_draft_join = left_join(combine_data_clean, 
                               draft_position_data_clean) %&amp;gt;% 
                      filter(pos %in% c(&amp;#39;RB&amp;#39;, &amp;#39;WR&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
And let’s have a look at the first few rows of data.
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:410px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
year
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
name
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
college
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
pos
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
height_in
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
weight_lbs
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
wonderlic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
x40_yard
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
bench_press
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
vert_leap_in
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
broad_jump_in
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
shuttle
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
x3cone
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
pick
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nfl_team
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Darvin Adams
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Auburn
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74.13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
190
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.56
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Anthony Allen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Georgia Tech
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72.75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
228
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.56
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
41.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
120
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.06
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
225
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Ravens
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Armando Allen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Notre Dame
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
68.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
199
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.52
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Matt Asiata
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Utah
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
229
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.77
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
30.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
104
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.37
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.09
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jon Baldwin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Pittsburgh
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
76.38
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
228
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.49
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
42.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
129
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.34
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Chiefs
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Damien Berry
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Miami (FL)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
211
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
120
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Armon Binns
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cincinnati
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
75.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
209
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.86
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Allen Bradford
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Southern California
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70.88
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
242
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.53
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
28
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
113
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.39
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.97
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
187
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Buccaneers
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
DeAndre Brown
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Southern Mississippi
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
77.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
233
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
117
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.33
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.93
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Vincent Brown
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
San Diego State
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
187
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.68
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
121
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
82
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Chargers
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Stephen Burton
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
West Texas A&amp;amp;M
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
73.38
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
221
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
117
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.04
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
236
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Vikings
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Delone Carter
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Syracuse
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
68.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
222
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.54
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
27
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
120
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.92
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
119
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Colts
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
John Clay
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wisconsin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
230
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.83
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
111
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Randall Cobb
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kentucky
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
191
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
115
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.34
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.08
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Packers
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Graig Cooper
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Miami (FL)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
205
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.60
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
114
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.66
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mark Dell
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Michigan State
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
193
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.54
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Noel Devine
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
West Virginia
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
67.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
179
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.34
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tandon Doss
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Indiana
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
201
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.56
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Ravens
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Shaun Draughn
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
North Carolina
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70.88
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
213
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.73
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Darren Evans
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Virginia Tech
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
227
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.56
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
111
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.96
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mario Fannin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Auburn
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70.38
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
231
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.37
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
115
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Edmond Gates
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Abilene Christian (TX)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71.75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
192
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
131
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
111
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Dolphins
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A.J. Green
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Georgia
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
75.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
211
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
126
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bengals
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Alex Green
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Hawaii
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
225
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.45
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
114
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
96
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Packers
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tori Gurley
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
South Carolina
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
76.13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
216
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.53
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.05
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Overall looks good. One thing you’ll notice is that a lot of the players who participated in the combine don’t have a draft pick, which means that they were never drafted by a team. While there are many examples of undrafted players going on to have stellar rookie careers, these guys don’t show up much during the fantasy draft process. Thus any player without a draft pick is considered in the following analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;collecting-rookie-performance-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Collecting Rookie Performance Data&lt;/h3&gt;
&lt;p&gt;Next, we’ll collect the outcome variable – how many points each player scored during their rookie season - and enlist the service of the &lt;code&gt;nflgame&lt;/code&gt; python package. We’ll write out the players and their rookie years to the &lt;code&gt;rookies.csv&lt;/code&gt; file, pull that data into python, collect the first year stats, and then pull the output back into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;input_file_name = &amp;quot;rookie_names_years.csv&amp;quot;
python_script_name = &amp;quot;collect_rookie_stats.py&amp;quot;
output_file_name = &amp;quot;year_1_rookie_stats.csv&amp;quot;
combine_draft_join %&amp;gt;% 
  select(year, name) %&amp;gt;% 
  write.csv(input_file_name, 
            row.names = FALSE)

exe_pyscript_command = paste0(&amp;quot;//anaconda/bin/python &amp;quot;,
                              python_script_name,
                              &amp;quot; &amp;quot;,
                              &amp;quot;&amp;#39;&amp;quot;, input_file_name, &amp;quot;&amp;#39;&amp;quot;,
                              &amp;quot; &amp;quot;,
                              &amp;quot;&amp;#39;&amp;quot;, output_file_name, &amp;quot;&amp;#39;&amp;quot;
                              
                              )
print(exe_pyscript_command)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll execute the &lt;code&gt;collect_rookie_stats.py&lt;/code&gt; script from within R, then read the &lt;code&gt;rookie_stats.csv&lt;/code&gt; file back into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system(exe_pyscript_command)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import sys
import pandas as pd
import nflgame
from pandasql import *

def collect_rushing_stats(year, week, players):
    rushing_stats = list()
    for p in players.rushing():
        rushing_stats.append([year,
                              week,
                              &amp;quot; &amp;quot;.join(str(p.player).split(&amp;quot; &amp;quot;)[:2]), 
                              p.rushing_tds, 
                              p.rushing_yds,
                              p.fumbles_lost])
    rushing_df = pd.DataFrame(rushing_stats)
    rushing_df.columns = [&amp;#39;year&amp;#39;, 
                            &amp;#39;week&amp;#39;,
                            &amp;#39;name&amp;#39;,
                            &amp;#39;rushing_tds&amp;#39;,
                            &amp;#39;rushing_yds&amp;#39;,
                            &amp;#39;rushing_fb&amp;#39;]
    return(rushing_df)

def convert_rushing_pts(td_pts, rushing_pts, fb_pts):
    return(pysqldf(&amp;quot;&amp;quot;&amp;quot;
        SELECT year, 
               name, 
               td_pts + rushing_pts + fb_pts AS total_pts 
        FROM 
            (SELECT year, 
                    name, 
                    SUM(rushing_tds) * {td_pts} AS td_pts, 
                    SUM(rushing_yds) * {rushing_pts} AS rushing_pts,
                    SUM(rushing_fb) * {fb_pts} AS fb_pts
            FROM rb_df_temp
            GROUP BY year, name)
        NATURAL JOIN 
        rookie_df
        &amp;quot;&amp;quot;&amp;quot;.format(td_pts = td_pts,
                   rushing_pts = rushing_pts,
                   fb_pts = fb_pts
                    )))

def collect_receiving_stats(year, week, players):
    receiving_stats = list()

    for p in players.receiving():
        receiving_stats.append([year,
                                week,
                                &amp;quot; &amp;quot;.join(str(p.player).split(&amp;quot; &amp;quot;)[:2]), 
                                p.receiving_tds, 
                                p.receiving_yds,
                                p.fumbles_lost])
    receiving_df = pd.DataFrame(receiving_stats)
    receiving_df.columns = [&amp;#39;year&amp;#39;, 
                        &amp;#39;week&amp;#39;,
                        &amp;#39;name&amp;#39;,
                        &amp;#39;receiving_tds&amp;#39;,
                        &amp;#39;receiving_yds&amp;#39;,
                        &amp;#39;receiving_fb&amp;#39;]
    return(receiving_df)

def convert_receiving_pts(td_pts, receiving_pts, fb_pts):
    return(pysqldf(&amp;quot;&amp;quot;&amp;quot;
        SELECT year, 
               name, 
               td_pts + receiving_pts + fb_pts AS total_pts 
        FROM 
            (SELECT year, 
                    name, 
                    SUM(receiving_tds) * {td_pts} AS td_pts, 
                    SUM(receiving_yds) * {receiving_pts} AS receiving_pts,
                    SUM(receiving_fb) * {fb_pts} AS fb_pts
            FROM wr_df_temp
            GROUP BY year, name)
        NATURAL JOIN 
        rookie_df
        &amp;quot;&amp;quot;&amp;quot;.format(td_pts = td_pts,
                   receiving_pts = receiving_pts,
                   fb_pts = fb_pts
                    )))
                    

def main(input_file_path, output_file_path):
    # define scoring scheme, years, &amp;amp; weeks here 
    td_pts = 5
    receiving_pts = rushing_pts = 0.1
    fb_pts = -2
    game_years = range(2011, 2017)
    game_weeks = range(1, 17) 
    # input file 
    global rookie_df
    rookie_df = pd.read_csv(input_file_path)
    # store stats for each year in player_stats_df
    global player_stats_df    
    player_stats_df = pd.DataFrame()
    for year in game_years:
        print(&amp;quot;Processing Game Data From {year}&amp;quot;.format(year = year))
        temp_df = rookie_df[rookie_df[&amp;#39;year&amp;#39;] == year]
        global rb_df_temp 
        rb_df_temp = pd.DataFrame()
        global wr_df_temp
        wr_df_temp = pd.DataFrame()
        for week in game_weeks:
            games = nflgame.games(year, week)
            players = nflgame.combine_game_stats(games)

            rb_df_temp = rb_df_temp.append(collect_rushing_stats(year, 
                                                                 week, 
                                                                 players))
            wr_df_temp = wr_df_temp.append(collect_receiving_stats(year, 
                                                                   week, 
                                                                   players))
        print &amp;#39;calculating running back points&amp;#39;
        player_stats_df = player_stats_df.append(convert_rushing_pts(td_pts,
                                                                 rushing_pts,
                                                                 fb_pts,
                                                                 ))
        print &amp;#39;calculating wide receiver points&amp;#39;
        player_stats_df = player_stats_df.append(convert_receiving_pts(td_pts,
                                                                   receiving_pts,
                                                                   fb_pts
                                                                   ))

    # aggregate rookies that have both receiving and running stats
    stats_final_df = pysqldf(&amp;quot;&amp;quot;&amp;quot;
                             SELECT year, name, SUM(total_pts) as total_pts
                             FROM player_stats_df
                             GROUP BY year, name
                             &amp;quot;&amp;quot;&amp;quot;)

    stats_final_df.to_csv(output_file_path, index = False)

if __name__ == &amp;quot;__main__&amp;quot;:
    pysqldf = lambda q: sqldf(q, globals())
    main(sys.argv[1], sys.argv[2])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below we’ll read the output from the &lt;code&gt;collect_rookie_stats.py&lt;/code&gt; script back into R and examine the top 10 rows. We’ll also apply a few filters. First, all non-draft picks are eliminated from consideration. We want to control for draft pick, given that our central question is whether we can explain variation in first-year performance from information other than draft pick. Second, all rookies with less than 10 points (an arbitrary cutoff) during their rookie season are eliminated. This is to remove some of the noise that is unrelated to performance (e.g., low points due to injury and not poor performance).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rookie_stats = read_csv(file.path(working_directory, output_file_name)) %&amp;gt;% 
                      inner_join(combine_draft_join) %&amp;gt;% 
                      filter(pos %in% c(&amp;quot;WR&amp;quot;, &amp;quot;RB&amp;quot;) &amp;amp; 
                             is.na(pick) == FALSE &amp;amp; 
                             total_pts &amp;gt;= 10) %&amp;gt;% 
                      mutate(pick = as.numeric(pick),
                             pos = as.factor(pos)) %&amp;gt;% 
                      select(year, name, pos, pick,
                             height_in, weight_lbs, x40_yard,
                             bench_press, vert_leap_in, total_pts
                             ) %&amp;gt;% 
                      data.frame()&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:410px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
year
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
name
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
pos
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
pick
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
height_in
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
weight_lbs
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
x40_yard
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
bench_press
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
vert_leap_in
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
total_pts
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A.J. Green
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
75.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
211
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
143.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Austin Pettis
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
78
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
209
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.56
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Daniel Thomas
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
62
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
230
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
62.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Delone Carter
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
119
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
68.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
222
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.54
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
27
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
42.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Denarius Moore
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
148
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
194
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.43
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
36.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
87.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Evan Royster
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
177
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
212
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23.9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Greg Little
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
231
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.51
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
27
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
82.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Greg Salas
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
112
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
73.13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
210
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.53
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jacquizz Rodgers
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
145
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
65.88
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
196
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
41.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jeremy Kerley
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
153
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
189
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.56
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jon Baldwin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
76.38
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
228
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.49
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
42.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
27.7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Julio Jones
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74.75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
220
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.34
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
38.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
121.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kendall Hunter
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
115
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
67.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
199
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
68.1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Leonard Hankerson
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
73.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
209
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
36.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mark Ingram
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
28
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69.13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
215
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.62
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
75.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Randall Cobb
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
191
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Roy Helu
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
105
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
219
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
36.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
98.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Shane Vereen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
56
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
210
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.49
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10.7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Stevan Ridley
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
73
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
225
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
36.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
42.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Titus Young
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71.38
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
174
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.43
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
79.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Torrey Smith
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72.88
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
204
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.41
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
41.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
119.7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Vincent Brown
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
82
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
187
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.68
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
42.9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2012
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Alfred Morris
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
173
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69.88
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
219
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
189.1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2012
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Alshon Jeffery
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
45
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74.88
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
216
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44.1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2012
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bernard Pierce
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
84
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
218
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.45
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
36.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We now have all of the necessary data. We’ll visualize the bivariate relationships between our variables, segmented by position (Running Back or Wide Receiver) for a quick quality check.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g = ggscatmat(rookie_stats, 
              columns = 4:dim(rookie_stats)[2], 
              color = &amp;quot;pos&amp;quot;)
plot(g)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/combine_analysis_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;960&#34; /&gt; Everything looks good except for the 40 yard dash field. Most players fall in the 4-6 range except a few. Let’s examine observations with a 40 yard dash time greater than six seconds.&lt;/p&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:300px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
year
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
name
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
pos
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
pick
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
height_in
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
weight_lbs
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
x40_yard
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
bench_press
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
vert_leap_in
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
total_pts
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2016
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Corey Coleman
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
194
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
52.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2016
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Devontae Booker
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
136
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70.75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
219
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
81.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2016
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jonathan Williams
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
156
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
70.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
220
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2016
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jordan Howard
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RB
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
150
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71.88
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
230
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
180.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;It looks like missing or invalid 40 yard dash times were coded with 9.99. Let’s sub these values with NAs and re-run the plot above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rookie_stats$x40_yard = ifelse(rookie_stats$x40_yard == 9.99, 
                               NA, 
                               rookie_stats$x40_yard)

g = ggscatmat(rookie_stats, 
              columns = 4:dim(rookie_stats)[2], 
              color = &amp;quot;pos&amp;quot;)
plot(g)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/combine_analysis_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ahh that’s better. Now we’re ready to do some analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explaining-rookie-performance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Explaining Rookie Performance&lt;/h3&gt;
&lt;p&gt;Based on the scatterplot matrix, draft pick explains a fair amount of variability in first-year fantasy points. Teams shell out millions of dollars for rookie players, so it’s not surprising that points and pick number are inversely related, such that lower picks (1, 2, 3) score more fantasy points during their first season than higher picks. The other variable that exhibits a relationship with points is weight, which is moderated by position. Running Backs (RBs) appear to benefit more from having a few extra pounds relative to Wide Receivers (WRs). Being able to punch the ball in from the 1-yard line means more touchdowns. Having a more robust build might translate into fewer injuries and missed games. And being heavier would make an RB harder to tackle, which means more running yards. Having extra weight is less beneficial to a WR, as they rely on their speed and agility to create separation from defenders. Indeed, heavier players run slower 40-yard dash times, and I haven’t seen too many successful WRs in the NFL who were slow. This suggests that given the option between a lighter or heavier RB, go with the big guy.&lt;/p&gt;
&lt;p&gt;Teams do seem to factor the weight of an RB into their draft strategy, as heavier RBs are drafted earlier on. But the question is whether weight is still significant after controlling for draft pick number and accounting for position (WR vs. RB). I’ll apply my two favorite modeling approaches when interpretation is paramount: Linear Model (LM) and General Additive Model (GAM). Pick number is present in both models. The LM features an interaction term to capture the disparate relationship between weight and position. The GAM model has a smoother for both weight and pick-number, and position is included as a dummy variable. For those unfamiliar with GAM, it’s a fantastic approach for modeling non-linear relationships while maintaining a highly interpretable model. The one drawback is that it is &lt;em&gt;Additive&lt;/em&gt; and thus doesn’t account for interactions (of course you can add interactions terms into the model, but then it’s no longer a GAM). We’ll use &lt;em&gt;LOOCV&lt;/em&gt; (leave-one-out-cross-validation) to determine which method of describing the data generating process is most generalizable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function for cross validation
split_data = function(input_df, pct_train){
  set.seed(1)
  data_list = list()
  random_index = sample(1:nrow(input_df), nrow(input_df))
  train_index = random_index[1:(floor(nrow(input_df) * pct_train))]
  data_list[[&amp;#39;train_df&amp;#39;]] = input_df[train_index,]
  data_list[[&amp;#39;test_df&amp;#39;]] = input_df[setdiff(random_index, train_index)  ,]
  return(data_list)
}

input_df = rookie_stats
pct_train = 0.8
fdata = split_data(input_df, pct_train)

row_index = sample(1:nrow(fdata$train_df), nrow(fdata$train_df))

# empty vectors to store prediction errors during LOOCV
lm_pred = c()
gam_pred = c()

for(i in row_index){
  # training data
  temp_train = fdata$train_df[setdiff(row_index, i),]
  
  # validation datum
  temp_validation = fdata$train_df[i,]  
  
  # linear model
  temp_fit_lm = lm(total_pts ~ weight_lbs * pos + pick, 
                   data = temp_train)
  
  # GAM model
  temp_fit_gam = mgcv::gam(total_pts ~ s(weight_lbs) + s(pick) + pos,
                                     data = temp_train)
  # linear model prediction
  lm_pred = c(lm_pred,
               predict(temp_fit_lm, temp_validation))
  # GAM model prediction
  gam_pred = c(gam_pred,
               predict(temp_fit_gam, temp_validation))

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s consider two different measures of performance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_squared = function(actual, predicted){
  return(1 - (sum((actual-predicted )^2)/sum((actual-mean(actual))^2)))
} 

mdn_abs_error = function(actual, predicted){
  return(median(abs(actual-predicted)))
}

val_actual = fdata$train_df$total_pts[row_index]

val_perf = data.frame(model = c(&amp;quot;LM&amp;quot;, &amp;quot;GAM&amp;quot;),
                      # calculate r-squared
                      r_squared = c(r_squared(val_actual,lm_pred),
                                    r_squared(val_actual,gam_pred)),
                      # calculate MAE
                      median_abs_err = c(mdn_abs_error(val_actual, lm_pred),
                                         mdn_abs_error(val_actual, gam_pred))) %&amp;gt;% 
                      mutate_if(is.numeric, round, 2)
print(val_perf)               &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##     model   r_squared    median_abs_err
## 1    LM        0.17         26.15
## 2   GAM        0.17         27.08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The LM performed similiarly to the GAM, so we’ll use the simpler, LM model on the entire training set and examine the coefficients.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_fit = lm(total_pts ~ weight_lbs * pos + pick, 
                   data = fdata$train_df)
summary(train_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Call:
## lm(formula = total_pts ~ weight_lbs * pos + pick, data = fdata$train_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -70.946 -27.584  -9.839  17.944 157.722 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)      -176.63257  101.66213  -1.737  0.08490 .  
## weight_lbs          1.31363    0.46120   2.848  0.00518 ** 
## posWR             235.87977  119.20973   1.979  0.05016 .  
## pick               -0.31399    0.07396  -4.245 4.35e-05 ***
## weight_lbs:posWR   -1.21318    0.56198  -2.159  0.03288 *  
## ---
## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
## 
## Residual standard error: 43.19 on 119 degrees of freedom
## Multiple R-squared:  0.2424, Adjusted R-squared:  0.217 
## F-statistic:  9.52 on 4 and 119 DF,  p-value: 1.033e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interestingly, even after controlling for pick number, the interaction term is still significant. Interpreting interactions terms can be tricky, and I’ve found it helpful to just plug in a range of numbers for the variable you’re interested in and hold everything else constant to see how the predicted value changes. So that’s what we’ll do here. We’ll hold pick number at the mean, enter “RB” for the position dummy variable, and then generate a sequence between the 5th and 95th percentiles of weight. The slope of our predicted value indicates how many additional points we expect to score during a season for each RB pound.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# select 5th and 95th percentiles for weight
# hold pick number and position constant

q5_95 = unname(quantile(fdata$train_df$weight_lbs, c(0.05, 0.95)))

sim_df = data.frame(weight_lbs = seq(q5_95[1], q5_95[2], 1),
                    pick = mean(fdata$train_df$pick),
                    pos = &amp;quot;RB&amp;quot;
                    )
sim_df$predicted_pts = predict(train_fit, sim_df)
print(diff(sim_df$predicted_pts)[1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## [1] 1.313628&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As the weight of an RB increases by one pound, we expect the total number of fantasy points scored in a season to increase by ~1.3. Thus if you had two RBs drafted around the same position, but one weighed 30lbs more than the other, you would expect 39 more points from the heavier RB. Over a 16 game season, that difference translates to 2.4 more points per game, which could be the difference between making or missing the playoffs.&lt;/p&gt;
&lt;p&gt;90 percent of weight values fall between 180 and 230, so I’d feel comfortable generalizing these findings to all RBs within this range. The data is pretty sparse outside of this range, so the relationships in the model would likely break down Indeed, I’m fairly certain a 320lb RB would not score more points than a 220lb RB; there is some point where weight likely has a negative effect on performance, but we won’t see that in the data because a 320lb RB won’t ever see the field.&lt;/p&gt;
&lt;p&gt;While these findings are interesting, it’s not clear how helpful the model is in terms of informing our draft strategy. To answer that question, we’ll determine how the model performs on the hold-out set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_sq_test = round(r_squared(fdata$test_df$total_pts, 
                            predict(train_fit, fdata$test_df)), 
                  1)
median_abs_err = round(mdn_abs_error(fdata$test_df$total_pts, 
                                        predict(train_fit, fdata$test_df)), 
                       2)
print(paste0(&amp;quot;TEST R-SQUARED: &amp;quot;, r_sq_test))
print(paste0(&amp;quot;TEST MAE: &amp;quot;, median_abs_err))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## [1] &amp;quot;TEST R-SQUARED: 0.3&amp;quot;
## [1] &amp;quot;TEST MAE: 31.7&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can explain approximately 30 percent of the variance in the test set, and the median absolute error is around 30. The fact that performance on the test set is comparable with the validation set indicates good generalizability of our model. However, there is a lot of variance left to explain! If we wanted to improve these predictions, there are a number of other variables to consider, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Injury&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Strength of Offensive Line&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Presence of skilled existing running back&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ratio of pass to run plays in the prior season&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These inputs have an impact on how many points a fantasy RB will score. But if we want to keep it simple, the key takeaway is this: When it’s fantasy draft time and you’re debating between a few rookie RBs drafted around the same spot, go with the big guy. It just might be the difference between fantasy glory or facing the receiving end of one of these…&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;combine_analysis_image/brick_tamlin.png&#34; width=&#34;500px&#34; height=&#34;500px&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Two Flavors of Parallel Simulation</title>
      <link>/post/two_flavors_of_parallel_simulation.html</link>
      <pubDate>Fri, 01 Sep 2017 21:13:14 -0500</pubDate>
      
      <guid>/post/two_flavors_of_parallel_simulation.html</guid>
      <description>&lt;p&gt;&lt;img src=&#34;images/two_flavors.jpg&#34; width=&#34;800px&#34; height=&#34;800px&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;In a prior &lt;a href=&#34;https://thecodeforest.github.io/post/monte_carlo_power_mixed_effects&#34;&gt;post&lt;/a&gt;, we discussed how to use Monte Carlo simulation for power analysis. We kept the total number of iterations relatively low to illustrate the process. However, the real value of simulation emerges with many iterations, because the more iterations you run the better idea you get about the thing you are trying to estimate (see &lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_large_numbers&#34;&gt;Law of Large Numbers&lt;/a&gt;). In the case of estimating the power of an experiment, the more simulated experiments we run the closer we’ll get to the true probability of committing a &lt;em&gt;Type II Error&lt;/em&gt;. Simulating the experimental paradigm sequentially is fine but it takes a long time when you increase the number of simulations to 10K or 100K. Any time you come across a task that involves repeated sampling from a distribution – &lt;strong&gt;think parallel&lt;/strong&gt;. The results of one simulation do not feed into or depend on the results of another. Thus we can run many simulated experiments at the same time. This is a common theme of any task that is parallelizable, which might be one of the most challenging words to say. In this post, I’m going to discuss two separate ways to implement a power analysis simulation in R. And although we’ll focus only on parallelism (yes, apparently parallelism &lt;em&gt;is&lt;/em&gt; a word) in the context of experimental power, the workflow discussed here can be generalized to almost any task that involves repeated sampling.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parallel-simulations-with-foreach&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parallel Simulations with Foreach&lt;/h3&gt;
&lt;p&gt;Before starting let’s get a high-level understanding of the analytical dataset. Researchers conducted a study examining the impact of continued sleep deprivation (defined as receiving only three hours of sleep per night) on reaction time. The study ran for nine days and the researchers found a significant effect for Number of Days. As you can imagine, participants were a lot slower to react on days eight and nine relative to days zero and one. We want to replicate this effect but don’t have the time to wait nine days for a result. Our question, then, is whether we could still detect an effect of sleep deprivation after only three days. The goal is to achieve at least 80% power, which means that if we replicated the experiment 10 times under the exact same conditions, we would find a significant effect (p &amp;lt; 0.05) in at least eight experiments.&lt;/p&gt;
&lt;p&gt;We’ll use the findings from the prior study over the first three days as our base data set. The process will be modeled with a mixed effects model with a random intercept for each participant. Our fixed effect – the thing we are interested in – is days of sleep deprivation. Let’s load up our libraries and fit the initial model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs = c(&amp;#39;foreach&amp;#39;, &amp;#39;doParallel&amp;#39;, &amp;#39;lme4&amp;#39;, 
         &amp;#39;dplyr&amp;#39;, &amp;#39;broom&amp;#39;, &amp;#39;ggplot2&amp;#39;, &amp;#39;knitr&amp;#39;, &amp;#39;janitor&amp;#39;)
lapply(libs, require, character.only = TRUE)
sleep_df = lme4::sleepstudy %&amp;gt;% 
           clean_names() %&amp;gt;% 
           filter(days &amp;lt;= 3)

fit = lmer(reaction ~ days + (1|subject), data = sleep_df)
confidence_intervals = confint(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s examine the estimate and confidence intervals for the influence days on reaction time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(summary(fit))
print(confidence_intervals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: Reaction ~ Days + (1 | Subject)
##    Data: sleep_df
## 
## REML criterion at convergence: 660.4
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -3.14771 -0.50969 -0.08642  0.48985  2.05082 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  Subject  (Intercept) 755.7    27.49   
##  Residual             379.1    19.47   
## Number of obs: 72, groups:  Subject, 18
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  255.392      7.532   33.91
## Days           7.989      2.052    3.89
## 
## Correlation of Fixed Effects:
##      (Intr)
## Days -0.409

##                  2.5 %    97.5 %
## .sig01       18.702382  39.73719
## .sigma       16.152095  23.58800
## (Intercept) 240.429427 270.35528
## Days          3.931803  12.04555&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our model indicates that after accounting for baseline differences in participant reaction time (i.e., our random intercept), each additional day increases reaction time by about 8 seconds (7.989 to be exact). Our confidence interval for this coefficient indicates a significant effect, as the range does not contain zero. However, the range of our estimate is fairly wide. Let’s determine how this uncertainty affects overall experimental power. We’ll make predictions on our base dataset with the model defined above, and then add noise (defined by our residuals from our initial model fit) to simulate the sampling process.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_predictions = predict(fit, sleep_df)
standard_deviation = sd(fit@resp$y - fit@resp$mu)
n_simulations = 1000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;for&lt;/code&gt; loops in R are great for small operations but can be slow for larger operations. Enter &lt;code&gt;foreach&lt;/code&gt;. The syntax is a little bit different from your typical &lt;code&gt;for&lt;/code&gt; loop. Let’s first see how to implement our power simulation sequentially using &lt;code&gt;foreach&lt;/code&gt;. Note that this approach is identical to using a regular &lt;code&gt;for&lt;/code&gt; loop.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq_start_time = Sys.time()
seq_results = foreach(
                  i = 1:n_simulations,
                  .combine = &amp;quot;rbind&amp;quot;,
                  .packages = c(&amp;quot;lme4&amp;quot;, &amp;quot;broom&amp;quot;, &amp;quot;dplyr&amp;quot;)) %do% {
                  #generate residuals
                  temporary_residuals = rnorm(nrow(sleep_df), 
                                              mean = 0, 
                                              sd = standard_deviation)
                  #create simulated reaction time
                  sleep_df$simulated_reaction = model_predictions + temporary_residuals
                  #refit our model on the simulated data
                  temp_fit = lmer(simulated_reaction ~ days + (1|subject), 
                                  data = sleep_df)
                  #return confidence interval for the Days coefficient
                  tidy(confint(temp_fit)) %&amp;gt;%
                  dplyr::rename(coefficients = .rownames,
                  lower_bound = X2.5..,
                  upper_bound = X97.5..) %&amp;gt;%
                  dplyr::filter(coefficients == &amp;#39;days&amp;#39;) %&amp;gt;%
                  dplyr::select(lower_bound, upper_bound)
}

seq_end_time = Sys.time()
seq_run_time = seq_end_time - seq_start_time
print(paste0(&amp;quot;TOTAL RUN TIME: &amp;quot;, seq_run_time))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## [1] &amp;quot;TOTAL RUN TIME: 10.4966700077057&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Implementing the power simulation sequentially took about 10 minutes on my computer. Let’s compare that to a parallel implementation. All we have to do is change the &lt;code&gt;%do%&lt;/code&gt; to &lt;code&gt;%dopar%&lt;/code&gt; to shift the execution from sequential to parallel. But first, we’ll have to set up a computing cluster.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# register our cluster
cl = makeCluster(detectCores())
registerDoParallel(cl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we’ve registered our computing cluster, let’s re-run the above code block but replace &lt;code&gt;%do%&lt;/code&gt; with &lt;code&gt;%dopar%&lt;/code&gt; and compare the run-time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;para_start_time = Sys.time()
para_results = foreach(
                 i = 1:n_simulations,
                 .combine = &amp;quot;rbind&amp;quot;,
                 .packages = c(&amp;quot;lme4&amp;quot;, &amp;quot;broom&amp;quot;, &amp;quot;dplyr&amp;quot;)) %dopar% {
                 # generate residuals
                 temporary_residuals = rnorm(nrow(sleep_df),
                                             mean = 0,
                                             sd = standard_deviation)
                 #create simulated reaction time
                 sleep_df$simulated_reaction = model_predictions + temporary_residuals
                 #refit our model on the simulated data
                 temp_fit = lmer(simulated_reaction ~ days + (1|subject), 
                                 data = sleep_df)
                 #return confidence interval for the Days coefficient
                 tidy(confint(temp_fit)) %&amp;gt;%
                 dplyr::rename(coefficients = .rownames,
                 lower_bound = X2.5..,
                 upper_bound = X97.5..) %&amp;gt;%
                 dplyr::filter(coefficients == &amp;#39;days&amp;#39;) %&amp;gt;%
                 dplyr::select(lower_bound, upper_bound)
}

para_end_time = Sys.time()
para_run_time = para_end_time - para_start_time
print(paste0(&amp;quot;TOTAL RUN TIME: &amp;quot;, para_run_time))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## [1] &amp;quot;TOTAL RUN TIME: 2.09167686700821&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So that only took 2.09 minutes, which a substantial reduction in runtime! Let’s check and see how our power calculations panned out. Every instance in which we find a zero in our confidence interval for the Days estimate is a type II error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power_results = para_results %&amp;gt;%
                mutate(row_index = 1:nrow(para_results)) %&amp;gt;%
                group_by(row_index) %&amp;gt;%
                do(result = dplyr::between(0, .$lower_bound, .$upper_bound)) %&amp;gt;%
                mutate(result = as.integer(unlist(result)))
print(paste0(&amp;quot;TOTAL POWER: &amp;quot;, (n_simulations - sum(power_results$result))/nrow(power_results) * 100, &amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## [1] &amp;quot;TOTAL POWER: 98.9%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we ran our experiment under these conditions, we’d detect an effect that we know exists in about 99 of every 100 experiments. So it turns out we can reliably detect an effect with only three days instead of running it for all nine, saving us time and money. Let’s move on to the second approach to parallelism with &lt;code&gt;spark&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parallel-simulations-with-spark&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parallel Simulations with Spark&lt;/h3&gt;
&lt;p&gt;Spark has recently become one the most popular processing engines for data science. It has several APIs for different programming languages, and R has recently been added to growing list of supported languages. The main benefit of Spark is that it allows for the rapid processing and modeling of data at scale (think lots of data). The main drawback, at the moment, is that it does not support all of the built-in R libraries. This is the primary reason why the section on &lt;code&gt;foreach&lt;/code&gt; loops was included, as you might want to speed up an operation that cannot be accessed in Spark.&lt;/p&gt;
&lt;p&gt;Indeed, in this section,we’ll use a seperate library, &lt;code&gt;nlme&lt;/code&gt;, for mixed-effect modeling. We’ll implement the same process described in the previous section, and compare its runtime with &lt;code&gt;foreach&lt;/code&gt;. Let’s get started by setting up a “local” spark instance, which means leveraging all of the compute power available on the machine that’s also running our R-script. The real power of Spark is realized in a computing cluster, where the data is spread to and processed by many computers simultaneously. However, we’re going to keep it simple and just run everything on a single machine.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sparklyr)
library(nlme)
## create spark context (sc)
sc = spark_connect(master = &amp;quot;local&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can verify that Spark is using all of available cores for computation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste0(&amp;quot;N Cores in use: &amp;quot;, sc$config$sparklyr.cores.local))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## [1] &amp;quot;N Cores in use: 8&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s refit the model above and get an estimate of our standard deviation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_nlme = nlme::lme(reaction ~ days, random = ~ 1|subject, sleep_df)
fit_pred = predict(fit_nlme, sleep_df)
standard_deviation = sd(sleep_df$reaction - unname(fit_pred))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we’ll make 1000 copies of our dataset and bind them together. We’ll also generate all the errors for each of the iterations as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_sim = 1000

sleep_df_sim = data.frame(sapply(sleep_df, 
                                  rep.int, 
                                  times = n_sim))
residual_i = c()
for(i in 1:n_sim){
  residual_i = c(residual_i, 
                          rnorm(nrow(sleep_df), 
                                mean = 0, 
                                sd = standard_deviation))
}

sleep_df_sim$iteration = rep(1:n_sim, 
                              each = nrow(sleep_df))
sleep_df_sim$sim_reaction = residual_i + rep(fit_pred, n_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, each study has 72 observations (18 participants with 4 data points each (days 0 - 3). We created 1000 replications of the study, so our total dataset size is now 72000 rows. Each 72 observation “group” is identified by the &lt;code&gt;iteration&lt;/code&gt; field. Thus each core should receive approximately 125 iterations with 72 observations per iteration, for a total of ~9000 observations per core (assuming you are also using eight cores). Rarely is data so perfectly balanced, but this should provide an intuition into what’s happening under the hood. Let’s first translate our &lt;code&gt;R DataFrame&lt;/code&gt; into a &lt;code&gt;Spark DataFrame&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sleep_tbl = copy_to(sc, 
                    sleep_df_sim, 
                    &amp;quot;sleep_df_sim&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below we’ll utilize the &lt;code&gt;spark_apply&lt;/code&gt; function to implement the power simulation. I frequently use this function in my day-to-day work, as it allows the user to define an arbitrary function and then apply it to the partitioned &lt;code&gt;Spark DataFrame&lt;/code&gt;. We are partitioning our DataFrame by the &lt;code&gt;iteration&lt;/code&gt; field. We’ll then pull the results back into R with the &lt;code&gt;collect&lt;/code&gt; verb and calculate the total number of confidence intervals that contain zero, which indicates a Type II Error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spark_start_time = Sys.time()
sim_results = spark_apply(sleep_tbl,
                   function(x) broom::tidy(
                               nlme::intervals(
                               nlme::lme(
                                 sim_reaction ~ days, 
                                         random = ~ 1|subject, x
                                        )
                                              )$fixed
                                           ),
                   names = c(&amp;#39;.rownames&amp;#39;, 
                             &amp;#39;lower&amp;#39;, 
                             &amp;#39;est.&amp;#39;, 
                             &amp;#39;upper&amp;#39;),
                   group_by = &amp;quot;iteration&amp;quot;
)

sim_ci = sim_results %&amp;gt;% 
         collect() %&amp;gt;% 
         data.frame() %&amp;gt;% 
         filter(.rownames == &amp;#39;days&amp;#39;) %&amp;gt;% 
         select(iteration, lower, upper) %&amp;gt;% 
         mutate(lower_bound = ifelse(lower &amp;lt; 0, 1, 0),
                upper_bound = ifelse(upper &amp;gt; 0, 1, 0)) %&amp;gt;% 
         mutate(type_ii_err = ifelse(lower_bound + upper_bound &amp;gt; 1, 1, 0))
spark_end_time = Sys.time()
spark_run_time = spark_end_time - spark_start_time
print(paste0(&amp;quot;TOTAL RUN TIME: &amp;quot;, spark_run_time))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## [1] &amp;quot;TOTAL RUN TIME: 22.3460450172424&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, Spark is considerably faster than &lt;code&gt;foreach&lt;/code&gt;, with a runtime of only 22.3 seconds! I retried the code above with the &lt;code&gt;nlme&lt;/code&gt; mixed effects model just to make sure that the drastic speed up wasn’t due to using a different library (it made almost no difference). Let’s check the power estimate as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste0(&amp;quot;TOTAL POWER: &amp;quot;, (nrow(sim_ci) - sum(sim_ci$type_ii_err))/nrow(sim_ci) * 100, &amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## [1] &amp;quot;TOTAL POWER: 98.9%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This simulation gave us an identical estimate relative to the &lt;code&gt;foreach&lt;/code&gt;.Taken together, the results from both of these simulations indicate that we wouldn’t need the full nine days to show an effect. When you are finished, remember to shut your cluster down with the following command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spark_disconnect(sc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I hope this post reveals how easy it is to add some parallelism to your R code. Time always seems to be in short supply when you are developing, and waiting around for an answer is a total momentum killer. Taking a bit more time up front to understand whether you can run your code in parallel – and avoiding sequential for loops – will save you a ton of time down the line.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Forecasting with Tom Brady</title>
      <link>/post/forecasting_with_tom_brady.html</link>
      <pubDate>Wed, 16 Aug 2017 21:13:14 -0500</pubDate>
      
      <guid>/post/forecasting_with_tom_brady.html</guid>
      <description>&lt;p&gt;&lt;img src=&#34;forecasting_with_tom_brady_images/hey_tom.jpg&#34; width=&#34;800px&#34; height=&#34;800px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Imagine it’s January 1st, 2015 and the New England Patriots made the playoffs yet again 😣. You run a Tom Brady Super Fan website and want to ensure you have enough servers to accommodate traffic to your website during the playoffs. Historically, site traffic during January and February increases when the Patriots win playoff games, so you want a forecast for these months to determine how many people will visit your site.&lt;/p&gt;
&lt;p&gt;You also want to quantify the effect of the number of playoff games won on monthly traffic. For example, what happens if the Patriots win two playoff games instead of one? Finally, you want an estimate of the probability of each of these scenarios unfolding–that is, the chances the Patriot’s winning zero, one, two, or all three playoff games. To address each of these questions, you’ll need the following sources of data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Month level internet traffic&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Historical game outcomes&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Historical betting lines&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll go through the process of collecting each data source and then we’ll generate some forecasts!&lt;/p&gt;
&lt;div id=&#34;collecting-month-level-traffic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Collecting Month Level Traffic&lt;/h3&gt;
&lt;p&gt;Let’s start by loading the required libraries and collecting historical page views. The page-view data comes from Wikipedia, which we assume emulates traffic volume on our website. We’ll first pull the data into R via the &lt;code&gt;wp_trend&lt;/code&gt; function, and then aggregate daily page views up to the monthly level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs = c(&amp;#39;wikipediatrend&amp;#39;, &amp;#39;dplyr&amp;#39;, &amp;#39;data.table&amp;#39;, 
         &amp;#39;rvest&amp;#39;, &amp;#39;forecast&amp;#39;, &amp;#39;lubridate&amp;#39;,
         &amp;#39;janitor&amp;#39;,&amp;#39;knitr&amp;#39;, &amp;#39;ggplot2&amp;#39;, 
         &amp;#39;forcats&amp;#39;, &amp;#39;lazyeval&amp;#39;, &amp;#39;readr&amp;#39;,
         &amp;#39;emo&amp;#39;
         )
lapply(libs, require, character.only = TRUE)
wiki_query = &amp;quot;Tom Brady&amp;quot;
start_date = &amp;quot;2006-09-01&amp;quot;
end_date = &amp;quot;2015-03-01&amp;quot;
working_directory = &amp;quot;your_working_directory&amp;quot;
setwd(working_directory)
page_views = wp_trend(wiki_query, 
                      from = start_date,
                      to = end_date) %&amp;gt;% 
             mutate(date = as.Date(date)) %&amp;gt;% 
             mutate(year = year(date),
                    month = month(date)) %&amp;gt;% 
             group_by(year, month) %&amp;gt;% 
             mutate(max_month_date = max(date),
                    page_views = sum(page_views)) %&amp;gt;%
             select(year, month, 
                    max_month_date, page_views) %&amp;gt;% 
             distinct() %&amp;gt;% 
             filter(year &amp;gt; 2007 &amp;amp; max_month_date &amp;lt; as.Date(&amp;quot;2015-03-01&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s have a look at the first few rows.&lt;/p&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:400px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
year
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
month
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
max_month_date
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
page_views
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008-01-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
452183
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008-02-29
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
414347
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008-03-31
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
74711
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008-04-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
83526
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008-05-31
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
74857
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008-06-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54377
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008-07-12
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20042
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008-08-31
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
83623
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008-09-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
201761
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008-10-31
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
105190
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008-11-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
95206
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008-12-31
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
127058
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2009
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2009-01-31
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
143411
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2009
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2009-02-28
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
116640
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2009
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2009-03-31
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
113011
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Looks good! One thing to note is that the &lt;code&gt;max_month_date&lt;/code&gt; field is simply the last day for that year-month combination. This field will be used for plotting. Next, we’ll pull down some actual game data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;collecting-historical-game-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Collecting Historical Game Data&lt;/h3&gt;
&lt;p&gt;We are interested in how the New England Patriots have historically performed during the playoffs. To obtain this information, we’ll switch over to Python to scrape the outcomes (win or lose) of the Patriots vs. each of the other 31 NFL teams. The &lt;code&gt;collect_game_data.py&lt;/code&gt; is the script we’ll execute. Feel free to configure the scripts and directories however you like, but I’ve located everything - data and scripts - in the &lt;code&gt;working_directory&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from urllib import urlopen
import re
import pandas as pd
from bs4 import BeautifulSoup
import sys
import os.path
base_url = &amp;#39;http://www.footballdb.com/teams/nfl/new-england-patriots/teamvsteam?opp=&amp;#39;
game_data = []
n_teams = 32
output_location = os.path.join(sys.argv[1], sys.argv[2])
for team_number in range(1, n_teams + 1, 1):
    page  = str(BeautifulSoup(urlopen(base_url + str(team_number)), 
                              &amp;#39;html.parser&amp;#39;).findAll(&amp;quot;table&amp;quot;))
    for row in [x.split(&amp;quot;&amp;lt;td&amp;gt;&amp;quot;) for x in page.split(&amp;quot;row&amp;quot;)]:
        try:
            game_date, outcome = str(re.findall(&amp;#39;gid=(.*)&amp;#39;, row[4])).split(&amp;quot;&amp;gt;&amp;quot;)[:2]
            game_data.append([game_date[2:10], outcome[0]])
        except:
            continue
game_data_df = pd.DataFrame(game_data)
game_data_df.columns = [&amp;#39;date&amp;#39;, &amp;#39;outcome&amp;#39;]
game_data_df.to_csv(output_location,  index = False)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below we’ll call the &lt;code&gt;exe_py_script&lt;/code&gt; function to collect the game data, and then write the result to a .csv file. Here is how we’ll execute it from R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exe_py_script = function(py_bin_location, py_script_path, py_script_name, py_args){
  exe_command = paste(py_bin_location,
                      file.path(py_script_path, py_script_name),
                      paste(py_args, collapse = &amp;quot; &amp;quot;),
                      sep = &amp;quot; &amp;quot;)
  system(exe_command)
}

py_bin_location =  &amp;quot;//anaconda/bin/python&amp;quot;
py_script_path = working_directory
output_file_name = &amp;quot;game_data.csv&amp;quot;
py_script_name = &amp;quot;collect_game_data.py&amp;quot;
py_args = c(working_directory, output_file_name)
exe_py_script(py_bin_location, py_script_path, py_script_name, py_args)
game_data = read_csv(output_file_name)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you aren’t familiar with executing scripts in other languages from R (or the terminal), we can break this command down into further detail. There are four arguments passed to the function executing the python script:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;py_bin_location: The location of the Python binaries on your machine ‘//anaconda/bin/python’&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;py_script_path: The location of the Python script ‘working_directory’&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;py_script_name: The name of the Python script ‘collect_game_data.py’&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;py_args: Additional arguments passed into the Python script ‘the write-location of .csv’&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s do the scraping and see what we get back. Recall that we are executing all of the commands from within R.&lt;/p&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:400px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
date
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
outcome
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20160911
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
W
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20120916
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
L
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20081221
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
W
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20040919
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
W
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
19991031
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
W
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
19960915
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
W
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
19931010
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
W
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
19910929
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
L
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
19901125
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
L
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
19841202
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
L
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
19811129
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
L
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
19780910
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
W
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20171022
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
W
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20170205
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
W
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20130929
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
W
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The data contains the date and outcome (Win or Lose) for every game the Patriots have played. We’ll generate two features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A sum of the playoff games played in Jan/Feb&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A sum of the playoff games won in Jan/Feb&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It’s not clear whether simply playing in a playoff game or winning a game drives traffic, so both features will be created. We’ll then test to determine which corresponds more closely to web traffic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;playoff_data = game_data %&amp;gt;% 
               mutate(date = as.Date(as.character(date), format = &amp;#39;%Y%m%d&amp;#39;),
                      outcome = ifelse(outcome == &amp;quot;W&amp;quot;, 1, 0)) %&amp;gt;% 
               mutate(year = year(date),
                      month = month(date, label = TRUE),
                      week = week(date)) %&amp;gt;% 
               mutate(playoff_game = ifelse(month %in% c(&amp;quot;Jan&amp;quot;, &amp;quot;Feb&amp;quot;) &amp;amp; week != 1, 
                               1, 
                               0)) %&amp;gt;% 
               mutate(playoff_game_win = ifelse(outcome == 1 &amp;amp; playoff_game == 1, 
                                   1, 
                                   0)) %&amp;gt;% 
               group_by(year) %&amp;gt;% 
               summarise(playoff_games_won = sum(playoff_game_win),
                         playoff_games_played = sum(playoff_game)) %&amp;gt;% 
               filter(year &amp;lt;= 2015 &amp;amp; year &amp;gt;= 2008)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again let’s see what the last few rows of the data look like:&lt;/p&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:300px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
year
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
playoff_games_won
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
playoff_games_played
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2008
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2009
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2010
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2011
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2012
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2014
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have our playoff features built, and now it’s time to merge them with the monthly page views.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;page_views = inner_join(page_views, 
                        playoff_data
                        ) %&amp;gt;% 
             mutate(part = ifelse(year == 2015, 
                                  &amp;quot;test&amp;quot;, 
                                  &amp;quot;train&amp;quot;),
                   playoff_games_won = ifelse(month %in% c(1, 2), 
                                              playoff_games_won, 
                                              0),
                   playoff_games_played = ifelse(month %in% c(1, 2), 
                                                 playoff_games_played, 
                                                 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our analytical dataset is ready for forecasting. We’ll start, like most data-related activities, with a high-level visualization of the relationship between playoff wins and web traffic.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-wins-vs.traffic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualizing Wins vs. Traffic&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_plot_theme = function(){
    font_family = &amp;quot;Helvetica&amp;quot;
    font_face = &amp;quot;bold&amp;quot;
    return(theme(
    axis.text.x = element_text(size = 18, face = font_face, family = font_family),
    axis.text.y = element_text(size = 18, face = font_face, family = font_family),
    axis.title.x = element_text(size = 20, face = font_face, family = font_family),
    axis.title.y = element_text(size = 20, face = font_face, family = font_family),
    strip.text.y = element_text(size = 18, face = font_face, family = font_family),
    plot.title = element_text(size = 18, face = font_face, family = font_family),
    legend.position = &amp;quot;top&amp;quot;,
    legend.title = element_text(size = 16,
    face = font_face,
    family = font_family),
    legend.text = element_text(size = 14,
    face = font_face,
    family = font_family)
))
}
color_values = c(&amp;quot;#272822&amp;quot;, &amp;quot;#66D9EF&amp;quot;,&amp;quot;#F92672&amp;quot;,&amp;quot;#A6E22E&amp;quot;, &amp;quot;#A6E22E&amp;quot;, &amp;quot;#F92672&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;page_views %&amp;gt;% 
  filter(part == &amp;#39;train&amp;#39;) %&amp;gt;% 
  mutate(playoff_games_won = as.factor(playoff_games_won)) %&amp;gt;% 
  ggplot(aes(x  = max_month_date, y = page_views)) + 
  geom_point(aes(color = playoff_games_won), size = 4) + 
  geom_line() + 
  theme_bw() + 
  scale_color_manual(values = color_values[1:3],
                     guide = guide_legend(title = &amp;quot;Playoff Games Won&amp;quot;)) +
  my_plot_theme() + 
  xlab(&amp;quot;Date&amp;quot;) + 
  ylab(&amp;quot;Page Views&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/forecasting_with_tom_brady_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above plot suggests that playoff wins do relate to page views. Now we’ll do some validation to see if including this information as an external regressor improves our forecasts.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-selection-and-validation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model Selection and Validation&lt;/h3&gt;
&lt;p&gt;Since the forecasting period of interest is Jan/Feb 2015, we’ll hold out two months of traffic volume from Jan/Feb 2014 as a way to identify which inputs will likely yield the most accurate forecasts. An ARIMA model with a single external regressor (games won or games played) will be used to generate the forecasts. The accuracy between the two models with external regressors (games played and games won) will be compared against a model that relies only on history (i.e., no external regressors).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;val_df = page_views %&amp;gt;% 
  filter(part == &amp;#39;train&amp;#39;) %&amp;gt;% 
  mutate(part = ifelse(year == 2014 &amp;amp; month %in% c(1, 2),
                       &amp;quot;validation&amp;quot;, &amp;quot;train&amp;quot;
                       )) %&amp;gt;% 
  filter(part == &amp;#39;validation&amp;#39;)
train_df = page_views %&amp;gt;% 
           filter(max_month_date &amp;lt; min(val_df$max_month_date))
# create our time-series object 
page_views_ts = ts(train_df$page_views,
                   frequency = 12,
                   start = c(head(train_df, 1)$year, 
                             head(train_df, 1)$month),
                   end = c(tail(train_df, 1)$year, 
                           tail(train_df, 1)$month)
                   )
# specify forecast horizon
horizon = 2
# arima model with no external regressors
f_no_xreg = forecast(auto.arima(page_views_ts), 
                     h = horizon)$mean
#  with playoff games played
f_playoff_played = forecast(auto.arima(page_views_ts,
                                       xreg = train_df$playoff_games_played),
                            h = horizon, xreg = val_df$playoff_games_played)$mean
#  with playoff games won                                
f_playoff_won = forecast(auto.arima(page_views_ts,
                                    xreg = train_df$playoff_games_won),
                         h = horizon, xreg = val_df$playoff_games_won)$mean

accuracy_df = data.frame(model = c(rep(&amp;quot;No Xreg&amp;quot;, horizon),
                                   rep(&amp;quot;Games Played&amp;quot;, horizon),
                                   rep(&amp;quot;Games Won&amp;quot;, horizon)),
                         forecasted_views = c(f_no_xreg,
                                              f_playoff_played,
                                              f_playoff_won
                                              ),
                         actual_views = rep(val_df$page_views, 3)
                        )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are a number of ways to measure error in forecasting. In this case, we’ll use the Mean Average Percent Error (MAPE), which is calculated as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;forecasting_with_tom_brady_images/mape_equation.png&#34; width=&#34;800px&#34; height=&#34;800px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;e&lt;sub&gt;t&lt;/sub&gt; is the difference between the predicted and actual and y&lt;sub&gt;t&lt;/sub&gt; is the actual value. As with all error metrics, there are pros and cons to quantifying error with MAPE. The main advantage is ease of interpretation. Telling someone “our forecasts were off by 50%” is easier than saying “our forecasts were off by 10,458 units”. The main disadvantage is that the scale of the error matters. For example, a 10% MAPE on 10 units (1) is a lot smaller than a 10% MAPE on 100,000 units (10K), yet they are treated the same. Additionally, having a small value in the denominator can make a forecast look much worse than it actually is. Thus, if we were forecasting small quantities, a different error metric would be better suited.&lt;/p&gt;
&lt;p&gt;With that in mind, let’s determine how our three approaches performed on the validation set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_mape = function(predicted_amt, actual_amt){
  return(round(mean(abs(predicted_amt - actual_amt)/actual_amt) * 100, 1))
}

accuracy_df %&amp;gt;% 
  group_by(model) %&amp;gt;% 
  do(mape = calc_mape(.$forecasted_views,
                     .$actual_views
                     )) %&amp;gt;% 
  mutate(mape = unlist(mape),
         model = factor(model)) %&amp;gt;% 
  mutate(model = fct_reorder(model, mape, .desc = FALSE)) %&amp;gt;% 
  ggplot(aes(x = model, y = round(mape, 0),
             fill = model, label = as.character(round(mape, 0)))) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) + 
  theme_bw() + 
  my_plot_theme() + 
  scale_fill_manual(values = color_values[1:length(unique(accuracy_df$model))]) + 
  xlab(&amp;quot;Forecasting Inputs&amp;quot;) + ylab(&amp;quot;MAPE&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;) + 
  geom_label(label.size = 1, size = 10, color = &amp;quot;white&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/forecasting_with_tom_brady_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model using Games Won as an external regressor performed the best with a less than stellar 114 percent MAPE. We could reformulate our external regressor, try a different forecasting approach, or bring in additional covariates to improve our MAPE, but we’ll keep it simple and just consider only the approaches outlined above.&lt;/p&gt;
&lt;p&gt;We figured out which approach works best, and we have all of the data we need to make a traffic forecast. There’s only one problem: We dont’t know how many games the Patriots will win during the playoffs. Thus, we’ll need to generate a prediction – zero, one, two, or three – for the expected number of games won, which in turn will serve as an input into the final model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;collecting-betting-lines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Collecting Betting Lines&lt;/h3&gt;
&lt;p&gt;To help us make an informed decision about the number of games the Patriots will win during the playoffs, we can leverage historical NFL betting lines. If you aren’t familiar with the concept of a betting line, it’s a way for odds-makers to encourage an equal number bets for both teams playing in a game.&lt;/p&gt;
&lt;p&gt;We’ll again leverage &lt;code&gt;BeautifulSoup&lt;/code&gt; and call the &lt;code&gt;collect_betting_line_data.py&lt;/code&gt; script from R.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import urllib2
from bs4 import BeautifulSoup
import re
import pandas as pd
import sys
import os.path
base_url = &amp;quot;https://www.teamrankings.com/nfl/odds-history/results/&amp;quot;
output_location = os.path.join(sys.argv[1], sys.argv[2])
opener = urllib2.build_opener()
opener.addheaders = [(&amp;#39;User-Agent&amp;#39;, &amp;#39;Mozilla/5.0&amp;#39;)]
page = BeautifulSoup(opener.open(base_url), &amp;#39;html.parser&amp;#39;)
table_data = page.find_all(&amp;quot;tr&amp;quot;, {&amp;quot;class&amp;quot;: &amp;quot;text-right nowrap&amp;quot;})
betting_lines = []
for line in table_data:
    line_list = str(line).splitlines()
    try:
        betting_lines.append([re.search(&amp;#39;&amp;lt;td&amp;gt;(.*)&amp;lt;/td&amp;gt;&amp;#39;, line_list[1]).group(1),
                              line_list[4].split(&amp;quot;&amp;gt;&amp;quot;)[1].split(&amp;quot;&amp;lt;&amp;quot;)[0]])
    except:
        betting_lines.append([None, None])

historic_lines_df = pd.DataFrame(betting_lines)
historic_lines_df.columns = [&amp;#39;spread&amp;#39;, &amp;#39;win_pct&amp;#39;]
historic_lines_df.to_csv(output_location, index = False)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;output_file_name = &amp;quot;historic_betting_lines.csv&amp;quot;
py_script_name = &amp;quot;collect_betting_line_data.py&amp;quot;
py_args = c(working_directory, output_file_name)
exe_py_script(py_bin_location, py_script_path, py_script_name, py_args)
betting_lines = read_csv(output_file_name)&lt;/code&gt;&lt;/pre&gt;
Let’s examine the betting lines data:
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:600px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
spread
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
win_pct
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-26.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-24.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-22.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-20.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-19.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-19.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-17.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-17.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-16.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-16.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-15.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-15.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-14.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
91.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-14.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
90.2%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-13.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
84.3%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-13.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
85.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-12.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
76.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-12.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
81.2%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-11.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
82.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-11.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
93.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-10.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
82.3%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-10.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
81.1%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-9.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
81.2%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-9.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
73.6%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-8.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
83.3%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-8.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
67.4%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-7.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
76.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-7.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
74.1%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-6.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
69.4%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-6.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
69.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-5.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
68.3%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-5.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
66.2%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-4.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
68.1%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-4.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
69.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-3.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
65.1%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-3.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
55.5%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-2.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
50.4%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-2.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.2%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-1.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.3%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-1.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
56.2%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
50.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The interpretation is really simple: A team favored by 12 points (-12) has historically won ~81.2% of their games; bump that spread up to 17 points and there has never been a team favored by 17 points that lost. Let’s see what that looks like starting at a zero-point spread when both teams are perceived by odds-makers to be an equal match.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;betting_lines %&amp;gt;% 
  filter(spread &amp;lt;= 0) %&amp;gt;% 
  mutate(win_pct = substring(as.character(win_pct), 1, 
                                    (nchar(as.character(win_pct)) - 1))) %&amp;gt;% 
  mutate(win_pct = as.numeric(win_pct),
                spread = abs(spread)) %&amp;gt;% 
  rename(favorite = spread) %&amp;gt;% 
  ggplot(aes(x = favorite, y = win_pct)) + 
  geom_point(alpha = 0) + 
  geom_line(alpha = 0) + 
  stat_smooth(span = 2.0, se = FALSE, size = 2, colour = color_values[1]) + 
  ylim(50, 110) + 
  xlim(0, 27) + 
  scale_x_continuous(breaks = seq(0, 25, 5)) + 
  scale_y_continuous(breaks = seq(50, 110, 5)) + 
  theme_bw() + 
  my_plot_theme() + 
  xlab(&amp;quot;Point Favorite&amp;quot;) + ylab(&amp;quot;Chance of Winning&amp;quot;) + 
  geom_vline(xintercept = 7, size = 2, colour = color_values[2]) + 
  geom_vline(xintercept = 5, size = 2, colour = color_values[3]) + 
  geom_vline(xintercept = 3, size = 2, colour = color_values[4]) + 
  annotate(&amp;quot;rect&amp;quot;, xmin = 18, xmax = 19, ymin = 88, ymax = 90, fill = color_values[2]) + 
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Game 1 Spread&amp;quot;, x = 23, y = 89, size = 8, color = color_values[2]) + 
  annotate(&amp;quot;rect&amp;quot;, xmin = 18, xmax = 19, ymin = 85, ymax = 87, fill = color_values[3]) + 
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Game 2 Spread&amp;quot;, x = 23, y = 86, size = 8, color = color_values[3]) + 
  annotate(&amp;quot;rect&amp;quot;, xmin = 18, xmax = 19, ymin = 82, ymax = 84, fill = color_values[4]) + 
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Game 3 Spread&amp;quot;, x = 23, y = 83, size = 8, color = color_values[4])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/forecasting_with_tom_brady_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We only know the spread for Game 1 because we are generating our forecasts at the beginning of January. The Patriots are favored by seven points, and historically teams favored by this amount win ~73% of games. So I’m feeling at least one win. What about two? Here we are going to make an educated guess. We can assume that each subsequent game will be more challenging for the Patriots, so we’ll make a prediction of a five-point favorite. Finally, if the Patriots play in the Superbowl, let’s predict they’ll be a three-point favorite. If we assume that the outcome of each playoff game is independent of the prior game (which, barring a major injury to a key player, is a reasonable assumption), we can calculate the probability of each of these scenarios unfolding:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;forecasting_with_tom_brady_images/win_prob.png&#34; width=&#34;800px&#34; height=&#34;800px&#34; /&gt; There is about a 50% chance the Patriots will win two playoff games, so let’s pick two as our number. Before proceeding to the end result, I’ll briefly discuss how the forecasts are being generated.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seasonal-arimax-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Seasonal ARIMAX Model&lt;/h3&gt;
&lt;p&gt;Let’s train our final model and examine the coefficients.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;input_ts = ts(page_views %&amp;gt;% 
              filter(part == &amp;#39;train&amp;#39;) %&amp;gt;% 
              pull(page_views),
              frequency = 12
              )
xreg_train = page_views %&amp;gt;% 
             filter(part == &amp;#39;train&amp;#39;) %&amp;gt;% 
             pull(playoff_games_won)
model_fit = auto.arima(input_ts, xreg = xreg_train)
print(summary(model_fit))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Series: input_ts 
## Regression with ARIMA(1,1,1)(0,1,1)[12] errors 
## 
## Coefficients:
##          ar1      ma1     sma1  xreg_train
##       0.2673  -0.8559  -0.7373   296550.82
## s.e.  0.1458   0.0723   0.2563    36020.02
## 
## sigma^2 estimated as 1.28e+10:  log likelihood=-930.05
## AIC=1870.1   AICc=1871.02   BIC=1881.41
## 
## Training set error measures:
##                     ME     RMSE      MAE       MPE     MAPE      MASE        ACF1
## Training set -22993.67 101058.3 61230.45 -26.36236 38.57438 0.6260494 -0.01938565&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The great thing about the &lt;code&gt;auto.arima&lt;/code&gt; function is that it does the hard work of identifying the best model specification from our training data. While this is a huge time-saver, it helps to understand how and why certain parameters were selected. If you happen to be a forecasting expert and just want to know how to implement the model, feel free to skip this next section.&lt;/p&gt;
&lt;p&gt;Our model is &lt;code&gt;ARIMA(1,1,1)(0,1,1)[12]&lt;/code&gt;. Let’s first focus on the first part &lt;code&gt;ARIMA(1,1,1)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;ARIMA stands for Auto-Regressive Integrated Moving Average, which is why it is abbreviated. Identifying the Integrated part ARIMA(1, 1, 1) is the first thing we do. It says ‘this is how much you need to difference (Y~t - Y&lt;sub&gt;t-1&lt;/sub&gt;) our time series by to make it stationary on the mean. Cool story Hansel. Now in English. Stationary implies that the average (or variance, or any other parameter) of our time series remains constant across time. If we have an upward or downward trend, our mean is not stationary (it’s changing!), so the model captures how much it is going up or down by each time step, and then subtracts that from each value so it remains flat (or stationary). Indeed, the values of a stationary time series do not depend on where we are in the time series.&lt;/p&gt;
&lt;p&gt;Next the &lt;strong&gt;Auto-Regressive&lt;/strong&gt; or &lt;strong&gt;(AR(1))&lt;/strong&gt; part. Auto-regressive roughly translates to ‘regressed on itself’, and implies that we can learn something about Y&lt;sub&gt;t+1&lt;/sub&gt; from Y&lt;sub&gt;t&lt;/sub&gt;. Said differently, prior values (in this case from 1 prior time-step) are good indicators of subsequent future values, so we capture this with an auto-regressive term.&lt;/p&gt;
&lt;p&gt;Finally the &lt;strong&gt;Moving Average&lt;/strong&gt; or &lt;strong&gt;(MA(1))&lt;/strong&gt; part. Moving-average is like the AR part, in that we use prior values to inform our prediction of future values, but instead of focusing on the actual values we focus instead on prior errors, specifically our forecasting errors. These errors are computed as we fit the model, and like many things in life, we use our past errors to inform our future predictions.&lt;/p&gt;
&lt;p&gt;Now let’s discuss the second part: &lt;code&gt;(0,1,1)[12]&lt;/code&gt;. This is the seasonality portion of our model and can be interpreted similarly to the first part. The model is determining the difference in each month’s number of views across time, so Jan. 2015 - Jan. 2014 - Jan 2013…you get the point. That’s the integrated part. The model also calculates a historical moving average with exponential weighting. The amount of weighting (or smoothing) is determined by the &lt;code&gt;sma1&lt;/code&gt; coefficient contained in the above model. Coefficients that are closer to 1 indicate that more months (across history) are being used to determine how much future months will differ from the average of previous months.&lt;/p&gt;
&lt;p&gt;Finally the coefficient for our external regressor – number of post-season games won – has a value of 296550. This coefficient is interpreted just like a linear regression model; for each additional Patriot’s post-season win, we expect ~296K more visits to the website.&lt;/p&gt;
&lt;p&gt;If that all makes sense, let’s test our model on the final data, with our external variable set to two playoff games, and see how our prediction of Tom Brady’s page views compared to what actually happened. In essence, we are saying “The Patriots will make the Superbowl but will not win.” It turns out betting against the Patriots in the Superbowl can be a bad move, something I’ve experienced firsthand 😕.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;actual_views = page_views %&amp;gt;% 
  filter(part == &amp;#39;test&amp;#39;) %&amp;gt;% 
  pull(page_views)
# prediction for how many playoff games we think the Patriots will win
games_won = 2
test_xreg = c(games_won, games_won)
forecasted_views = forecast(model_fit,
                              h = horizon,
                              xreg = test_xreg)$mean
print(paste0(&amp;quot;MAPE IS: &amp;quot; , calc_mape(forecasted_views, actual_views), &amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## MAPE IS: 34.9%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our MAPE is ~35%, which is considerably better than the MAPE on our holdout set. However, our prediction of the Patriots only winning two games was wrong. The Patriots won three postseason games and beat the Seattle Seahawks 28-24 to win the Superbowl. So what would’ve happened if the value of our external regressor was correct (i.e., three instead of two)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;games_won = 3
test_xreg = c(games_won, games_won)
forecasted_views = forecast(model_fit,
                              h = horizon,
                              xreg = test_xreg)$mean
print(paste0(&amp;quot;MAPE IS: &amp;quot; , calc_mape(forecasted_views, actual_views), &amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## MAPE IS: 70.7%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hang on a second 😣. The model with the correct number of playoff games won (three) had lower accuracy? Yes, and here’s why: Across the history of our time series, the Patriots never won three playoff games. They had only won none, one or two games. Therefore, we are extrapolating to values not contained in our data set, which can be a recipe for disaster. If you look at the change in our forecast as we increase the number of playoff games won by one, we expect an additional 296K visitors. We are making the assumption that there is a linear relationship between wins and page views, such that each additional win generates +296K views. This is not the case, and the incorrect assumption is reflected in the accuracy of the resulting forecast.&lt;/p&gt;
&lt;p&gt;Hopefully, this post has eliminated some of the mystery around creating forecasts with external regressors. This is a common topic of confusion when starting to implement forecasts in R. However, it is no different than building a regular regression model. Happy forecasting!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Feature Selection for the Wine Connoisseur</title>
      <link>/post/feature_selection_wine.html</link>
      <pubDate>Mon, 14 Aug 2017 21:13:14 -0500</pubDate>
      
      <guid>/post/feature_selection_wine.html</guid>
      <description>&lt;p&gt;&lt;img src=&#34;feature_selection_images/grapes.jpg&#34; width=&#34;800px&#34; height=&#34;800px&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;There are few reasons for reducing the number of potential input features when building a model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Faster training times&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Easier interpretation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improved accuracy by reducing the chance of overfitting&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The focus here will primarly be on point three, as we address two questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Does having noisy, unnecessary variables as inputs reduce classification performance?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;And, if so, which algorithms are the most robust to the ill-effects of these unnecessary variables?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We’ll explore these questions with one my favorite datasets located &lt;a href=&#34;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/&#34;&gt;here&lt;/a&gt;. Based on the methodology outlined &lt;a href=&#34;http://projects.csail.mit.edu/wiki/pub/Evodesign/SensoryEvaluationsDatabase/winequality09.pdf&#34;&gt;here&lt;/a&gt;, a bunch of Wine experts got together and evaluated both White and Red wines. Each row represents the chemical properties of a wine in addition to a rating (0 = very bad, 10 = excellent). Each wine was rated by at least three people, and the median of their ratings was used as the final score. I have no idea how you get paid to drink wine, but somewhere in my life, I chose the wrong career path. Thus, the goal here is to predict the rating based on the chemical properties of the wine, such as its pH or Alcohol Concentration.&lt;/p&gt;
&lt;p&gt;Let’s load up the libraries we’ll need as well as a custom plotting function.&lt;/p&gt;
&lt;p&gt;Here we’ll read the wine quality ratings directly for the source. We’ll also convert the rating system from a 1-10 scale to binary for the sake of simplicity. Any wine with a rating greater than seven receives the highly coveted “IdDrinkThat”, while all others receive the dreaded “NoThanks” label.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;red_file = &amp;#39;winequality-red.csv&amp;#39;
white_file = &amp;#39;winequality-white.csv&amp;#39;
base_url = &amp;#39;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/&amp;#39;
red_wine = fread(paste0(base_url, red_file)) %&amp;gt;% 
           data.frame()
white_wine = fread(paste0(base_url, white_file)) %&amp;gt;% 
           data.frame()
wine_df = bind_rows(red_wine,
                    white_wine
                    ) %&amp;gt;% 
          clean_names() %&amp;gt;% 
          mutate(wine_class = ifelse(quality &amp;gt;= 7,
                                            &amp;quot;IdDrinkThat&amp;quot;,
                                            &amp;quot;NoThanks&amp;quot;)) %&amp;gt;%
          mutate(wine_class = as.factor(wine_class)) %&amp;gt;% 
          select(-quality)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As usual, let’s peak at the data to make sure there aren’t any unwanted surprises.&lt;/p&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:410px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
fixed_acidity
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
volatile_acidity
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
citric_acid
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
residual_sugar
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
chlorides
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
free_sulfur_dioxide
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
total_sulfur_dioxide
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
density
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
ph
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
sulphates
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
alcohol
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
wine_class
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.700
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.076
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
34
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9978
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.51
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.56
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.880
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.098
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
25
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
67
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9968
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.20
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.68
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.760
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.092
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9970
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.26
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.65
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
11.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.280
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.56
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.075
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
17
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9980
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.16
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.58
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.700
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.076
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
34
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9978
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.51
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.56
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.660
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.075
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
13
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9978
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.51
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.56
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.600
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.069
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
59
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9964
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.46
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.650
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.065
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9946
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.39
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
IdDrinkThat
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.580
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.073
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9968
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.36
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.57
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
IdDrinkThat
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.500
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.36
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.071
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
17
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
102
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9978
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.35
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.80
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.580
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.097
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
65
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9959
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.28
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.500
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.36
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.071
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
17
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
102
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9978
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.35
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.80
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5.6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.615
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.089
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
59
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9943
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.58
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.52
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.610
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.29
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.114
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
29
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9974
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.26
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.56
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.620
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.18
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.176
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
52
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
145
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9986
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.16
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.88
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.620
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.19
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.170
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
51
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
148
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9986
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.17
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.93
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.280
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.56
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.092
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
35
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
103
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9969
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.75
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
IdDrinkThat
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8.1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.560
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.28
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.368
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
56
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9968
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.11
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.28
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.590
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4.4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.086
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
29
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9974
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.38
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.50
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.320
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.51
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.341
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
17
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
56
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9969
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.220
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.48
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.077
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
29
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9968
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.39
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.53
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.390
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.31
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.082
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
71
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9982
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.52
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.65
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.430
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.21
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.106
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
37
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9966
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.17
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.91
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.490
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.11
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.084
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
67
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9968
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.17
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.53
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.400
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.14
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.085
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9968
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.43
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.63
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NoThanks
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;This dataset is exceptionally clean. Typically we would do some exploratory analysis to check for outliers, missing values, incorrect codings, or any other number of problems that can sabotage our predictions. In this case, we can go right into the variable selection.&lt;/p&gt;
&lt;p&gt;We’ll initially leverage the &lt;strong&gt;gradient boosted machine (GBM)&lt;/strong&gt; algorithm in the &lt;code&gt;h2o&lt;/code&gt; library for variable selection. One of the nice features of GBM is that it automatically tells you which variables are important. Recall that GBM is an iterative algorithm that fits many simple decision trees, where the errors from the prior tree become the dependent variable for each subsequent tree. A feature’s importance is determined by how much its introduction into a given tree reduces the error. The total reduction in error for a given feature is then averaged across all of the trees the feature appeared in. Thus splitting on important variables should lead to larger reductions in error during training relative to less important variables.&lt;/p&gt;
&lt;p&gt;With that in mind let’s do an initial pass and see which variables are important. We’ll do a 70⁄30 train/test split, start &lt;code&gt;h2o&lt;/code&gt;, and fit an initial model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trainTestSplit = function(df, split_proportion){
  set.seed(123)
  out_list = list()
  data_split = sample(1:2, 
                      size = nrow(df), 
                      prob = split_proportion, 
                      replace = TRUE)
  
  out_list$train = df[data_split == 1,]
  out_list$test = df[data_split == 2,]
  return(out_list)
}
split_proportion = c(0.7, 0.3)
df_split = trainTestSplit(wine_df, split_proportion)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we’ll calculate our variable importance metric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.init(nthreads = -1)
train_h2o = as.h2o(df_split$train)
test_h2o = as.h2o(df_split$test)
y_var = &amp;quot;wine_class&amp;quot;
x_var = setdiff(names(train_h2o), y_var)
gbm_fit = h2o.gbm(x = x_var, 
                  y = y_var,
                  distribution = &amp;quot;bernoulli&amp;quot;,
                  training_frame = train_h2o,
                  stopping_rounds = 3,
                  stopping_metric = &amp;quot;AUC&amp;quot;,
                  verbose = FALSE
                  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And let’s plot our results based on the relative importance of each feature.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  data.frame(gbm_fit@model$variable_importances) %&amp;gt;% 
    select(variable, relative_importance) %&amp;gt;% 
    mutate(relative_importance = round(relative_importance),
                variable = factor(variable)) %&amp;gt;% 
    mutate(variable = fct_reorder(variable, 
                                  relative_importance, .desc = TRUE)) %&amp;gt;% 
  ggplot(aes(x = variable, 
             y = relative_importance,
             fill = variable, 
             label = as.character(relative_importance))) + 
    geom_bar(stat = &amp;quot;identity&amp;quot;) + 
    geom_label(label.size = 1, size = 5, color = &amp;quot;white&amp;quot;)  + 
    theme_bw() + 
    my_plot_theme() + 
    theme(legend.position = &amp;quot;none&amp;quot;) + 
    ylab(&amp;quot;Relative Importance&amp;quot;) + 
    xlab(&amp;quot;Variable&amp;quot;) + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 12))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/feature_selection_wine_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like all of the variables have some degree of influence on classification, so we’ll continue with the full model. Next, we’re going to calculate AUC (Area Under Curve) for the following methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GBM&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random Forest (RF)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Neural Network (NN)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logistic Regression (LR)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We’ll use this as a point of comparison as additional irrelevant predictor variables are added. Our goal is to see how the addition of such variables affects AUC on the test set. Let’s calculate our baseline numbers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_fit = h2o.randomForest(x = x_var,
                          y = y_var,
                          training_frame = train_h2o,
                          stopping_rounds = 3,
                          stopping_metric = &amp;quot;AUC&amp;quot;
                          )

nn_fit = h2o.deeplearning(x = x_var,
                          y = y_var,
                          training_frame = train_h2o,
                          stopping_rounds = 3,
                          stopping_metric = &amp;quot;AUC&amp;quot;)

glm_fit = h2o.glm(x = x_var,
                   y = y_var,
                   family = &amp;quot;binomial&amp;quot;,
                   training_frame = train_h2o)

gbm_auc = h2o.auc(h2o.performance(gbm_fit, newdata = test_h2o))
rf_auc = h2o.auc(h2o.performance(rf_fit, newdata = test_h2o))
nn_auc = h2o.auc(h2o.performance(nn_fit, newdata = test_h2o))
glm_auc = h2o.auc(h2o.performance(glm_fit, newdata = test_h2o))

auc_df = data.frame(n_noise_vars = rep(0, 4),
                    method = c(&amp;#39;gbm&amp;#39;, &amp;#39;rf&amp;#39;, &amp;#39;nn&amp;#39;, &amp;#39;glm&amp;#39;),
                    AUC = c(gbm_auc, rf_auc, nn_auc, glm_auc)) %&amp;gt;% 
         arrange(desc(AUC))&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:200px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
n_noise_vars
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
method
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
AUC
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
rf
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9186806
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
gbm
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.8889589
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
nn
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.8417844
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
glm
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.8179276
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;This provides us with a general baseline. In the following section, we’ll see what happens to the Area Under the Curve (AUC) as we add in lots of irrelevant predictors. But first, let’s explore why we are using AUC as our evaluation metric.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;auc-as-a-measure-of-classification-accuracy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;AUC as a Measure of Classification Accuracy&lt;/h3&gt;
&lt;p&gt;Measuring model performance when your dependent variable is binary is more “involved” than measuring performance with a continuous dependent variable. With a continuous DV, large residuals are worse than small residuals, and this can be quantified along a continuum. In contrast, you can have a classifier that’s extremely accurate – it provides the correct classification 99.99% of the time – but actually doesn’t tell you anything useful. This situation typically arises when you have imbalanced classes, such as trying to predict whether or not someone has a disease when it only occurs in 1 of 100,000 people. By chance alone, the classifier would be right 99,999 times out of 100,000 if you just said: “no one has the disease ever”.&lt;/p&gt;
&lt;p&gt;Let’s consider the class balance in our wine dataset&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(round(table(wine_df$wine_class)/nrow(wine_df) * 100, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 
## IdDrinkThat    NoThanks 
##        19.7        80.3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to our refined, discerning pallette, 80% of wines would fall by chance alone in the “NoThanks”&amp;quot; category. We could achieve 80% accuracy by simply assigning a label of “NoThanks” to every new wine we encountered. This would obviously be a terrible idea, because then we’d never get to drink any wine! Thus, in cases where your classes are unevenly distributed, accuracy might not be the best evaluation metric.&lt;/p&gt;
&lt;p&gt;AUC isn’t affected by class imbalances because it considers both True Positives and False Positives. The True Positive Rate (TPR) captures all the instances in which our model said ‘Drink this wine’ and it was, in fact, a wine we would want to drink; False Positive Rate (FPR) captures instances in which our model said ‘Drink this Wine’ but it was actually a wine that we would not want to drink. Obtaining more TPRs and fewer FPRs will move the AUC closer to one, which means our model is improving. Hopefully, this clarifies the rationale for using AUC relative to accuracy. Now let’s get back to the original question.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-irrelevant-predictors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding Irrelevant Predictors&lt;/h3&gt;
&lt;p&gt;A little disclaimer before moving on. There are a number of ways you can reduce the influence of irrelevant features on predictive accuracy, namely &lt;em&gt;regularization&lt;/em&gt;. However, we’re interested in which algorithms are least susceptible to modeling the noise associated with an irrelevant predictor variable without any additional regularization or parameter optimization. Accordingly, we’ll sample the irrelevant predictors from a normal distribution with a mean of zero and a standard deviation of one. Each iteration adds an additional 100 irrelevant predictors across a total of 20 iterations. All of the algorithms use the default parameters that come with the &lt;code&gt;h2o&lt;/code&gt; library.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_noise_vars = seq(100, 2000, length.out = 20)

for(i in n_noise_vars){
  print(i)
  temp_noise_df_train = data.frame(placeholder = rep(NA, nrow(df_split$train)))
  temp_noise_df_test = data.frame(placeholder = rep(NA, nrow(df_split$test)))
  
  # add in i irrelevant predictors to train and test
  for(j in 1:i){
    temp_noise_df_train = cbind(temp_noise_df_train, 
                                data.frame(noise.var = rnorm(nrow(df_split$train), 
                                                           0, 
                                                           1)))
    temp_noise_df_test = cbind(temp_noise_df_test, 
                               data.frame(noise.var = rnorm(nrow(df_split$test), 
                                                           0, 
                                                           1)))
  }
  # format names of irrelevant variables
  temp_noise_df_train = temp_noise_df_train[,2:dim(temp_noise_df_train)[2]]
  names(temp_noise_df_train) = gsub(&amp;quot;\\.&amp;quot;, &amp;quot;&amp;quot;, names(temp_noise_df_train))
  temp_noise_df_train = as.h2o(cbind(temp_noise_df_train,
                              df_split$train))
  
  temp_noise_df_test = temp_noise_df_test[,2:dim(temp_noise_df_test)[2]]
  names(temp_noise_df_test) = gsub(&amp;quot;\\.&amp;quot;, &amp;quot;&amp;quot;, names(temp_noise_df_test))
  temp_noise_df_test = cbind(temp_noise_df_test,
                             df_split$test)
  
  x_var = setdiff(names(temp_noise_df_train),y_var)

  gbm_fit = h2o.gbm(x = x_var, 
                    y = y_var,
                    distribution = &amp;quot;bernoulli&amp;quot;,
                    training_frame = temp_noise_df_train,
                    stopping_rounds = 3,
                    stopping_metric = &amp;quot;AUC&amp;quot;)
  
  rf_fit = h2o.randomForest(x = x_var,
                            y = y_var,
                            training_frame = temp_noise_df_train,
                            stopping_rounds = 3,
                            stopping_metric = &amp;quot;AUC&amp;quot;)
  
  nn_fit = h2o.deeplearning(x = x_var,
                            y = y_var,
                            training_frame = temp_noise_df_train,
                            stopping_rounds = 3,
                            stopping_metric = &amp;quot;AUC&amp;quot;)
  
  glm_fit = h2o.glm(x = x_var,
                    y = y_var,
                    family = &amp;quot;binomial&amp;quot;,
                    training_frame = temp_noise_df_train)
  
  temp_noise_df_test = as.h2o(temp_noise_df_test)
  
  gbm_auc = h2o.auc(h2o.performance(gbm_fit, newdata = temp_noise_df_test))
  rf_auc = h2o.auc(h2o.performance(rf_fit, newdata = temp_noise_df_test))
  nn_auc = h2o.auc(h2o.performance(nn_fit, newdata = temp_noise_df_test))
  glm_auc = h2o.auc(h2o.performance(glm_fit, newdata = temp_noise_df_test))
  
  auc_df = rbind(auc_df,
                 data.frame(n_noise_vars = i,
                 method = c(&amp;#39;gbm&amp;#39;, &amp;#39;rf&amp;#39;, &amp;#39;nn&amp;#39;, &amp;#39;glm&amp;#39;),
                 AUC = c(gbm_auc, rf_auc, nn_auc, glm_auc))) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now let’s plot the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(auc_df, aes(x = n_noise_vars, 
                   y = AUC, 
                   color = method)) + 
         geom_point(size = 2) + 
  stat_smooth(span = 1.75, 
              se = FALSE, 
              size = 2) + 
  theme_bw() + 
  my_plot_theme() + 
  ylab(&amp;quot;AUC&amp;quot;) + 
  xlab(&amp;quot;N Noise Variables&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/feature_selection_wine_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A few initial observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;AUC for all methods, with the exception of GLM, decreased as the number of irrelevant predictors in the model increased&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GBM is the most robust against the ill-effects of irrelevant predictors, while NN was the most susceptible&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although we can’t generalize these outcomes to every situation, they do align with my experience. In fact, one of the reasons I like GBM is that it tends to ignore irrelevant predictors during modeling. Neural networks are extremely powerful and with the right tuning/regularization will often outperform any of the methods outlined here. However, they are susceptible to overfitting because of their flexibility, which leads to noise being leveraged as signal during the model building process. This goes to show that pre-processing and feature selection is an important part of the model building process if you are dealing with lots of potential features. Beyond reducing the training time, pruning irrelevant features can vastly improve model performance. As illustrated above, including irrelevant features can wreak havoc on your model when using methods with lots of flexible parameters. It would be interesting to see how these results change if we added some regularization measures to protect against overfitting. For example, one way to reduce overfitting with Random Forest is to limit the maximum depth of a tree (i.e., each tree can only have a single split instead of multiple splits). Or for Neural Networks L2 regularization can be used, which discourages weight vectors in the network from becoming too large. If you try any of these approaches, I’d love to hear how it goes!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Establishing Causality with Counterfactual Prediction</title>
      <link>/post/counterfactual_prediction.html</link>
      <pubDate>Sat, 05 Aug 2017 21:13:14 -0500</pubDate>
      
      <guid>/post/counterfactual_prediction.html</guid>
      <description>&lt;p&gt;&lt;img src=&#34;images/dealership_tube_man.jpg&#34; width=&#34;800px&#34; height=&#34;800px&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;Businesses have several “levers” they can pull to boost sales. For example, a series of advertisements might be shown in a city or region with the hopes of increasing sales for the advertised product. Ideally, the cost associated with the advertisement would be outstripped by the expected increase in sales, visits, conversion, or whatever KPI (key performance indicator) the business hopes to drive. But how do you capture the ROI of the advertisement? In a parallel universe, where all experiments are possible (and unicorns roam the land), we would create two copies of the same city, run the advertisement in one of them, track our metric of interest over the same time period, and then compare the difference between the two cities. All potential confounding variables, such as differences in seasonal variation, viewership rates, or buying preference, would be controlled. Thus, any difference (lift) in our KPI could be attributed to the intervention.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;used-cars-and-wacky-inflatable-tube-men&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Used Cars and Wacky Inflatable Tube Men&lt;/h3&gt;
&lt;p&gt;We can’t clone cities, but there is a way to statistically emulate the above situation. This post provides a general overview of how to generate a &lt;strong&gt;counterfactual prediction&lt;/strong&gt;, which is a way to quantify what would’ve happened had we never run the advertisement, event, or intervention.&lt;/p&gt;
&lt;p&gt;Incremental Sales, ROI, KPIs, Lift – these terms are somewhat abstract. Cars and Wacky-Inflatable-Tube-Men (WITM for short) – that all seems pretty straight-forward. So imagine you run a car dealership and you’re looking for ways to attract additional customers and (hopefully) increase sales. You decide to purchase this guy below, hoping it will draw attention to your dealership, increase foot traffic, and ultimately produce more car sales.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;counterfactual_prediction_images/wacky_inflatable_man.jpg&#34; width=&#34;800px&#34; height=&#34;800px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Being a savvy business owner, you’re interested in the effect of your new WITM on sales. Does having this form of advertisement drive sales beyond what would’ve happened in the absence of any WITM? You look to the other car dealerships in separate parts of town to answer this question. The other shops will serve as a control group because they don’t have a WITM. Additionally, the other shops are located far enough away from your shop so there is no way for their sales to be affected by your WITM (this is a critical assumption that in real-world contexts should be verified). After accounting for baseline differences in sales between your shop and the control shops, you build a forecast – or counterfactual prediction – premised on what would’ve happened in your shop had the WITM intervention never taken place. This prediction will then serve as the baseline from which to compare what happened to sales.&lt;/p&gt;
&lt;p&gt;In short, you (the owner) are faced with two tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Identifying which control shop(s) are most like your shop&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generating the counterfactual prediction&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll go through each of these steps at a high level, just to give you a general idea of the logic underlying the process. For an excellent, more in-depth explanation of the technical details check out this &lt;a href=&#34;http://multithreaded.stitchfix.com/blog/2016/01/13/market-watch/&#34;&gt;post&lt;/a&gt; by the author of the &lt;code&gt;MarketMatching&lt;/code&gt; package, which we will leverage to address both tasks.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-the-synthetic-data-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating the Synthetic Data Set&lt;/h3&gt;
&lt;p&gt;Let’s read the dataset, which can be downloaded &lt;a href=&#34;https://datamarket.com/data/set/22n4/monthly-car-sales-in-quebec-1960-1968#!ds=22n4&amp;amp;display=line&#34;&gt;here&lt;/a&gt;, and load the required libraries. The original dataset is the total number of cars sold each month in Quebec between 1960 and 1968. In this case, let’s assume that the numbers represent the total number of cars sold each month at our dealership.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs = c(&amp;quot;devtools&amp;quot;, &amp;quot;CausalImpact&amp;quot;, &amp;quot;forecast&amp;quot;,
         &amp;quot;data.table&amp;quot;, &amp;quot;kableExtra&amp;quot;,&amp;quot;dplyr&amp;quot;, 
         &amp;quot;forcats&amp;quot;, &amp;quot;MarketMatching&amp;quot;, &amp;quot;knitr&amp;quot;, 
         &amp;quot;ggplot2&amp;quot;, &amp;quot;ggforce&amp;quot;, &amp;quot;readr&amp;quot;,
         &amp;quot;janitor&amp;quot;, &amp;quot;lubridate&amp;quot;
         )

lapply(libs, require, character.only = TRUE)
# Uncomment if you dont have the package installed 
#devtools::install_github(&amp;quot;klarsen1/MarketMatching&amp;quot;, build_vignettes=TRUE)
working_dir = &amp;quot;path_to_data&amp;quot;
file_name = &amp;quot;monthly-car-sales-in-quebec-1960.csv&amp;quot;
car_sales = read_csv(file.path(working_dir, file_name)) %&amp;gt;% 
            clean_names() %&amp;gt;% 
            data.frame() %&amp;gt;% 
            mutate(month = as.Date(paste0(month, &amp;quot;-01&amp;quot;),
                                   format = &amp;quot;%Y-%m-%d&amp;quot;
                                   )) %&amp;gt;% 
            rename(sales = monthly_car_sales_in_quebec_1960_1968,
                   date = month) %&amp;gt;% 
            mutate(sales = floor(sales/100)) # to make numbers more realistic&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a high-level glance at our time series to see if we need to clean it up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_plot_theme = function(){
    font_family = &amp;quot;Helvetica&amp;quot;
    font_face = &amp;quot;bold&amp;quot;
    return(theme(
    axis.text.x = element_text(size = 18, face = font_face, family = font_family),
    axis.text.y = element_text(size = 18, face = font_face, family = font_family),
    axis.title.x = element_text(size = 20, face = font_face, family = font_family),
    axis.title.y = element_text(size = 20, face = font_face, family = font_family),
    strip.text.y = element_text(size = 18, face = font_face, family = font_family),
    plot.title = element_text(size = 18, face = font_face, family = font_family),
    legend.position = &amp;quot;top&amp;quot;,
    legend.title = element_text(size = 16,
    face = font_face,
    family = font_family),
    legend.text = element_text(size = 14,
    face = font_face,
    family = font_family)
))
}

ggplot(car_sales, aes(x = date, y = sales)) + geom_point(size = 2) + 
                                             geom_line(size = 2) + 
  theme_bw() + 
  my_plot_theme()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/counterfactual_prediction_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Overall, everything looks good. No major outliers or missing data. The magnitude of seasonal changes and random variation remains relatively constant over time, so the data generating process can be described with an additive model. Let’s transform our outcome variable (total monthly car sales) into a time series format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cars_ts = ts(car_sales$sales,
             frequency = 12,
             start = c(lubridate::year(min(car_sales$date)), 
                       lubridate::month(min(car_sales$date))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we’ll generate a 6-month ahead forecast. To simulate the positive effect that the WITM intervention had on sales during the 6-month intervention period, we’ll sample from a normal distribution where the mean is 105% of the forecasted value and the standard deviation is defined by the residuals of the fitted model. This would simulate a 5% lift above the number of cars we would expect to sell for that month. Using the residuals from the fitted model will result in a smaller standard deviation than using a holdout set, but to keep things simple we’ll estimate uncertainty based on model residuals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(111)
# define length of intervention
trial_period_months = 6
# define simulated effect size as a percentage (e.g., a 5% lift above what is expected each month)
effect_size = 0.05
arima_fit = auto.arima(cars_ts)
simulated_intervention = forecast(arima_fit, trial_period_months)$mean
# create simulated effect 
simulated_intervention = sapply(simulated_intervention, function(x) x + rnorm(1, 
                                                     mean = (x * effect_size),
                                                     sd = sd(arima_fit$residuals)))

monthly_sales = c(cars_ts, simulated_intervention)

study_dates = seq(max(car_sales$date), 
                  by = &amp;quot;month&amp;quot;, 
                  length.out = (trial_period_months + 1))

study_dates = study_dates[2:length(study_dates)]
intervention_start = study_dates[1]

cars_sim = data.frame(measurement_date = c(car_sales$date, study_dates),
                          sales = c(cars_ts, simulated_intervention),
                          shop = &amp;quot;1&amp;quot;) %&amp;gt;% 
                mutate(time_period = ifelse(measurement_date &amp;gt;= intervention_start, 
                                            &amp;quot;post-intervention&amp;quot;,
                                            &amp;quot;pre-intervention&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s plot the results to get a better idea of when each of the events is happening.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(cars_sim, aes(x = measurement_date, 
                         y = sales,
                         color = time_period
                         )) + 
  geom_point(size = 2) + geom_line(size = 2) +
  theme_bw() +
  my_plot_theme() + ylab(&amp;quot;Sales&amp;quot;) + xlab(&amp;quot;Date&amp;quot;) + 
  facet_zoom(x = measurement_date %in% as.Date(cars_sim %&amp;gt;%
                                      filter(time_period == &amp;quot;post-intervention&amp;quot;) %&amp;gt;%
                                      pull(measurement_date))) + 
  theme(legend.title=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/counterfactual_prediction_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The blue region is the time before the intervention, while the red region is the time during the intervention. It’s hard to determine if the level (mean) has increased for the last six months of our time series. The uptick could be due to seasonal variation or where the trend of the series was naturally going (Obviously, we know this to be false). We want to rule these explanations out. To do so, we’ll make a counterfactual prediction based on the other control shops. We’ll use the original time series as a basis to create six other time series. Three will be the original time series with an added Gaussian error term while the other three will be random walks. All six will serve as initial candidates for our control shops. Let’s generate the time series for our comparison shops.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(111)
comparison_df = data.frame(NULL)
similiar_shops = c(&amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;)
# generate time series for similar shops
for(i in 1:length(similiar_shops)){
  temp_ts = cars_ts

  # add a bit of random noise to comparison shop sales
  temp_ts = data.frame(sales = temp_ts + rnorm(length(temp_ts), 
                                               0, 
                                               (sd(temp_ts) * 0.25)))
  
  # fit a arima model to the simulated sales data
  temp_fit = auto.arima(ts(temp_ts$sales, 
                           frequency = 12, 
                           start = c(lubridate::year(min(car_sales$date)), 
                                     lubridate::month(min(car_sales$date)))))
  
  # generate forecast
  forecasted_values = data.frame(forecast(temp_fit,
                                          h = trial_period_months))$Point.Forecast
  
  temp_ts = data.frame(sales = c(temp_ts$sales, forecasted_values),
                       shop = similiar_shops[i])
  
  comparison_df = bind_rows(comparison_df, temp_ts)
}

# generate time series for random shops
random_shops = c(&amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;7&amp;quot;)

for(r in random_shops){
  temp_random = data.frame(sales = rnorm((length(cars_ts) + trial_period_months), 
                                         mean = mean(cars_ts), 
                                         sd = sd(cars_ts)),
                           shop = r
  )
  comparison_df = rbind(comparison_df, temp_random)
}

comparison_df = comparison_df %&amp;gt;% 
                mutate(measurement_date = rep(seq(min(car_sales$date),
                                    by = &amp;quot;month&amp;quot;,
                                    length.out = nrow(cars_sim)
                                    ),
                                length(unique(comparison_df$shop)))
                       ) %&amp;gt;% 
                bind_rows(cars_sim %&amp;gt;% 
                          dplyr::select(-time_period))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have all of time series in a neat dataframe. Let’s visualize what that looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(comparison_df, aes(x = as.Date(measurement_date), 
                          y = sales, 
                          color = shop)) + geom_point() + geom_line() +
  facet_wrap(~shop, ncol = 1) + 
  theme_bw() + 
  my_plot_theme() + ylab(&amp;quot;Sales&amp;quot;) + 
  xlab(&amp;quot;Date&amp;quot;) + 
  theme(strip.text.x = element_text(size = 14, face = &amp;quot;bold&amp;quot;),
        axis.text.y = element_blank(),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/counterfactual_prediction_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Shop 1 is our shop, while Shops 2-7 are the potential control shops. To make inferences about the effect of our intervention, we want to identify a seperate shop with similar sales history to serve as the control shop We’ll keep it simple here and only use a single shop, but you could use any number of shops as a control. I’ll discuss in the next section how we go about defining and identifying similarity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;selecting-a-control-time-series-with-dynamic-time-warping&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Selecting a Control Time Series with Dynamic Time Warping&lt;/h3&gt;
&lt;p&gt;We’ll implement a technique known as Dynamic Time Warping. It sounds like something you’d hear in a Steven Hawking TED talk, but it’s just a way to measure the similarity between two sequences. A common approach for measuring the similarity between sequences is to take the squared or absolute difference between them at the same period and then sum up the results. The main drawback with this approach is that it fails to account for seasonal changes that might be shifted or delayed but still occur in the same order. It’s like if two people said the same sentence but one person said it much slower than the other. The order and content of the utterance is the same but the cadence is different. This is where Dynamic Time Warping shines. It stretches or compresses (within some constraints) one time series to make it as similar as possible to the other. An individual’s speech cadence wouldn’t affect our ability to determine the similarity between two separate utterances.&lt;/p&gt;
&lt;p&gt;In the current context, we aren’t concerned with leading or lagging seasonality, as a random error was added to each value in the absence of any forward or backward shifts. However, this can be an issue when dealing with phenomena that are impacted by, say, weather. For example, imagine you sell a seasonal product like ice cream. Ice cream sales go up when the weather gets hot, and perhaps it warms up in some parts of the same region before others. This is a case when you might see the exact same sales patterns but some emerge a few months before or after others. Therefore, it is important to use a matching method that can account for these shifts when comparing the similarity of two time-series.&lt;/p&gt;
&lt;p&gt;We’ll leverage the &lt;code&gt;MarketMatching&lt;/code&gt; package to select our control time series. The selection process is done via DTW.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;start = min(comparison_df$measurement_date)
end = unique(comparison_df$measurement_date)[(length(unique(comparison_df$measurement_date)) - 
                                                                                       trial_period_months)]
most_similar_shop = MarketMatching::best_matches(data = comparison_df,
                             id_variable = &amp;quot;shop&amp;quot;,
                             date_variable = &amp;quot;measurement_date&amp;quot;,
                             matching_variable = &amp;quot;sales&amp;quot;,
                             warping_limit = 3,
                             matches = 1,
                             start_match_period = start,
                             end_match_period = end
                             )&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:310px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
shop
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
BestControl
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
RelativeDistance
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Correlation
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Length
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
rank
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
MatchingStartDate
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
MatchingEndDate
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0892325
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9749544
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
108
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1960-01-01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1968-12-01
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.1046940
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9675096
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
108
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1960-01-01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1968-12-01
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.1007655
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9667409
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
108
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1960-01-01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1968-12-01
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0887502
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9749544
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
108
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1960-01-01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1968-12-01
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.2950050
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.1958782
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
108
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1960-01-01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1968-12-01
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.3073312
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.1539837
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
108
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1960-01-01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1968-12-01
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.2831873
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.1958782
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
108
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1960-01-01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1968-12-01
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;That was too easy! This table indicates that the pre-intervention sales for Shop Number 4 are the most similar to those in Shop Number 1. Thus, we’ll use Shop 4 as our reference when generating the counterfactual prediction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-counterfactual-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generating Counterfactual Predictions&lt;/h3&gt;
&lt;p&gt;This is the inference part – namely, can we infer that our intervention impacted sales?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results = MarketMatching::inference(matched_markets = most_similar_shop,
                                     test_market = &amp;quot;1&amp;quot;,
                                     end_post_period = max(comparison_df$measurement_date)
                                    )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##  ------------- Inputs -------------
##  Test Market: 1
##  Control Market 1: 4
##  Market ID: shop
##  Date Variable: measurement_date
##  Matching (pre) Period Start Date: 1960-01-01
##  Matching (pre) Period End Date: 1968-12-01
##  Post Period Start Date: 1969-01-01
##  Post Period End Date: 1969-06-01
##  Matching Metric: sales
##  Local Level Prior SD: 0.01
##  Posterior Intervals Tail Area: 95%
##
##
##  ------------- Model Stats -------------
##  Matching (pre) Period MAPE: 5.75%
##  Beta 1 [4]: 0.9362
##  DW: 1.92
##
##
##  ------------- Effect Analysis -------------
##  Absolute Effect: 93.19 [15.59, 161.41]
##  Relative Effect: 7.72% [1.29%, 13.37%]
##  Probability of a causal impact: 98.996%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s break down the &lt;strong&gt;Effect Analysis&lt;/strong&gt; portion of the output. The &lt;strong&gt;Absolute Effect&lt;/strong&gt; indicates that we sold ~93 more cars over the 6-month intervention period relative to what we expected to sell. The &lt;strong&gt;Relative Effect&lt;/strong&gt; provides a standardized view, indicating a 7.7% lift in sales. Finally, the probability of a causal impact indicates that the likelihood of observing this effect by chance is extremely low (e.g., 100 - 98.9). We can see what was just described above in visual format below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(results$PlotActualVersusExpected)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/counterfactual_prediction_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This drives home that our intervention had a clear effect – and we should promote the WITM to shop manager for the excellent work in driving up sales. Hopefully, this post gave you a better idea of how to quantify the impact of an intervention. The ability to run a randomized experiment in the real-world is often too expensive or simply not possible. In many cases creating a statistical control group is the best (and cheapest) option available. For more information about the forecasting methodology used produce counterfactual predictions, check out the &lt;a href=&#34;https://cran.r-project.org/web/packages/bsts/bsts.pdf&#34;&gt;bsts&lt;/a&gt; and &lt;a href=&#34;https://google.github.io/CausalImpact/CausalImpact.html&#34;&gt;CausalImpact&lt;/a&gt; packages. Happy experimenting!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Monte Carlo Power Calculations for Mixed Effects Models</title>
      <link>/post/monte_carlo_power_mixed_effects.html</link>
      <pubDate>Fri, 28 Jul 2017 21:13:14 -0500</pubDate>
      
      <guid>/post/monte_carlo_power_mixed_effects.html</guid>
      <description>&lt;p&gt;&lt;img src=&#34;images/monte-carlo.jpg&#34; width=&#34;800px&#34; height=&#34;800px&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;what-is-power&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What is Power?&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Power&lt;/em&gt; is the ability to detect an effect, given that one exists. I’ll illustrate this concept with a simple example. Imagine you are conducting a study on the effects of a weight loss pill. One group receives the experimental weight loss pill while the other receives a placebo. For the pill to be marketable, you want to claim an average loss of at least 3lbs. This is our &lt;em&gt;effect size&lt;/em&gt;. Estimating an &lt;em&gt;effect size&lt;/em&gt; can be a bit tricky; you can use history or simply figure out what size of effect you want to see (in this case a difference of 3lbs). Additionally, we’ll have to specify a standard deviation. This can come from similar, previous studies or a pilot study. To keep things simple, let’s say we reviewed some literature on similiar weight loss studies and observed a standard deviation of 8lbs. We’ll also assume that both groups exhibit the same degree of variance. Now that we’ve established our effect size and standard deviation, we need to know how many people to include in the study. If each participant is paid $100 for completing the 1-month study and we have a budget of $10,000, then our sample size can be at most 100 participants, with 50 randomly assigned to the placebo condition and the remaining 50 assigned to the pill condition. Finally we’ll use the conventional significance level of 0.05 as our &lt;em&gt;alpha&lt;/em&gt;. So now we have our pieces required to calculate experimental power:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Effect Size &amp;amp; Standard Deviation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sample Size&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Significance Level&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s illustrate how we would implement the above situation–and calculate power–in R. We’ll specify the parameters outlined above and then run a quick power-analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs = c(&amp;#39;broom&amp;#39;, &amp;#39;dplyr&amp;#39;, &amp;#39;ggplot2&amp;#39;,
         &amp;#39;artyfarty&amp;#39;, &amp;#39;lme4&amp;#39;, &amp;#39;janitor&amp;#39;,
         &amp;#39;simr&amp;#39;)
lapply(libs, require, character.only = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_size_per_group = 50
desired_effect_size = 3
standard_deviation = 8
alpha = 0.05
expected_power = power.t.test(n = sample_size_per_group,
                              delta = desired_effect_size,
                              sd = standard_deviation,
                              sig.level = alpha,
                              type = &amp;quot;two.sample&amp;quot;,
                              alternative = &amp;quot;two.sided&amp;quot;)
print(paste0(&amp;quot;EXPECTED POWER IS: &amp;quot;, round(expected_power$power * 100, 0), &amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## [1] &amp;quot;EXPECTED POWER IS: 46%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a 46% chance of actually detecting an effect if we ran a study with these exact parameters – an effect that we know exists! That’s pretty low. What would happen if we increased our budget to $20,000 and doubled the number of participants in each group.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_size_per_group = 100
desired_effect_size = 3
standard_deviation = 8
alpha = 0.05
expected_power = power.t.test(n = sample_size_per_group,
                              delta = desired_effect_size,
                              sd = standard_deviation,
                              sig.level = alpha,
                              type = &amp;quot;two.sample&amp;quot;,
                              alternative = &amp;quot;two.sided&amp;quot;)
print(paste0(&amp;quot;EXPECTED POWER IS: &amp;quot;, round(expected_power$power * 100, 0), &amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## [1] &amp;quot;EXPECTED POWER IS: 75%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So our power went up, which makes sense. As we increase our sample size, we become more confident in our ability to estimate the true effect. In the next section I’ll discuss how to obtain these exact results through a simulated experiment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-power&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simulating Power&lt;/h3&gt;
&lt;p&gt;If you are like me, I need to know what’s actually going on. The process of calculating power initially seemed a bit mysterious to me. And when things don’t make sense, the best way to understand them is to write some code and go line by line to figure out where the numbers come from. So that’s what we’ll do.&lt;/p&gt;
&lt;p&gt;Imagine we ran 1000 studies with the parameters outlined above. After each study, we conducted an independent-samples t-test, calculated the p-value, and determined whether it was less than our alpha (0.05). In all the cases where the value was greater than 0.05, we commited a &lt;em&gt;type II error&lt;/em&gt;, because we know there is a difference between our two groups. Let’s add up this number, subtract it from 1000, and the total is our power.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
pvalue_vector = c()
n_iter = 1000
type_II_error = NULL
placebo_group_mean = 0
pill_group_mean = -3
sample_size_per_group = 100

for(i in 1:n_iter){
    placebo_group = rnorm(sample_size_per_group, 
                      mean = placebo_group_mean,
                      sd = standard_deviation)

    pill_group = rnorm(sample_size_per_group,
                   mean = pill_group_mean,
                   sd = standard_deviation)

    temp_dataframe = data.frame(weight_difference = c(pill_group, placebo_group),
                            group = c(rep(&amp;quot;pill&amp;quot;, sample_size_per_group),
                                      rep(&amp;quot;placebo&amp;quot;, sample_size_per_group)))

    temp_p_value = tidy(t.test(weight_difference ~ group,
                               data = temp_dataframe))$p.value
    if(temp_p_value &amp;lt; 0.05){
    type_II_error = 0
    } else {
    type_II_error = 1
    }
    pvalue_vector = c(pvalue_vector, type_II_error)
}
print(paste0(&amp;quot;EXPECTED POWER IS: &amp;quot;, round(100 - sum(pvalue_vector)/1000 * 100, 0), &amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## [1] &amp;quot;EXPECTED POWER IS: 76%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pretty neat! Our 76% estimate is almost identical to the 75% estimate generated from the built-in power calculator. In the next section we’ll extend these ideas to a class of models referred to as &lt;em&gt;Random Effects&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-brief-primer-on-random-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Brief Primer on Random Effects&lt;/h3&gt;
&lt;p&gt;Have you ever ran a study where the things you were studying (e.g., people, animals, plants) were part of a hierarchy? For example, students are part of the same classroom; workers are part of the same company; animals live in the same tree; plants grow on the same plot of land. In each of the instances, the things from the same environment will (presumably) be more like one another than the things from different environments. These are typically referred to as &lt;em&gt;nested&lt;/em&gt; data, and we want to account for the fact that there is structure to the variability between the things we are studying.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-power-for-mixed-effects-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calculating Power for Mixed Effects Models&lt;/h3&gt;
&lt;p&gt;The study we’ll use to illustrate these concepts comes with the &lt;code&gt;lme4&lt;/code&gt; package. Being part of this study sounded pretty terrible, so I hope the participants got some decent compensation. Anyways, here’s the lowdown: 18 truck drivers were limited to three hours of sleep over a period of 10 days. Their reaction times on a series of tests were measured several times a day, and single reaction time measurement was entered for each participant per day. The fixed effect – or the thing we are interested in – was number of days of sleep deprivation. The random effect is the participant. The researchers want to generalize their findings to all people – not just the people included in the study– so differences between participants is of little interest. Thus they want to control for this variability by specifiying a random effect, and ideally get a better idea of how the fixed effect (Number of days of sleep deprivation) impacts reaction time. We’ll keep it simple and use a random effect for the intercept. What this means is that some participants will react faster (slower) than others, regardless of whether they were sleep deprived. We can check the validity of this assumption with a basic boxplot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sleep_df = lme4::sleepstudy %&amp;gt;% 
           clean_names()

ggplot(sleep_df, aes(x = factor(subject), y = reaction)) + 
  geom_boxplot(color = pl_colors[1]) + 
    theme_bw() + 
    my_plot_theme() + 
    xlab(&amp;quot;Subject ID&amp;quot;) + ylab(&amp;quot;Reaction Time&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/monte_carlo_power_mixed_effects_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If the median of each of our boxplots were approximately equal, then we could use a fixed effect or simply not including the subject effect at all. But clearly this isn’t the case. Accordingly, here is the form of our model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y_var = &amp;quot;reaction&amp;quot;
fixed_effect = &amp;quot;days&amp;quot;
random_effect = &amp;quot;subject&amp;quot;
model_form = as.formula(paste0(y_var, &amp;quot; ~ &amp;quot;, fixed_effect, &amp;quot; + &amp;quot;, &amp;quot;(1|&amp;quot;, random_effect, &amp;quot;)&amp;quot;))
print(model_form)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## reaction ~ days + (1 | subject)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next let’s fit the model to the complete dataset and determine the experimental power based on 100 simulations using the &lt;code&gt;simr&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
sleep_fit = lmer(model_form, 
                 data = sleep_df)
nsim = 100
expected_power = powerSim(sleep_fit, nsim = nsim)
print(expected_power)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Power for predictor &amp;#39;days&amp;#39;, (95% confidence interval):
##       100.0% (96.38, 100.0)
## 
## Test: Kenward Roger (package pbkrtest)
##       Effect size for days is 10.
## 
## Based on 100 simulations, (0 warnings, 0 errors)
## alpha = 0.05, nrow = 180
## 
## Time elapsed: 0 h 0 m 26 s
## 
## nb: result might be an observed power calculation&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This indicates that we are all but guaranteed to detect an effect if we run the study with 18 participants for a period of 10 days. The folks who did this study obviously wanted to leave nothing to chance! What if we tried to replicate the study with the same number of participants but for fewer days? How would our power change if we only ran the study for, say, three days? We’ll filter the data to only include days 0-3, and then repeat the power estimate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(321)
n_days_max = 3
min_days_df = sleep_df %&amp;gt;% 
              filter(days &amp;lt;= n_days_max)
min_days_fit = lmer(model_form, min_days_df)
expected_power = powerSim(min_days_fit, nsim = nsim)
print(expected_power)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Power for predictor &amp;#39;days&amp;#39;, (95% confidence interval):
##       98.00% (92.96, 99.76)
## 
## Test: Kenward Roger (package pbkrtest)
##       Effect size for days is 4.4.
## 
## Based on 100 simulations, (0 warnings, 0 errors)
## alpha = 0.05, nrow = 72
## 
## Time elapsed: 0 h 0 m 18 s
## 
## nb: result might be an observed power calculation&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Reducing the number of days from 10 to three had almost no effect on our power, which is estimated to be 98%. Thus, the recommendation here would be to run the study for fewer days. Great! That was easy. Indeed, I was excited after using the &lt;code&gt;simr&lt;/code&gt; package because of its ability to fit many different model specifications. The only issue was that I wasn’t entirely clear on the underlying process being used to calculate power. According the tutorial from the authors (see &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12504/full&#34;&gt;here&lt;/a&gt;), there are three steps involved in calculating power for mixed effects models via simulation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Simulate new values for the response variable using the model provided&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Refit the model to the simulated response&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Apply a statistical test to the model&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It was that first part where I wasn’t entirly clear, so I decided to build my own from scratch and compare the results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-power-simulator&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Building a Power Simulator&lt;/h3&gt;
&lt;p&gt;We established our model above as &lt;code&gt;reaction ~ days + (1|subject)&lt;/code&gt;. So what do they mean by “simulate new values for the response variable”? The new values are produced by our model plus error associated with sampling. That second part is thing we are going to simulate. We will assume the residuals are normally distributed, with a mean of zero and a standard deviation of…and a standard deviation of…aww shoot how do we come up with this number? We can initially start with the residuals from our model and use that as the standard deviation. Let’s see if we can produce a result similiar to the 98% estimate observed with the &lt;code&gt;simr&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(321)

# create empty vector to store Type II errors
type_II_vec = c()
# Step 1: make predictions from  model based on the original data
model_predictions = predict(min_days_fit, min_days_df)
# Step 2: Calculate  standard deviation based on the residuals from our initial model
standard_deviation = sd(min_days_fit@resp$y - min_days_fit@resp$mu)

for(i in 1:nsim){
  # Step 3: Simulate  sampling error
  temporary_residuals = rnorm(nrow(min_days_df), 
                              mean = 0, 
                              sd = standard_deviation)
  # Step 4: Save simulated response values
  min_days_df$simulated_reaction = model_predictions + temporary_residuals
  # Step 5: refit the model with the new, simulated response variable
  temp_fit = lmer(simulated_reaction ~ days + (1|subject), data = min_days_df)
  # Step 6: Check to see if confidence interval for the Days coefficient contains zero
  ci_df = tidy(confint(temp_fit)) %&amp;gt;% 
              dplyr::rename(coefficients = .rownames,
              lower_bound = X2.5..,
              upper_bound = X97.5..) %&amp;gt;% 
          filter(coefficients == &amp;#39;days&amp;#39;) %&amp;gt;% 
          dplyr::select(lower_bound, upper_bound)
  # Step 7: If Confidence interval contains zero, store as 1 (Type II error), else return store zero
  type_II_vec = c(type_II_vec, 
                  as.integer(dplyr::between(0, ci_df$lower_bound, ci_df$upper_bound)))
}
print(paste0(&amp;quot;EXPECTED POWER IS: &amp;quot;, (nsim - sum(type_II_vec))/nsim * 100, &amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## [1] &amp;quot;EXPECTED POWER IS: 99%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Exactly what we expected. Of our 100 simulations, we commited a &lt;em&gt;type II error&lt;/em&gt; in only one instance, which matches closely with the 98% estimate provided from the &lt;code&gt;simr&lt;/code&gt; package. If we increased the number of simulations, the power estimates would converge to the same number. However, if you copied this code and ran it in R, you’ll notice that it ran slow. In a separate &lt;a href=&#34;https://thecodeforest.github.io/post/two_flavors_of_parallel_simulation.html&#34;&gt;post&lt;/a&gt;, I’ll show you how to speed up any simulation by executing the entire process in parallel to run more simulations and get better estimates! Stay tuned!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Time Series Outlier Detection</title>
      <link>/post/time_series_outlier_detection.html</link>
      <pubDate>Fri, 28 Jul 2017 21:13:14 -0500</pubDate>
      
      <guid>/post/time_series_outlier_detection.html</guid>
      <description>&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;While there are several definitions for an outlier, I generally think of an outlier as the following: An unexpected event that is unlikely to happen again. For example, website traffic drops precipitously because of a server fire, or insurance claims spike because of an unprecedented, 500-year weather event. These events are occurrences that (we hope) do not occur a regular cadence. Yet if you were attempting to predict future website traffic or understand the seasonal patterns of insurance claims, the aforementioned events may greatly impact our forecasting accuracy. Thus there are two main reasons for conducting an outlier analysis prior to generating a forecast:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Outliers can bias forecasts&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outliers can inflate estimated variance and produce prediction intervals that are too wide&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This post discusses the use of seasonal-decomposition to isolate errors that cannot be explained as a function of trend or seasonality. We’ll also discuss two simple, commonly used approaches for identifying outliers, followed by a brief overview of how to replace outliers with more sensible values. Finally, we’ll test how including or excluding outliers affects the accuracy of our forecasts as well as the width of our prediction intervals. These questions will be explored within the context of monthly milk production from 1962-1975, where the value for each month represents the pounds of milk produced per cow (Riveting stuff, right?).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;/h3&gt;
&lt;p&gt;The data for this post is located &lt;a href=&#34;https://datamarket.com/data/set/22ox/monthly-milk-production-pounds-per-cow-jan-62-dec-75#!ds=22ox&amp;amp;display=line&#34;&gt;here&lt;/a&gt;. Let’s load up the required libraries, bring the data into R, do a bit of cleaning, and then have a quick look at the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs = c(&amp;#39;dplyr&amp;#39;, &amp;#39;artyfarty&amp;#39;, &amp;#39;ggplot2&amp;#39;,
         &amp;#39;forecast&amp;#39;, &amp;#39;reshape&amp;#39;,&amp;#39;readr&amp;#39;,
         &amp;#39;reshape&amp;#39;)
lapply(libs, require, character.only = TRUE)
working_directory = &amp;quot;path_to_file&amp;quot;
file_name = &amp;quot;monthly-milk-production-pounds-p.csv&amp;quot;
milk_df = read_csv(file.path(working_directory, file_name))
names(milk_df) = c(&amp;#39;month&amp;#39;, &amp;#39;milk_production&amp;#39;)
milk_df = milk_df %&amp;gt;% 
          mutate(month = as.Date(paste(month, &amp;quot;01&amp;quot;, sep = &amp;quot;-&amp;quot;),
                                 format = &amp;quot;%Y-%m-%d&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
month
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
milk_production
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1962-01-01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
589
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1962-02-01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
561
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1962-03-01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
640
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1962-04-01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
656
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1962-05-01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
727
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1962-06-01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
697
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now that we have our time series dataframe, let’s introduce six outliers into our time series. I picked these data points and their values at random.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;milk_df$milk_production[30] = milk_df$milk_production[30] + 70
milk_df$milk_production[55] = milk_df$milk_production[55] - 60
milk_df$milk_production[99] = milk_df$milk_production[99] - 220
milk_df$milk_production[100] = milk_df$milk_production[100] + 100
milk_df$milk_production[152] = milk_df$milk_production[152] + 40
milk_df$milk_production[153] = milk_df$milk_production[153] - 70

outlier_milk_df = milk_df[c(30, 55, 99, 100, 152, 153),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s examine what our time series. The pink points are the outliers we just introduced.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(milk_df, aes(x = month, y = milk_production)) +
    geom_point(size = 2, color = color_values[1]) + 
    geom_line(size = 2, color = color_values[1]) +
    theme_bw() + 
    my_plot_theme() + 
    theme(legend.title = element_blank()) + 
    xlab(&amp;quot;Date&amp;quot;) + ylab(&amp;quot;Milk Production&amp;quot;) + 
    geom_point(data = outlier_milk_df, colour = color_values[2], size = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/time_series_outlier_detection_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s break our time series into three separate components: Seasonal, Trend, and Remainder. The &lt;code&gt;seasonal&lt;/code&gt; and &lt;code&gt;trend&lt;/code&gt; are structural parts of the time series that we can explain, while the &lt;code&gt;remainder&lt;/code&gt; is everything that’s left over that we cannot explain. We’ll focus on this portion of the time series when looking for anomalous data points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;decomp_ts = stl(ts(milk_df$milk_production, frequency = 12), 
                s.window = &amp;quot;periodic&amp;quot;,
                robust = TRUE
                )
ts_decomposition = data.frame(decomp_ts$time.series) %&amp;gt;%
                   melt() %&amp;gt;%
                   mutate(month = rep(milk_df$month, 3)) %&amp;gt;%
                   dplyr::rename(component = variable)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(ts_decomposition, aes(x = month, y = value, fill = component)) + 
    geom_bar(stat = &amp;quot;identity&amp;quot;) + 
    facet_grid(component~ ., scales = &amp;quot;free&amp;quot;) + 
    theme_bw() + 
    my_plot_theme() + 
    scale_fill_manual(values = c(color_values[1:3])) +
    theme(legend.title = element_blank()) +
    ylab(&amp;quot;&amp;quot;) + xlab(&amp;quot;Date&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/time_series_outlier_detection_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on this breakout, there is one clear anomaly (the -200 point). The other five aren’t as salient but we know they are there. Let’s try out our two approaches: The first is the +- 3 standard deviation rule. Any residual that is +- 3 SDs is considered an anomaly. If our residuals are normally distributed, then by chance alone 27 of every 10000 points should fall outside of these boundaries. Thus it is possible (but unlikely) when you detect a residual of this magnitude or greater. The second method leverages the Interquartile Range (or IQR). The IQR is the difference between the value at the 75th and 25th percentiles of your residuals. You then multiply the range by some constant (often 1.5). Any values outside of this range are considered an anomaly.&lt;/p&gt;
&lt;p&gt;I don’t want to bury the lede here, so I’ll just come right out and say it: The +-3 SD rule is not the approach you want to use. In the real world, which can admittedly be a scary place, forecasting is done at scale. You’ll be generating lots and lots of forecasts, so many that you won’t be able to verify the assumptions or examine the residuals for each one (very scary). Thus it is imperative to use methods for outlier detection that are robust. What does this mean? In short, a measure that quantifies “outlierness” must be immune to the effects of outliers. Hang on…I’m confused. I’ll visualize what this means below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sd-approach&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;+- 3 SD Approach&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;remainder = ts_decomposition %&amp;gt;% 
            filter(component == &amp;#39;remainder&amp;#39;)
sd_remainder = sd(remainder$value) 
anomaly_boundary = c(-sd_remainder * 3, 
                      sd_remainder * 3)
remainder_sd = remainder %&amp;gt;% 
               mutate(is_anomaly = ifelse(value &amp;gt; anomaly_boundary[2] | 
                             value &amp;lt; anomaly_boundary[1], &amp;quot;yes&amp;quot;, &amp;quot;no&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(remainder_sd, aes(x = value, fill = is_anomaly)) +  geom_histogram(aes(y=..density..),
    bins = 30,
    colour=&amp;quot;black&amp;quot;) +
    theme_bw() + 
    my_plot_theme() +
    geom_vline(xintercept = anomaly_boundary[1], size = 2, linetype = &amp;quot;dotted&amp;quot;,
    color = color_values[1]) + 
    geom_vline(xintercept = anomaly_boundary[2], size = 2, linetype = &amp;quot;dotted&amp;quot;,
    color = color_values[1]) + 
    xlab(&amp;quot;Remainder&amp;quot;) + 
    scale_fill_manual(values = c(color_values[1],color_values[2]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/time_series_outlier_detection_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This method identified four of the six outliers It missed the other two because the one really big outlier (-220) inflated our standard deviation, making the other two, smaller outliers undetectable. Let’s see how the IQR approach performed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iqr-approach&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;IQR Approach&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spread = 1.5
pct_50 = unname(quantile(remainder$value, 0.5))
iqr = diff(unname(quantile(remainder$value, c(0.25, 0.75))))
lb = unname(quantile(remainder$value, 0.25)) - (spread * iqr)
ub = unname(quantile(remainder$value, 0.75)) + (spread * iqr)
remainder_iqr = remainder %&amp;gt;% 
                mutate(is_anomaly = ifelse(value &amp;gt; ub | value &amp;lt; lb, &amp;quot;yes&amp;quot;, &amp;quot;no&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(remainder_iqr, aes(x = component, y = value)) + 
    geom_boxplot() + 
    geom_point(size = 4, aes(color = is_anomaly,
    fill = is_anomaly)) +
    theme_bw() + 
    my_plot_theme() + 
    xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;Remainder&amp;quot;) + 
    theme(axis.text.x = element_blank()) + 
    scale_color_manual(values = c(color_values[1],color_values[2]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/time_series_outlier_detection_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The IQR method detected all 6 of the synthetic outliers in addition to two non-outliers. However, does any of this matter? Data cleaning steps are a means to an end. We address outliers in our data so we (presumably) get better forecasts and more accurate prediction intervals. Let’s compare the forecasting accuracy between three methods:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Do Nothing (just leave outliers in there and make a forecast)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;+- 3 SDs&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;+- 1.5 Interquartile Range&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Methods 2 and 3 use linear interpolation to replace the outliers. Let’s create our validation dataset below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anom_index_sd = which(match(remainder_sd$is_anomaly, &amp;quot;yes&amp;quot;) %in% c(1))
anom_index_iqr = which(match(remainder_iqr$is_anomaly, &amp;quot;yes&amp;quot;) %in% c(1))
sd_df = milk_df
iqr_df = milk_df
sd_df$milk_production[anom_index_sd] = NA
iqr_df$milk_production[anom_index_iqr] = NA
n_holdout = 12 # number of months in the validation set
validation_set = milk_df$milk_production[(nrow(sd_df) - n_holdout + 1):nrow(sd_df)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then create three seperate forecasts via the &lt;code&gt;auto.arima&lt;/code&gt; function. The &lt;code&gt;na.interp&lt;/code&gt; function is used to replace the &lt;strong&gt;NA&lt;/strong&gt; values with an interpolated (i.e., non-anomalous) value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd_ts = na.interp(ts(sd_df$milk_production[1:(nrow(sd_df) - n_holdout)],
                     frequency = 12))
sd_forecast = forecast(auto.arima(sd_ts), h = n_holdout)
iqr_ts = na.interp(ts(iqr_df$milk_production[1:(nrow(iqr_df) - n_holdout)],
                      frequency = 12))
iqr_forecast = forecast(auto.arima(iqr_ts), h = n_holdout)
none_ts = ts(sd_df$milk_production[1:(nrow(sd_df) - n_holdout)],
                      frequency = 12)
none_forecast = forecast(auto.arima(none_ts), h = n_holdout)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forecast_df = data.frame(anom_method = c(rep(&amp;quot;IQR&amp;quot;, n_holdout),
                                         rep(&amp;quot;SD&amp;quot;, n_holdout),
                                         rep(&amp;quot;None&amp;quot;, n_holdout)),
                         forecasted_amt = c(iqr_forecast$mean,
                                            sd_forecast$mean,
                                            none_forecast$mean),
                         actual_amt = rep(validation_set, 3)) %&amp;gt;% 
              mutate(residual_squared = (actual_amt - forecasted_amt)^2) %&amp;gt;% 
              group_by(anom_method) %&amp;gt;% 
              summarise(mse = mean(residual_squared)) %&amp;gt;% 
              mutate(anom_method = factor(anom_method)) %&amp;gt;% 
              mutate(anom_method = fct_reorder(anom_method, 
                                               mse, 
                                               .desc = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(forecast_df, aes(x = anom_method, y = round(mse, 0),
    fill = anom_method, label = as.character(round(mse, 0)))) + 
    geom_bar(stat = &amp;quot;identity&amp;quot;) + 
    theme_bw() + 
    my_plot_theme() + 
    scale_fill_manual(values = color_values[1:length(unique(forecast_df$anom_method))]) + 
    xlab(&amp;quot;Outlier Replacement Method&amp;quot;) + ylab(&amp;quot;Mean Square Error&amp;quot;) + 
    theme(legend.position = &amp;quot;none&amp;quot;) + 
    geom_label(label.size = 1, size = 10, color = &amp;quot;white&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/time_series_outlier_detection_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Replacing the outliers via the IQR method produced the most accurate 12-month ahead forecast. Now let’s examine the prediction intervals of each method, specifically the range as well as the coverage rate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pi_df = data.frame(anom_method = c(rep(&amp;quot;IQR&amp;quot;, n_holdout),
                                   rep(&amp;quot;SD&amp;quot;, n_holdout),
                                   rep(&amp;quot;None&amp;quot;, n_holdout)),
                   upper_bound = c(iqr_forecast$upper[,2],
                                   sd_forecast$upper[,2],
                                   none_forecast$upper[,2]),
                   lower_bound = c(iqr_forecast$lower[,2],
                                   sd_forecast$lower[,2],
                                   none_forecast$lower[,2]),
                   actual_amt = rep(validation_set, 3)) %&amp;gt;% 
        mutate(pi_range = upper_bound - lower_bound) %&amp;gt;% 
        mutate(actual_in_pi_range = as.integer(actual_amt &amp;lt; upper_bound &amp;amp; actual_amt &amp;gt; lower_bound))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(pi_df, aes(x = anom_method, y = pi_range, color = anom_method)) + 
    geom_boxplot() + 
    theme_bw() + 
    my_plot_theme() + 
    scale_color_manual(values = c(color_values[1:3])) + 
    xlab(&amp;quot;Outlier Replacement Method&amp;quot;) + ylab(&amp;quot;Prediction Interval Range&amp;quot;) + 
    theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/time_series_outlier_detection_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The median PI range is the narrowest for the IQR and the widest when we don’t replace any of the outliers. Finally let’s consider the coverage rate, which is how often the actual value fell within the monthly prediction interval.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coverage_df = pi_df %&amp;gt;% 
              group_by(anom_method) %&amp;gt;% 
              summarise(coverage_rate = round(sum(actual_in_pi_range)/12 * 100, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
anom_method
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
coverage_rate
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IQR
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
100.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
None
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
83.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SD
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
91.7
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The IQR and None approaches provided 100% coverage, while the SD method missed one month, yielding a coverage rate of ~92%.&lt;/p&gt;
&lt;p&gt;Taken together, the IQR method provided the most accurate forecasts and prediction intervals. While this is only one example, it is interesting to note that using the SD method actually reduced our forecasting accuracy and coverage rate below what would’ve happened had we not taken any steps to remedy our outliers prior to fitting a model. This really illustrates the value of using a robust method for anomaly detection.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exception Handling with Ron Burgundy</title>
      <link>/post/error_handling_markdown.html</link>
      <pubDate>Sun, 23 Jul 2017 21:13:14 -0500</pubDate>
      
      <guid>/post/error_handling_markdown.html</guid>
      <description>&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;Exception handling is a critical component of any data science workflow. You write code. It breaks. You create exceptions. Repeat. From my experience, a point of confusion for new R users is how to handle exceptions, which I believe is more intuitive in Python (at least initially). This post provides a practical overview of how to handle exceptions in R by first illustrating the concept in Python.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exception-handling-in-python&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exception Handling in Python&lt;/h3&gt;
&lt;p&gt;I’ve listed some of my favorite Ron Burgundy vocal warm-up phrases below. As a side note, all are excellent for preparing your speaking voice before a big speech.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;error_handling_images/burgundy_1.jpg&#34; width=&#34;500px&#34; height=&#34;500px&#34; /&gt;&lt;img src=&#34;/Users/mlebo1/Desktop/site/content/post/error_handling_images/burgundy_2.jpg&#34; width=&#34;500px&#34; height=&#34;500px&#34; /&gt;&lt;img src=&#34;error_handling_images/burgundy_3.jpg&#34; width=&#34;500px&#34; height=&#34;500px&#34; /&gt;&lt;img src=&#34;error_handling_images/burgundy_4.jpg&#34; width=&#34;500px&#34; height=&#34;500px&#34; /&gt;&lt;img src=&#34;error_handling_images/burgundy_5.jpg&#34; width=&#34;500px&#34; height=&#34;500px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s convert these into a list.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;strings = [&amp;quot;how now brown cow&amp;quot;, 
          &amp;quot;a tarantula enjoys a fine piece of chewing gum&amp;quot;, 
          &amp;quot;the human torch was denied a bank loan&amp;quot;,
          &amp;quot;the arsonist has oddly shaped feet&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our goal is to extract the 5th element from each phrase. We’ll use a &lt;code&gt;list comprehension&lt;/code&gt;, which maps or applies a function to each element in a list. The &lt;code&gt;extract_element&lt;/code&gt; function below takes 3 arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;strings - the string we want to split&lt;/li&gt;
&lt;li&gt;split_character - the character we want to split our strings on (in this case a space “ “)&lt;/li&gt;
&lt;li&gt;element_index - the index of the element we want to extract after splitting the string&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def extract_element(strings, split_character, element_index):
  return(strings.split(split_character)[element_index])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what we’ll get if we want only the first word:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print([extract_element(x, &amp;quot; &amp;quot;, 0) for x in strings])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;## [&amp;#39;how&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;the&amp;#39;, &amp;#39;the&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! Let’s extract the fifth word.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print([extract_element(x, &amp;quot; &amp;quot;, 4) for x in strings])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;## IndexError: list index out of range&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Uh oh. This error message indicates that we tried an index value that was out of range, which makes sense: “how now brown cow” only has 4 words. In order to address this issue, let’s add two keywords to our function – &lt;code&gt;try&lt;/code&gt; and &lt;code&gt;except&lt;/code&gt;. Just like it sounds, the function will first try to execute the top code block. If an exception occurs, the function will pass control to the bottom code block. Let’s update the above function with our new, exception-handling logic and try again.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def extract_element_tc(strings, split_character, element_index):
    try:
        return(strings.split(split_character)[element_index])
    except IndexError:
        return(&amp;quot;NO STRING HERE&amp;quot;)
        
print([extract_element_tc(x, &amp;quot; &amp;quot;, 4) for x in strings])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;## [&amp;#39;NO STRING HERE&amp;#39;, &amp;#39;fine&amp;#39;, &amp;#39;denied&amp;#39;, &amp;#39;shaped&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time it worked without an issue. We simply said,“if there isn’t an element at this index, give me back a ‘NO STRING HERE’ label”. Now let’s achieve the same result in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exception-handling-in-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exception Handling in R&lt;/h3&gt;
&lt;p&gt;As I mentioned in the intro, dealing with exceptions in R can be a bit tricky at first. Let’s set up the previous task in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr)

strings = c(&amp;quot;how now brown cow&amp;quot;, 
          &amp;quot;a tarantula enjoys a fine piece of chewing gum&amp;quot;, 
          &amp;quot;the human torch was denied a bank loan&amp;quot;,
          &amp;quot;the arsonist has oddly shaped feet&amp;quot;)

extract_element = function(strings, split_character, element_index){
  string_out = purrr::map_chr(strsplit(strings, split_character),
                              element_index
                              )
  return(string_out)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll approach this via the &lt;code&gt;sapply&lt;/code&gt; function, which maps or applies a function to each element of a list or vector. We’ll also wrap the output of &lt;code&gt;sapply&lt;/code&gt; with the &lt;code&gt;unname&lt;/code&gt; function, which converts our list into a vector by removing the labels associated with each parsed string.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(unname(sapply(strings, extract_element, &amp;quot; &amp;quot;, 1)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;how&amp;quot; &amp;quot;a&amp;quot;   &amp;quot;the&amp;quot; &amp;quot;the&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I think we know what happens next…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(unname(sapply(strings, extract_element, &amp;quot; &amp;quot;, 5)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Error: Result 1 is not a length 1 atomic vector &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok. So how do we replicate the &lt;code&gt;try-except&lt;/code&gt; control flow in R? Via the &lt;code&gt;tryCatch&lt;/code&gt; function. Let’s examine the complete function – &lt;code&gt;extract_element_tc&lt;/code&gt; – in order to deal with the problem stated above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_element_tc = function(strings_vector, split_character, element_index){
  string_out = tryCatch({
    return(purrr::map_chr(strsplit(strings_vector, split_character), 
                          element_index
                          )
          )
  },
  error = function(cond){
    return(&amp;quot;NO STRING HERE&amp;quot;)
  })
}

print(unname(sapply(strings, extract_element_tc, &amp;quot; &amp;quot;, 5)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## [1] &amp;quot;NO STRING HERE&amp;quot; &amp;quot;fine&amp;quot;           &amp;quot;denied&amp;quot;         &amp;quot;shaped&amp;quot; &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Boom. No errors this time. R is different in that tryCatch will have separate functions, referred to as handlers, that get called depending on the condition that occurs. In this case, we have 2 conditions– our desired condition, and the error condition. There are additional conditions that can be handled (see &lt;a href=&#34;http://adv-r.had.co.nz/Exceptions-Debugging.html&#34;&gt;here&lt;/a&gt; for more detail). For example, you can also include a handler for &lt;code&gt;warnings&lt;/code&gt; as well, which can be extremely helpful for finding exceptions that don’t break your code but instead give you undesired results.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Early Trend Detection</title>
      <link>/post/early_trend_detection.html</link>
      <pubDate>Sat, 24 Jun 2017 21:13:14 -0500</pubDate>
      
      <guid>/post/early_trend_detection.html</guid>
      <description>&lt;p&gt;&lt;img src=&#34;early_trend_detection_images/stock_exchange.jpg&#34; width=&#34;800px&#34; height=&#34;800px&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;Trend detection is way to anticipate where something will go in the future. The logic underlying the inference is simple: Are there patterns of activity that precede an upward or downward trend? For example, a fashion trend might start in a big city like New York, London, or Paris. The emergence of the trend might then occur at a later date in smaller nearby cities. Thus, if you were charged with anticipating future demand for the trending product, you could use the early signal from the big cities to anticipate demand in the smaller cities. This information could inform everything from design to demand planning (i.e., getting the right amount of product in the right place). This is often referred to as a &lt;strong&gt;leading indicator&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For this approach to be effective, there has to be a “common way” in which a something trends. Maybe it’s a gradual increase over the first five days, followed by a rapid increase over the next 10 days. Or maybe you just need the first three days to make an accurate prediction of a positive trend when the change in demand is very rapid. There are many ways in which something can trend, but the pattern of demand, tweets, views, purchases, likes, etc. for a positive trend has to be different from a negative or flat trend.&lt;/p&gt;
&lt;p&gt;This post outlines one approach to separate the signatures associated with different types of trends. We’ll use stock data from Fortune 500 companies as our analytical dataset to predict if a stock’s price will trend positively based on its closing price over the first 30 days in Jan/Feb of 2015? Before jumping into the example, though, I’ll briefly introduce the technique with a smaller dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;non-parametric-trend-detection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Non-Parametric Trend Detection&lt;/h3&gt;
&lt;p&gt;We’ll use a K-Nearest-Neighbor (KNN) approach for classifying where a stock’s price trend in the future (i.e., up or flat/down). KNN is non-parametric, which means we don’t make any assumptions about our data. There is also no ‘model’; we simply let the data make the classification or prediction. Without generalizing too much, I’ve liked KNN when my feature set is small and consists mostly of continuous variables. It is easy to explain and surprisingly effective relative to more complicated, parametric models. All we have to do is find the right “K”, which is the number of neighbors we use to make our prediction/classification. “K” is determined through cross-validation, where we try several values and see which gives the best results.&lt;/p&gt;
&lt;p&gt;So how do we determine our neighbors? There are many distance metrics used to define “similarity”, the most common being Euclidean Distance. However, the main drawback to this approach is that it cannot account for forward or backward shifts between series, which is an issue when dealing with sequential data. Thus we’ll use &lt;strong&gt;Dynamic Time Warping&lt;/strong&gt;, which can account for temporary shifts, leads, or lags.&lt;/p&gt;
&lt;p&gt;We’ll use 10 fictitious stocks in our initial example. Five will trend positively after their first 50 days while the remaining five will trend negatively over the same time period. These stocks will serve as our “training set”. We’ll then take a new stock for which we only have the first 50 days and classify it as either a “positive” or “negative” trending stock. This prediction is where we think the stock will go relative to its position at day 50.&lt;/p&gt;
&lt;p&gt;Let’s first generate the initial 50 days for our 10 stocks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs = c(&amp;#39;BatchGetSymbols&amp;#39;, &amp;#39;rvest&amp;#39;, &amp;#39;Kendall&amp;#39;, 
         &amp;#39;dplyr&amp;#39;,&amp;#39;openair&amp;#39;, &amp;#39;broom&amp;#39;,
         &amp;#39;MarketMatching&amp;#39;, &amp;#39;ggplot2&amp;#39;, &amp;#39;knitr&amp;#39;,
         &amp;#39;ggforce&amp;#39;, &amp;#39;rvest&amp;#39;, &amp;#39;xml2&amp;#39;,
         &amp;#39;janitor&amp;#39;
         )
lapply(libs, require, character.only = TRUE)
set.seed(123)
pre_date_range = seq.Date(as.Date(&amp;quot;2016-01-01&amp;quot;), 
                          by = &amp;quot;day&amp;quot;, 
                          length = 100) 
pre_period = 50
trend_patterns = data.frame()
for(i in 1:5){
  # generate pattern for stocks that trend negatively from days 51-100
  neg_trend_pre = cos(seq(0, 10, 0.1))[1:pre_period] + rnorm(pre_period, 0, 0.7)
  trend_patterns = bind_rows(trend_patterns,
                         data.frame(date = pre_date_range[1:pre_period],
                                    value = neg_trend_pre,
                                    class = paste0(&amp;#39;negative_&amp;#39;, i)))
  
  # generate pattern for stocks that trend positively from days 51-100
  pos_trend_pre = cos(seq(0, 10, 0.1))[(pre_period + 1):100] + rnorm(pre_period, 0, 0.7)
  trend_patterns = bind_rows(trend_patterns,
                        data.frame(date = pre_date_range[1:pre_period],
                                          value = pos_trend_pre,
                                          class = paste0(&amp;quot;positive_&amp;quot;, i)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s add in our positive and negative trends.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;generate_trend = function(trend_length, increment, sd, base_value){
    trend_vector = c()
    for(j in 1:trend_length){
        trend_vector = c(trend_vector, rnorm(1, 
                                             base_value, 
                                             sd = sd))
        base_value = base_value + increment
    }
    return(trend_vector)
}

increment = 0.03
standard_dev = 0.08
trend_patterns_future = data.frame(NULL)
for(temp_trend in unique(trend_patterns$class)){
    temp_ts = trend_patterns %&amp;gt;% 
              filter(class == temp_trend)
    # extend date 50 days forward
    future_dates = pre_date_range[51:length(pre_date_range)]
    base_value = temp_ts$value[length(temp_ts$value)]
    future_trend = generate_trend(length(future_dates), 
                                  increment, 
                                  standard_dev, 
                                  base_value)
    if(strsplit(temp_trend, &amp;quot;_&amp;quot;)[[1]][1] == &amp;#39;negative&amp;#39;){
        future_trend = future_trend * -1
        }
    trend_patterns_future = bind_rows(trend_patterns_future,
                                      data.frame(date = future_dates,
                                                 value = future_trend,
                                                 class = temp_trend)
                                  )
}

trend_patterns_ex = bind_rows(trend_patterns, 
                              trend_patterns_future) %&amp;gt;% 
                              arrange(class, date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And finally, we’ll set up our plotting function and plot the five positive and five negative trends. Additionally, we’ll look at the first 50 days to see if there is a pattern associated with positive/negative trends. The gray shaded area indicates the point at which we make our prediction regarding where we think the stock will go.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_plot_theme = function(){
    font_family = &amp;quot;Helvetica&amp;quot;
    font_face = &amp;quot;bold&amp;quot;
    return(theme(
    axis.text.x = element_text(size = 18, face = font_face, family = font_family),
    axis.text.y = element_text(size = 18, face = font_face, family = font_family),
    axis.title.x = element_text(size = 20, face = font_face, family = font_family),
    axis.title.y = element_text(size = 20, face = font_face, family = font_family),
    strip.text.y = element_text(size = 18, face = font_face, family = font_family),
    plot.title = element_text(size = 18, face = font_face, family = font_family),
    legend.position = &amp;quot;top&amp;quot;,
    legend.title = element_text(size = 16,
    face = font_face,
    family = font_family),
    legend.text = element_text(size = 14,
    face = font_face,
    family = font_family)
))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plotting colors
plot_colors = c(&amp;quot;#33FFFB&amp;quot;,&amp;quot;#33D6FF&amp;quot;,
                &amp;quot;#33BDFF&amp;quot;,&amp;quot;#3388FF&amp;quot;,
                &amp;quot;#33EAFF&amp;quot;,&amp;quot;#FF3371&amp;quot;,
               &amp;quot;#FF335C&amp;quot;,&amp;quot;#FF334C&amp;quot;,
               &amp;quot;#FF4833&amp;quot;,&amp;quot;#FF5833&amp;quot;)

ggplot(trend_patterns_ex, aes(x = date, y = value, color = class)) + 
    geom_point(alpha = 0.2) + stat_smooth(se = FALSE) + 
    scale_color_manual(values = plot_colors) + 
    theme_bw() + 
    my_plot_theme() + 
    ylab(&amp;quot;Stock Price&amp;quot;) + xlab(&amp;quot;Date&amp;quot;) + 
    facet_zoom(x = date %in% pre_date_range[1:pre_period])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/early_trend_detection_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, stocks that initially go up, drift down, then flatten out all trend positively. In contrast, stocks that decline and then increase from days 30-50 trend downward from days 51-100. Thus, there is a common pattern that precedes a sustained increase or decrease in price. Based on this information, let’s introduce our new stock – the “target trend” – that we only have pricing information for days 1-50.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_trend_pre = bind_rows(data.frame(date = pre_date_range[1:pre_period],
                                        value = cos(seq(0, 10, 0.1))[(pre_period + 1):100] + rnorm(pre_period, 0, 0.7),
                                        class = &amp;quot;target_trend&amp;quot;),
                             data.frame(date = rep(NA, pre_period),
                                        value = rep(NA, pre_period),
                                        class = &amp;quot;target_trend&amp;quot;)
                             )
trend_patterns_ex = bind_rows(sample_trend_pre, 
                              trend_patterns_ex)             &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(trend_patterns_ex, aes(x = date, y = value, color = class)) + 
    geom_point(alpha = 0.1) + geom_line(stat = &amp;quot;smooth&amp;quot;,
                                        method = &amp;quot;gam&amp;quot;,
                                        formula = y ~ s(x),
                                        size = 2,
                                        alpha = 0.2
                                        ) + 
    scale_color_manual(values = c(&amp;quot;black&amp;quot;, plot_colors)) + 
    theme_bw() + 
    my_plot_theme() + 
    stat_smooth(data = sample_trend_pre, 
                aes(x = date, y = value), 
                color = &amp;quot;black&amp;quot;,
                se = FALSE, 
                size = 3, 
                lty = 4) + 
    theme(legend.position = &amp;quot;none&amp;quot;) + 
    ylab(&amp;quot;Stock Price&amp;quot;) + 
    facet_zoom(x = date %in% pre_date_range[1:pre_period])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/early_trend_detection_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s filter our dataset to days 1-50 and see how many of the nearest neighbors for our target trend are positive/negative. In this case &lt;em&gt;K&lt;/em&gt; is set to 5.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trend_patterns_test_time = trend_patterns_ex %&amp;gt;% 
                           filter(date &amp;lt;= as.Date(&amp;quot;2016-02-19&amp;quot;)) %&amp;gt;% 
                           mutate(class = as.character(class))

trend_pattern_matches = best_matches(data = trend_patterns_test_time,
                                     id_variable = &amp;quot;class&amp;quot;,
                                     date_variable = &amp;quot;date&amp;quot;,
                                     matching_variable = &amp;quot;value&amp;quot;,
                                     parallel = TRUE,
                                     warping_limit = 3,
                                     matches = 5,
                                     start_match_period = min(trend_patterns_test_time$date),
                                     end_match_period = max(trend_patterns_test_time$date)
)

best_matches = trend_pattern_matches$BestMatches %&amp;gt;% 
               filter(class == &amp;#39;target_trend&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:270px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
class
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
BestControl
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
RelativeDistance
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Correlation
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Length
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
rank
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
MatchingStartDate
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
MatchingEndDate
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
target_trend
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive_2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.234196
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.6145986
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2016-01-01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2016-02-19
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
target_trend
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive_4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.375498
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.5205792
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2016-01-01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2016-02-19
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
target_trend
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive_3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.394093
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.5089169
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2016-01-01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2016-02-19
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
target_trend
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive_1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.408172
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.5831835
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2016-01-01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2016-02-19
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
target_trend
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive_5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.453815
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4916183
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2016-01-01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2016-02-19
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let’s focus on the &lt;strong&gt;BestControl&lt;/strong&gt; field. All five of the most similar time-series to our target time-series have a positive label. Thus we would predict that our target time-series will trend positively from days 51-100, based on its pattern from days 1-50. If all five of our best matches were stocks that negatively trended, then we would make the opposite prediction. I told you this method is simple (and effective)! However, let’s see if how we feel the same way after working with real-world stock data, introduced in the next section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predicting-stocks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Predicting Stocks&lt;/h3&gt;
&lt;p&gt;We are trying to answer the following question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Is there a pattern of daily closing prices that comes before a ~90-day increase in the price of a stock?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, maybe a small increase in the first 60 days is often followed by a big 60-day upward climb. Or maybe stocks that gradually dip for a period of 60 days are more likely to rebound over the subsequent 60 days. In essence, are there a collection of 60-day patterns that come before an upward trend in a stock’s price? Trying to formalize such patterns with a set of parameters is difficult, given the variety of patterns that could be associated with a positive trend. This is why we are using the approach outlined above and letting the data decide.&lt;/p&gt;
&lt;p&gt;We’ll begin by scraping the names of all Fortune 500 companies from Wikipedia, and then pull the actual closing prices for each company via the &lt;code&gt;BatchGetSymbols&lt;/code&gt; package. This will take a few minutes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;url = &amp;quot;https://en.wikipedia.org/wiki/List_of_S%26P_500_companies&amp;quot;

# collect fortunue 500 stock symbols
stock_names = url %&amp;gt;%
       read_html() %&amp;gt;%
       html_nodes(xpath=&amp;#39;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[1]&amp;#39;) %&amp;gt;%
       html_table() %&amp;gt;% 
       pull(Ticker.symbol)

train_period_begin = as.Date(&amp;quot;2015-01-01&amp;quot;)
train_period_end = max(seq(train_period_begin, by = &amp;quot;day&amp;quot;, length.out = 200))
# collect actual closing prices for stocks
stock_price = BatchGetSymbols(tickers = stock_names,
                              first.date = train_period_begin,
                              last.date = train_period_end
                              )$df.tickers %&amp;gt;% 
              clean_names() %&amp;gt;% 
              select(ref_date, ticker, price_close)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:400px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
ticker
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
ref_date
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
price_close
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MMM
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
164.06
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MMM
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
160.36
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MMM
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
158.65
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MMM
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
159.80
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MMM
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
163.63
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MMM
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
161.62
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MMM
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-12
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
160.74
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MMM
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-13
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
160.62
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MMM
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-14
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
159.84
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MMM
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-15
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
159.66
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Data looks ready to go! Let’s select the stocks that will comprise our training/test sets. We’ll do a 90⁄10 split. Note that the test stocks will be used a little later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
n_days_train = 60
n_day_outcome = 60
pct_stocks_in_test = 0.1
# select stocks that will be in the training and test sets
test_stocks = sample(stock_names, 
                     size = floor(length(unique(stock_price$ticker)) * pct_stocks_in_test), 
                     replace = FALSE)

train_stocks = setdiff(stock_price$ticker, test_stocks)
# select first 30 days of 2015 as feature set
input_dates = unique(stock_price$ref_date)[1:n_days_train]
# select next 90 days as trend outcome 
trend_dates = setdiff(unique(stock_price$ref_date), input_dates)[1:n_day_outcome]
stocks_train = stock_price %&amp;gt;% 
               filter(ticker %in% train_stocks) %&amp;gt;% 
               mutate(label = ifelse(ref_date %in% input_dates, 
                                     &amp;quot;input_dates&amp;quot;,
                                     &amp;quot;trend_dates&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have our data and it’s all cleaned up, let’s discuss how we’ll determine the labels for our “positive” and “flat|negative” time series.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;defining-a-trend&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Defining a Trend&lt;/h3&gt;
&lt;p&gt;We could plot out every time series and label them by hand but &lt;a href=&#34;https://www.youtube.com/watch?v=zGxwbhkDjZM&#34;&gt;ain’t nobody got time for that&lt;/a&gt;. Instead, we’ll use two automated approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linear Regression (LR)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mann-Kendall test (MK)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Labels will be generated based on the trend observed from days 60-120 and will be either &lt;strong&gt;positive&lt;/strong&gt;, &lt;strong&gt;negative&lt;/strong&gt;, or &lt;strong&gt;neutral&lt;/strong&gt;. We are interested in whether the coefficient for this index is significant as well as its overall direction. For example, if a stock exhibits a significant positive trend, then it will be labeled positive; if a stock exhibits random variation, then it will be labeled neutral. LR takes the average slope, while the MK approach takes the median slope. Note that we might be violating the assumption that our data points are uncorrelated with one another. There are ways around this (e.g., block bootstrapping), but for the sake of simplicity, we’ll assume an absence of serial correlation. Let’s run both tests and gather the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# use mann-kendall to determine class 
mk_class = stocks_train %&amp;gt;% 
              filter(label == &amp;quot;trend_dates&amp;quot;) %&amp;gt;% 
              group_by(ticker) %&amp;gt;% 
              do(trend_data = MannKendall(ts(.$price_close))) %&amp;gt;% 
              data.frame()

mk_class$tau = unlist(lapply(mk_class$trend_data,
                                function(x) x[[1]][1]))
mk_class$p_value = unlist(lapply(mk_class$trend_data,
                                    function(x) x[[2]][1]))

ELSE = TRUE
mk_class = mk_class %&amp;gt;%
           select(ticker, tau, p_value) %&amp;gt;% 
           mutate(trend_class_mk = case_when(tau &amp;gt; 0 &amp;amp; p_value &amp;lt; 0.01 ~ &amp;quot;positive&amp;quot;,
                                   tau &amp;lt; 0 &amp;amp; p_value &amp;lt; 0.01 ~ &amp;quot;negative&amp;quot;,
                                   ELSE ~ &amp;quot;neutral&amp;quot;
                                    )) %&amp;gt;% 
           select(ticker, trend_class_mk)

# use linear regression to determine class
lm_class = stocks_train %&amp;gt;%
  filter(label == &amp;quot;trend_dates&amp;quot;) %&amp;gt;% 
  group_by(ticker) %&amp;gt;%
  mutate(day_index = row_number()) %&amp;gt;%
  do(tidy(lm(price_close ~ day_index, data = .))) %&amp;gt;% 
  filter(term == &amp;#39;day_index&amp;#39;) %&amp;gt;%
  mutate(trend_class_lm = case_when(p.value &amp;lt; 0.01 &amp;amp; estimate &amp;gt; 0 ~ &amp;quot;positive&amp;quot;,
                                    p.value &amp;lt; 0.01 &amp;amp; estimate &amp;lt; 0 ~ &amp;quot;negative&amp;quot;,
                                    ELSE ~ &amp;quot;neutral&amp;quot;
                                    )) %&amp;gt;% 
  select(ticker, trend_class_lm)

trend_class = inner_join(mk_class,
                         lm_class,
                         on = &amp;quot;ticker&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:400px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
ticker
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
trend_class_mk
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
trend_class_lm
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
A
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
negative
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
AAL
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
negative
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
AAP
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
AAPL
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
neutral
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
neutral
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
ABBV
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
ABC
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
negative
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
ABT
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
ACN
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
ADBE
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
ADI
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
neutral
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
neutral
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The two methods produced the same classification for most of the stocks. However, let’s examine a few cases where they differ.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stocks_train %&amp;gt;% 
filter(ticker == &amp;#39;ADI&amp;#39; &amp;amp; label == &amp;#39;trend_dates&amp;#39;) %&amp;gt;% 
mutate(ref_date = as.Date(ref_date)) %&amp;gt;% 
ggplot(aes(x = ref_date, y = price_close)) + 
      geom_point(size = 2, color = &amp;quot;black&amp;quot;) + 
      geom_line(size = 2, color = &amp;quot;black&amp;quot;) + 
      theme_bw() + 
      my_plot_theme() + 
      ylab(&amp;quot;Closing Price&amp;quot;) + 
      xlab(&amp;quot;Date&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/early_trend_detection_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The price for this stock goes up but then trends downward. The LR indicates a positive trend while the MK test indicates a neutral trend. The LR is taking the average, and the average is positive overall, given that the stock showed big gains in June and July. In contrast, the MK tests whether there is a monotonically increasing or decreasing trend over time. It is considering the ratio of positive and negative differences across time. Since we went up and then back down, there will be a similar number of positive/negative differences, hence the neutral classification.&lt;/p&gt;
&lt;p&gt;Let’s consider a separate example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stocks_train %&amp;gt;% 
filter(ticker == &amp;#39;NI&amp;#39; &amp;amp; label == &amp;#39;trend_dates&amp;#39;) %&amp;gt;% 
mutate(ref_date = as.Date(ref_date)) %&amp;gt;% 
ggplot(aes(x = ref_date, y = price_close)) + 
      geom_point(size = 2, color = &amp;quot;black&amp;quot;) + 
      geom_line(size = 2, color = &amp;quot;black&amp;quot;) + 
      theme_bw() + 
      my_plot_theme() + 
      ylab(&amp;quot;Closing Price&amp;quot;) + 
      xlab(&amp;quot;Date&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/early_trend_detection_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This time series produced the exact oppositive pattern: The MK test classified the trend as neutral, while the LR produced a negative classification. The stock trended slightly upward for most of the time-period and then crashed the last few days in July. Those final few points have a lot of leverage on the coefficient in the linear model, while the MK test is relatively unaffected. In this case, I’d say it’s a negatively trending stock, but arguments for either classification could be made.&lt;/p&gt;
&lt;p&gt;Stocks such as these are why we are evaluating two separate viewpoints. We want to ensure that the trend is somewhat definitive. If there is disagreement amongst the methods, we are going to assign a stock the “neutral” label. In a more formal analytical setting, we would spend more time testing whether this labeling approach is valid. However, for the sake of illustration, we’re going to use a simple, heuristic-based approach, such that any classification disagreements amongst the two methods will result in a neutral classification. Let’s update all of the instances where a disagreement occurred, and then filter out all of the neutral cases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trend_class = trend_class %&amp;gt;% 
              mutate(trend_class = ifelse(trend_class_mk != trend_class_lm, 
                                         &amp;quot;neutral&amp;quot;,
                                         trend_class_mk)) %&amp;gt;% 
              select(ticker, trend_class) %&amp;gt;% 
              filter(trend_class != &amp;quot;neutral&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let’s examine the distribution of our classes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(table(trend_class$trend_class))
print(&amp;quot;----------------------------------&amp;quot;)
print(paste0(&amp;quot;PERCENT OF STOCKS POSITIVE: &amp;quot;,
      round(table(trend_class$trend_class)[[2]]/nrow(trend_class) * 100, 0),
      &amp;quot;%&amp;quot;)
      )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 
## negative positive 
##      195      150 
## [1] &amp;quot;----------------------------------&amp;quot;
## [1] &amp;quot;PERCENT OF STOCKS POSITIVE: 43%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are more time series in negative class relative to the positive class. This isn’t a huge imbalance, but it might skew our classifications toward the negative class, as there are simply more series to match on. Let’s downsample our negative stocks so we have an equal number of positive and negative classes in the training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
trend_class = trend_class %&amp;gt;% 
              filter(trend_class == &amp;quot;negative&amp;quot;) %&amp;gt;% 
              sample_n(trend_class %&amp;gt;% 
                       filter(trend_class == &amp;quot;positive&amp;quot;) %&amp;gt;% 
                       nrow()
                         ) %&amp;gt;% 
              bind_rows(trend_class %&amp;gt;% 
                        filter(trend_class == &amp;quot;positive&amp;quot;))
                       
print(table(trend_class$trend_class))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## negative positive 
##      152      152&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(&amp;quot;----------------------------------&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;----------------------------------&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste0(&amp;quot;PERCENT OF STOCKS POSITIVE: &amp;quot;,round(table(trend_class$trend_class)[[2]]/nrow(trend_class) * 100, 0), &amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;PERCENT OF STOCKS POSITIVE: 50%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 
## negative positive 
##     150      150 
## [1] &amp;quot;----------------------------------&amp;quot;
## [1] &amp;quot;PERCENT OF STOCKS POSITIVE: 50%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perfect. Now we have even numbers and are ready to test out the trend detector.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trend-detection-performance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Trend Detection Performance&lt;/h3&gt;
&lt;p&gt;Now that we’ve assigned a label to each of the stocks in our training set, we’ll download the closing prices for the test stocks, which are from the following year during the same time period (i.e., Jan/Feb 2016).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_period_begin = as.Date(&amp;quot;2016-01-01&amp;quot;)
test_period_end = max(seq(test_period_begin, 
                          by = &amp;quot;day&amp;quot;, 
                          length.out = 200))

stocks_test = BatchGetSymbols(tickers = test_stocks,
                              first.date = test_period_begin,
                              last.date = test_period_end)$df.tickers %&amp;gt;% 
                   clean_names() %&amp;gt;% 
                   select(ref_date, ticker, price_close)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Running BatchGetSymbols for:
##    tickers = DLPH, REGN, FTV, TWX, VRSN, AGN, JBHT, TJX, KMI, HRS, VZ, HAL, NEM, LH, AIV, TXT, CMS, ALGN, DPS, URI, SYF, NWSA, MET, WMT, MSFT, NI, IQV, LEG, CSX, BDX, UA, SRCL, MYL, PPL, AMG, HBI, PCAR, CERN, DVN, CMG, BK, FAST, FB, EW, VIAB, BHGE, CVX, GE, KO
##    Downloading data for benchmark ticker
## Downloading Data for DLPH from yahoo (1|49) - Nice!
## Downloading Data for REGN from yahoo (2|49) - Got it!
## Downloading Data for FTV from yahoo (3|49) - OK!
## Downloading Data for TWX from yahoo (4|49) - Nice!
## Downloading Data for VRSN from yahoo (5|49) - Good job!
## Downloading Data for AGN from yahoo (6|49) - Got it!
## Downloading Data for JBHT from yahoo (7|49) - Good stuff!
## Downloading Data for TJX from yahoo (8|49) - Got it!
## Downloading Data for KMI from yahoo (9|49) - Good stuff!
## Downloading Data for HRS from yahoo (10|49) - Nice!
## Downloading Data for VZ from yahoo (11|49) - Good job!
## Downloading Data for HAL from yahoo (12|49) - Nice!
## Downloading Data for NEM from yahoo (13|49) - Nice!
## Downloading Data for LH from yahoo (14|49) - Good stuff!
## Downloading Data for AIV from yahoo (15|49) - Well done!
## Downloading Data for TXT from yahoo (16|49) - Got it!
## Downloading Data for CMS from yahoo (17|49) - Nice!
## Downloading Data for ALGN from yahoo (18|49) - Got it!
## Downloading Data for DPS from yahoo (19|49) - Good job!
## Downloading Data for URI from yahoo (20|49) - Got it!
## Downloading Data for SYF from yahoo (21|49) - Boa!
## Downloading Data for NWSA from yahoo (22|49) - Well done!
## Downloading Data for MET from yahoo (23|49) - Good job!
## Downloading Data for WMT from yahoo (24|49) - Good job!
## Downloading Data for MSFT from yahoo (25|49) - Nice!
## Downloading Data for NI from yahoo (26|49) - Good stuff!
## Downloading Data for IQV from yahoo (27|49) - Boa!
## Downloading Data for LEG from yahoo (28|49) - Good job!
## Downloading Data for CSX from yahoo (29|49) - Well done!
## Downloading Data for BDX from yahoo (30|49) - Nice!
## Downloading Data for UA from yahoo (31|49) - Good job!
## Downloading Data for SRCL from yahoo (32|49) - Got it!
## Downloading Data for MYL from yahoo (33|49) - Good job!
## Downloading Data for PPL from yahoo (34|49) - Good stuff!
## Downloading Data for AMG from yahoo (35|49) - Got it!
## Downloading Data for HBI from yahoo (36|49) - Good stuff!
## Downloading Data for PCAR from yahoo (37|49) - Boa!
## Downloading Data for CERN from yahoo (38|49) - Boa!
## Downloading Data for DVN from yahoo (39|49) - Got it!
## Downloading Data for CMG from yahoo (40|49) - Nice!
## Downloading Data for BK from yahoo (41|49) - Boa!
## Downloading Data for FAST from yahoo (42|49) - Good job!
## Downloading Data for FB from yahoo (43|49) - Boa!
## Downloading Data for EW from yahoo (44|49) - Good stuff!
## Downloading Data for VIAB from yahoo (45|49) - Nice!
## Downloading Data for BHGE from yahoo (46|49) - Good job!
## Downloading Data for CVX from yahoo (47|49) - Got it!
## Downloading Data for GE from yahoo (48|49) - Good job!
## Downloading Data for KO from yahoo (49|49) - Got it!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s union our test and training datasets together. We’ll also change the time-stamps in our testing data set, despite the fact that our training data is from 2015 and our testing data is from 2016.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stocks_train_match = stocks_train %&amp;gt;% 
  inner_join(trend_class) %&amp;gt;% 
  filter(ref_date %in% input_dates)

stocks_test_match = stocks_test %&amp;gt;% 
  filter(ref_date %in% unique(stocks_test$ref_date)[1:n_days_train]) %&amp;gt;% 
  mutate(ref_date = rep(unique(stocks_train_match$ref_date), 
                        length(unique(stocks_test$ticker)))) %&amp;gt;% 
  mutate(label = &amp;#39;test_dates&amp;#39;,
         trend_class = &amp;#39;unknown&amp;#39;
         )

matching_df = bind_rows(stocks_train_match,
                        stocks_test_match
                        ) %&amp;gt;% 
              mutate(ref_date = as.Date(ref_date))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to initially set K to 10. The reason why we’re using such a high number is that some of the test stocks will match with other test stocks. Obviously, these won’t have labels, so we only want to consider matches with training stocks. This section will take a few minutes to run.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max_matches = 10
best_match_df = best_matches(data = matching_df,
                             id_variable = &amp;quot;ticker&amp;quot;,
                             date_variable = &amp;quot;ref_date&amp;quot;,
                             matching_variable = &amp;quot;price_close&amp;quot;,
                             parallel = TRUE,
                             warping_limit = 3,
                             matches = max_matches,
                             start_match_period = min(matching_df$ref_date),
                             end_match_period = max(matching_df$ref_date))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we’ll remove any matches that appeared in the test set and filter only to those stocks we are trying to predict.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_matches = 5
test_matches = best_match_df$BestMatches %&amp;gt;% 
               filter(ticker %in% stocks_test$ticker) %&amp;gt;% 
               filter(!(BestControl %in% stocks_test$ticker)) %&amp;gt;% 
               group_by(ticker) %&amp;gt;% 
               top_n(-n_matches, rank) %&amp;gt;% 
               dplyr::rename(test_stock = ticker,
                             ticker = BestControl) %&amp;gt;% 
               left_join(trend_class) %&amp;gt;% 
               data.frame()&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:400px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
test_stock
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
ticker
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
RelativeDistance
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Correlation
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Length
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
rank
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
MatchingStartDate
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
MatchingEndDate
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
trend_class
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
AGN
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
SHW
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0602364
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.4176124
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-03-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
AGN
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MTD
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.1413700
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.5561973
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-03-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
AGN
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
V
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.2732732
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.3993992
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-03-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
AGN
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
PPG
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.3660257
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0625775
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-03-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
AGN
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
EQIX
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.3835526
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.2642403
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-03-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
AIV
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
BBT
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0584007
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.1862317
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-03-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
AIV
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NWL
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0619271
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.1978514
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-03-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
AIV
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
BBY
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0732311
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.2444824
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-03-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
AIV
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
SYY
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0737522
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.3448536
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-03-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
AIV
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
IRM
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0765316
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.1706778
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-01-02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2015-03-30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;This readout has the same interpretation as the initial example. The stocks in our &lt;code&gt;ticker&lt;/code&gt; column are in the test set, while those in the &lt;code&gt;BestControl&lt;/code&gt; column are in our training set. Given that we are interested in whether a stock is going to trend upward or downward (i.e., is this a stock we should buy/sell?), the focus on is on &lt;strong&gt;accuracy &lt;/strong&gt; – or how many stocks we classify correctly. We’ll use four matches as our cutoff; that is, if at least four of the five matches in our training dataset belong to the same class, then we’ll predict that class. All others with less than four matches won’t be considered in the accuracy calculation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_matches = 4
predicted_class = test_matches %&amp;gt;% 
                  mutate(trend_class = ifelse(trend_class == &amp;#39;positive&amp;#39;, 1, 0)) %&amp;gt;% 
                  group_by(test_stock) %&amp;gt;% 
                  summarise(total_pos_class = sum(trend_class)) %&amp;gt;% 
                  mutate(class_pred = case_when(total_pos_class &amp;gt;= 4 ~ &amp;#39;positive&amp;#39;,
                                                total_pos_class &amp;lt;= 1 ~ &amp;#39;negative&amp;#39;,
                                                ELSE ~ &amp;#39;neutral&amp;#39;
                                                )) %&amp;gt;% 
                  filter(class_pred != &amp;#39;neutral&amp;#39;) %&amp;gt;% 
                  data.frame()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After eliminating all of the “neutral” classifications, there are 20 stocks remaining. Out of these, what percentage were correctly classified as positive or negative?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_dates = unique(stocks_test$ref_date)[n_days_train:(n_days_train + n_day_outcome - 1)]
trend_class_test = stocks_test %&amp;gt;% 
                   filter(ref_date %in% test_dates) %&amp;gt;% 
                   filter(ticker %in% predicted_class$test_stock)
# use mann-kendall to determine class 
mk_class = trend_class_test %&amp;gt;% 
  group_by(ticker) %&amp;gt;% 
  do(trend_data = MannKendall(ts(.$price_close))) %&amp;gt;% 
  data.frame()

mk_class$tau = unlist(lapply(mk_class$trend_data,
                             function(x) x[[1]][1]))
mk_class$p_value = unlist(lapply(mk_class$trend_data,
                                 function(x) x[[2]][1]))

mk_class = mk_class %&amp;gt;%
  select(ticker, tau, p_value) %&amp;gt;% 
  mutate(trend_class_mk = case_when(tau &amp;gt; 0 &amp;amp; p_value &amp;lt; 0.01 ~ &amp;quot;positive&amp;quot;,
                                    tau &amp;lt; 0 &amp;amp; p_value &amp;lt; 0.01 ~ &amp;quot;negative&amp;quot;,
                                    ELSE ~ &amp;quot;neutral&amp;quot;
  )) %&amp;gt;% 
  select(ticker, trend_class_mk)

# use linear regression to determine class
lm_class = trend_class_test %&amp;gt;%
  group_by(ticker) %&amp;gt;%
  mutate(day_index = row_number()) %&amp;gt;%
  do(tidy(lm(price_close ~ day_index, data = .))) %&amp;gt;% 
  filter(term == &amp;#39;day_index&amp;#39;) %&amp;gt;%
  mutate(trend_class_lm = case_when(p.value &amp;lt; 0.01 &amp;amp; estimate &amp;gt; 0 ~ &amp;quot;positive&amp;quot;,
                                    p.value &amp;lt; 0.01 &amp;amp; estimate &amp;lt; 0 ~ &amp;quot;negative&amp;quot;,
                                    ELSE ~ &amp;quot;neutral&amp;quot;
  )) %&amp;gt;% 
  select(ticker, trend_class_lm)

trend_class_actual = inner_join(mk_class,
                                lm_class,
                                on = &amp;quot;ticker&amp;quot;) %&amp;gt;% 
                     filter(trend_class_mk == trend_class_lm) %&amp;gt;% 
                     inner_join(predicted_class %&amp;gt;% 
                                select(-total_pos_class) %&amp;gt;% 
                                dplyr::rename(ticker = test_stock))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;ticker&amp;quot;
## Joining, by = &amp;quot;ticker&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s have a look at the accuracy of our predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;classification_accuracy = trend_class_actual %&amp;gt;% 
                          filter(trend_class_mk == class_pred &amp;amp; 
                                 trend_class_lm == class_pred) %&amp;gt;% 
                          nrow()/nrow(trend_class_actual) * 100

print(paste0(&amp;quot;PERCENT OF STOCKS CORRECTLY CLASSIFIED: &amp;quot;,classification_accuracy, &amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;PERCENT OF STOCKS CORRECTLY CLASSIFIED: 25%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An accuracy rate of 30% is no better than chance if we assume an equivalent distribution of positive, negative, and flat trends. However, this approach for evaluating performance doesn’t account for how much of an increase/decrease occurred It also doesn’t provide any insight into the overall percentage of stocks that trended upward or downward. Indeed, many stocks trended positively during this time period, so the overall classification accuracy wasn’t so hot. We would’ve missed out on a lot of stocks that did quite well, and here are a few reasons why:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A time-period of 60 days was too short&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The dynamics that lead to trends in our training set were different than those in our test set&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The assumptions used to classify different stocks weren’t valid&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;K was too high/low&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;This approach just doesn’t work with stock data&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Indeed, that last point is crucial. There are some things in life that are really hard to predict, as the number of factors that influence the thing we are trying to predict are too great. This is one advantage of using a non-parametric approach, in that you don’t have to quantify each of the factors with a parameter – you just let the data decide. However, the assumption that past patterns and relationships will persist in the future still holds. In our case, if the patterns that precede a positive trend in the past are different than those in the future, then you can’t use historical data (like we did here) as a basis for predictions. This assumption is hotly contested in the world of economics when it comes to predicting stocks. Some believe there are cyclical patterns that precede a downward or upward trend in price, while others believe that the stock market is simply a random walk – it cannot be predicted. Any forward-looking knowledge about where a stock might go is already baked into its current price. I don’t have the answer; if I did, I’d start a hedge fund, make lots of money, then buy my own soft-serve ice cream machine. I’m pretty sure that’s what we’d all do.&lt;/p&gt;
&lt;p&gt;Although the method outlined here didn’t perform well in predicting stocks, similar methods have shown promise detecting trends in other areas, such as social media (see &lt;a href=&#34;https://snikolov.wordpress.com/2012/11/14/early-detection-of-twitter-trends/&#34;&gt;here&lt;/a&gt;). I hope you enjoyed reading this post. If you have any suggestions for alternate approaches to trend detection, I’d love to hear!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Is that Home Price Negotiable?</title>
      <link>/post/is_that_home_price_negotiable.html</link>
      <pubDate>Sun, 14 May 2017 21:13:14 -0500</pubDate>
      
      <guid>/post/is_that_home_price_negotiable.html</guid>
      <description>&lt;p&gt;&lt;img src=&#34;prediction_interval_images/big_house.png&#34; width=&#34;800px&#34; height=&#34;800px&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;Buying a home is a big decision, and negotiating the final sales price is a central part of that decision. If your offer is too low, it might be ignored. If it is too high, you end up spending more than required. Obviously, both these scenarios should be avoided, so how do you know if there is room to negotiate? One way is to not only estimate the expected price of a potential home relative to the asking price but also to quantify the certainty surrounding that estimate. Popular real-estate websites, such as &lt;a href=&#34;www.redfin.com&#34;&gt;Redfin&lt;/a&gt; or &lt;a href=&#34;www.zillow.com&#34;&gt;Zillow&lt;/a&gt;, provide the first part of this equation but do not provide any indication as to the certainty surrounding their estimate of a home’s value. A negotiable price would have the following two characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An estimated value below the asking price&lt;/li&gt;
&lt;li&gt;A high level of confidence in the accuracy of the estimated value&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, I’ll cover how to generate insights on both fronts using the Boston Housing Dataset. However, before diving into the code, let’s briefly review some key concepts.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence-prediction-intervals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Confidence &amp;amp; Prediction Intervals&lt;/h3&gt;
&lt;p&gt;These two terms are often used interchangeably but they are quite different. In this case, we want to estimate a home’s sales price as a function of how many rooms are in the house. Confidence intervals capture uncertainty around a parameter estimate (e.g., the average increase in price for each additional room). If we were to repeatedly take samples of houses, measure their sales price and number of rooms, and calculate a different regression equation for each sample, then 95% of the confidence intervals generated from each regression should contain the true parameter (i.e., the increase in a home’s value resulting from each additional room).&lt;/p&gt;
&lt;p&gt;Prediction intervals take the uncertainty of parameter estimates into account as well. But they also consider the uncertainty around trying to predict the value of an unseen, single observation (variability of individual observations around the parameter). Therefore, prediction intervals are always wider than confidence intervals – because we are considering more sources of uncertainty. When purchasing a home, we aren’t particularly interested in the average price of all homes with, say, eight rooms. What we want to know is the price for that eight-room, English Tudor Revival home with the old Oak tree in the front yard and 1980s style Jacuzzi in the backyard that’s for sale up the street. To answer that question, you’ll need to use a prediction interval.&lt;/p&gt;
&lt;p&gt;With these differences in mind, let’s see how prediction intervals are used in practice. Imagine you are buying a home. You have actual historical sales data associated with various features of a home, such as the number rooms or commute time to major employment centers. You have the same data for homes you want to purchase, but instead of a sales price, you have an asking price. To recreate this situation, we’ll do a 70-30 split to create our training and test sets, respectively, using the Boston Housing dataset. We’ll implement cross-validation on the training set to get an idea of how well our prediction interval methodology will perform on new, unseen data. Finally, we’ll identify all the homes where an offer below the listing price has a high chance of being accepted. Let’s get started and pull all the data we’ll need into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs = c(&amp;#39;randomForest&amp;#39;, &amp;#39;dplyr&amp;#39;, &amp;#39;data.table&amp;#39;, 
         &amp;#39;knitr&amp;#39;, &amp;#39;ggplot2&amp;#39;, &amp;#39;kableExtra&amp;#39;,
         &amp;#39;car&amp;#39;, &amp;#39;forcats&amp;#39;, &amp;#39;lazyeval&amp;#39;, 
         &amp;#39;broom&amp;#39;, &amp;#39;VSURF&amp;#39;,&amp;#39;quantregForest&amp;#39;
         )

lapply(libs, require, character.only = TRUE)

housing_data = fread(&amp;quot;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data&amp;quot;,
                      data.table = FALSE)

names(housing_data) = c(&amp;#39;crim&amp;#39;, &amp;#39;zn&amp;#39;, &amp;#39;indus&amp;#39;,
                       &amp;#39;chas&amp;#39;, &amp;#39;nox&amp;#39;, &amp;#39;rm&amp;#39;, &amp;#39;age&amp;#39;,
                       &amp;#39;dis&amp;#39;, &amp;#39;rad&amp;#39;, &amp;#39;tax&amp;#39;, &amp;#39;ptratio&amp;#39;,
                       &amp;#39;b&amp;#39;, &amp;#39;lstat&amp;#39;, &amp;#39;sales_price&amp;#39;)
# covert outcome variable to 100K increments
housing_data$sales_price = housing_data$sales_price * 10e3
set.seed(123)
pct_train = 0.70

train_indices = sample.int(n = nrow(housing_data), 
                           size = floor(pct_train * nrow(housing_data)), 
                           replace = FALSE)

train_df = housing_data[train_indices,]
test_df = housing_data[-train_indices,] %&amp;gt;% 
          dplyr::rename(asking_price = sales_price)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As always, it’s a good idea to check out a few rows in the data.&lt;/p&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:410px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
crim
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
zn
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
indus
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
chas
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
nox
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
rm
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
age
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
dis
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
rad
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
tax
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
ptratio
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
b
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
lstat
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
sales_price
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
146
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.37934
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
19.58
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.871
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.130
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.4191
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
403
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
14.7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
172.91
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
27.80
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
138000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
399
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.35180
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.10
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.693
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5.453
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.4896
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
24
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
666
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
396.90
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
30.59
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
50000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
207
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.22969
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10.59
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.489
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.326
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
52.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4.3549
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
277
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
394.87
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10.97
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
244000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
445
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
12.80230
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.10
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.740
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5.854
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
96.6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.8956
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
24
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
666
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
240.52
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
23.79
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
108000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
473
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.56868
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.10
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.580
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.437
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
75.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.8965
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
24
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
666
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
393.37
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
14.36
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
232000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.23247
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8.14
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.538
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.142
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
91.7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.9769
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
307
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
21.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
396.90
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.72
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
152000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
265
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.55007
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.97
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.647
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.206
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
91.6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.9301
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
264
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
13.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
387.89
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8.10
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
365000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
446
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10.67180
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18.10
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.740
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.459
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
94.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.9879
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
24
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
666
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
23.98
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
118000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
275
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05644
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.41
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.447
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.758
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
32.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4.0776
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
254
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
17.6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
396.90
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.53
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
324000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
227
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.38214
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.20
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.504
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8.040
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
86.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.2157
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
307
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
17.4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
387.38
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.13
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
376000
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Great success! Each of the fields represents some attribute of the home For example, &lt;code&gt;rm&lt;/code&gt; is the number of rooms in the house, and &lt;code&gt;crim&lt;/code&gt; is the per capita crime rate per town (see &lt;a href=&#34;https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html&#34;&gt;here&lt;/a&gt; for more detail). We are now ready to build a model, and the first step is to understand which features are essential to our prediction accuracy. We’ll leverage the &lt;code&gt;VSURF&lt;/code&gt; package, or &lt;em&gt;Variable Selection Using Random Forest&lt;/em&gt;, to do variable selection, ensuring that only features that matter enter into the model. This is achieved in a stepwise fashion, where the Out of Bag (OOB) errors are compared between models that include/exclude one variable. If a big reduction in the average OOB error occurs when a variable is included, then we assume that the variable is important.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
vsurf_fit = VSURF(x = train_df[,1:13],
            y = train_df[,14],
            parallel = TRUE,
            ncores = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following four variables contributed significantly to the selling price of a home in Boston:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;lstat - % lower status of the population&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;rm - number of rooms&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;nox - nitric oxides concentration (not sure if this is good or bad)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;dis - weighted distance to five Boston employment centers&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s reduce our training set to only include these four features.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_df = train_df[c(names(train_df)[vsurf_fit$varselect.pred],
                       &amp;#39;sales_price&amp;#39;)]
x_var = setdiff(names(train_df), &amp;#39;sales_price&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have our new, slim data set, we’ll move on to the modeling part. In this case, we’ll use a technique called &lt;code&gt;Quantile Regression&lt;/code&gt; or QR. For those unfamiliar, QR works by differentially weighting errors during the model fitting process. It captures the relationship between our inputs and a specific quantile (i.e., percentile) of our output. For example, if the goal is to understand the relationship between X and Y at the 90th percentile of Y, 10% of the errors should be positive and 90% negative to minimize our loss function (e.g., MAE, MSE). This is especially useful for creating prediction intervals, while also providing a point estimate, in this case, the median (i.e., the 50th percentile). Additionally, it is agnostic to the statistical method being used; multiple regression is the most common form of quantile regression, but GBM, Random Forest, which is what will be used here, and even Neural Networks can be used to do QR.&lt;/p&gt;
&lt;p&gt;With that in mind, let’s move on to the actual modeling part. First, we’ll determine how well we can explain a home’s sale price via the &lt;code&gt;Mean Absolute Error&lt;/code&gt; (MAE). Second, we’ll validate our prediction intervals. An 80% prediction interval will be used, so at least 80% of observations should fall within the bounds of an interval – a metric referred to as &lt;code&gt;coverage&lt;/code&gt; or &lt;code&gt;tolerance&lt;/code&gt;. We’ll test both metrics via 5-fold cross-validation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
# add row index
train_df$row_index = 1:nrow(train_df)
# shuffle rows to randomized input order
train_df = train_df[sample(nrow(train_df)),]
# specify N folds
n_fold = 5
start_index = 1
end_index = floor(nrow(train_df)/n_fold)
increment = end_index
cv_df = data.frame(NULL)
for(i in 1:n_fold){
  temp_train_df = train_df %&amp;gt;% 
                  filter(!(row_index %in% start_index:end_index)) %&amp;gt;% 
                  select(-row_index)
  
  temp_validation_df = train_df %&amp;gt;% 
                  filter(row_index %in% start_index:end_index) %&amp;gt;% 
                  select(-row_index)
  
  cv_pred = tidy(
                  predict(quantregForest(x = temp_train_df[,1:(dim(temp_train_df)[2] - 1)],
                                         y = temp_train_df[,dim(temp_train_df)[2]],
                                         ntree = 500
                                        ),
                          temp_validation_df,
                          what = c(0.1, 0.5, 0.9)
                          )
                  ) %&amp;gt;% 
             mutate(sales_price = temp_validation_df$sales_price) %&amp;gt;% 
             rename(lwr = quantile..0.1,
                    upr = quantile..0.9,
                    predicted_price = quantile..0.5
                    ) %&amp;gt;% 
             mutate(residual = sales_price - predicted_price,
                    coverage = ifelse(sales_price &amp;gt; lwr &amp;amp; sales_price &amp;lt; upr, 1, 0)
                    )
  cv_df = bind_rows(cv_df,
                    data.frame(mean_abs_err = mean(abs(cv_pred$residual)),
                               coverage = sum(cv_pred$coverage)/nrow(cv_pred) * 100,
                               fold = i
                              )
                    )
  start_index = end_index
  end_index = end_index + increment
  if(end_index &amp;gt; nrow(train_df)){
    break
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-y: scroll; height:310px; overflow-x: scroll; width:720px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
mean_abs_err
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
coverage
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
fold
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
21192.86
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.14286
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
28345.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
84.50704
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
25563.38
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
83.09859
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
23119.72
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
90.14085
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
25654.93
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
81.69014
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Overall the model and prediction intervals performed well, at least well enough to the point where I’d feel comfortable negotiating based on the output. Let’s put our new-found knowledge to work! First, to make this example more realistic, we’ll apply a few constraints. For example, I want a house with at least six rooms, within an average distance of four miles to all major working areas (because sitting in traffic is no fun), and I don’t want to spend any more than 300K. Let’s filter the test data set to only include homes that meet these criteria.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm_threshold = 6
dis_threshold = 4
price_threshold = 300000
test_df = test_df %&amp;gt;% 
          select(c(x_var, &amp;#39;asking_price&amp;#39;)) %&amp;gt;% 
          filter(rm &amp;gt;= rm_threshold &amp;amp; 
                 dis &amp;lt;= dis_threshold &amp;amp;
                 asking_price &amp;lt;= price_threshold 
                   )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A total of 48 homes remain. Of these homes, which could we negotiate the sale price? Let’s examine the output from our final model and visualize where the opportunities exist.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qrf_fit = quantregForest(x=train_df[,1:4], 
                         y = train_df[,5], 
                         ntree = 1000)

qrf_quantile = predict(qrf_fit, 
                        test_df, 
                        what=c(0.1, 0.5, 0.9)) %&amp;gt;% 
                data.frame() %&amp;gt;% 
                rename(predicted_price = quantile..0.5,
                       lwr = quantile..0.1,
                       upr = quantile..0.9) %&amp;gt;% 
                mutate(asking_price = test_df$asking_price) %&amp;gt;% 
                mutate(coverage = ifelse(asking_price &amp;lt;= upr &amp;amp; asking_price &amp;gt;= lwr, 
                                         1, 
                                         0),
                       `PI Range` = upr - lwr,
                       residual = asking_price - predicted_price
                       ) %&amp;gt;% 
                mutate(Opportunity = ifelse(residual &amp;lt; 0, 
                                            &amp;quot;underpriced&amp;quot;, 
                                            &amp;quot;overpriced&amp;quot;)) %&amp;gt;% 
                filter(`PI Range` &amp;lt;= 90000)


qrf_plot_input = qrf_quantile %&amp;gt;% 
  mutate(asking_price = asking_price/1e3,
         predicted_price = predicted_price/1e3,
         `PI Range` = `PI Range`/1e3,
         lwr = lwr/1e3,
         upr = upr/1e3
         )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# custom plotting theme
my_plot_theme = function(){
    font_family = &amp;quot;Helvetica&amp;quot;
    font_face = &amp;quot;bold&amp;quot;
    return(theme(
    axis.text.x = element_text(size = 18, face = font_face, family = font_family),
    axis.text.y = element_text(size = 18, face = font_face, family = font_family),
    axis.title.x = element_text(size = 20, face = font_face, family = font_family),
    axis.title.y = element_text(size = 20, face = font_face, family = font_family),
    strip.text.y = element_text(size = 18, face = font_face, family = font_family),
    plot.title = element_text(size = 18, face = font_face, family = font_family),
    legend.position = &amp;quot;top&amp;quot;,
    legend.title = element_text(size = 16,
    face = font_face,
    family = font_family),
    legend.text = element_text(size = 14,
    face = font_face,
    family = font_family)
))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(qrf_plot_input, aes(x = asking_price, 
                          y = predicted_price, 
                          color = Opportunity)) + 
  theme_bw() + 
  geom_point(aes(size = `PI Range`)) + 
  geom_errorbar(aes(ymin = lwr, ymax = upr)) + 
  geom_abline(intercept = 0, slope = 1, 
              color = &amp;quot;black&amp;quot;, size = 2, 
              linetype = &amp;quot;solid&amp;quot;,
              alpha = 0.5
              ) + 
  xlab(&amp;quot;Asking Price (K)&amp;quot;) + ylab(&amp;quot;Predicted Price (K)&amp;quot;) + 
  my_plot_theme() +  
  xlim(0, price_threshold/1e3 + 30) + ylim(0, price_threshold/1e3 + 30) + 
  annotate(&amp;quot;segment&amp;quot;, 
           x = 260, 
           xend = 285, 
           y = 200, 
           yend = 228, 
           colour = &amp;quot;black&amp;quot;, 
           size = 2, 
           arrow = arrow(),
           alpha = 0.6
           )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/is_that_home_price_negotiable_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each point represents a home. All points below the diagonal line are overpriced homes (i.e., the asking price is above the predicted price), while those above the diagonal line are underpriced. Each of the points is sized according to the width of its prediction interval, such that smaller points have narrower intervals. The arrow identifies a potential home that we should feel comfortable negotiating on price because it not only is overpriced but also has a relatively narrow prediction interval. When both features are present, making an offer for less than the asking price should result in an acceptance more frequently than the points above of the line or those with wider prediction intervals. This is information we can use to decide when to make a purchase, continue negotiating, or simply walk away from the house.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;concluding-remarks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Concluding Remarks&lt;/h3&gt;
&lt;p&gt;This post outlined a framework for identifying opportunities where an offer lower than the asking price would likely be accepted when purchasing a home. However, this framework could be used in any decision-making context where uncertainty is a major factor in choosing a course of action. For example, imagine we were predicting demand for a new product instead of home prices. If our demand forecast was much larger than production numbers, and the accompanying prediction interval was small, then we could recommend an increase in production numbers to avoid lost sales. I’d love to hear about frameworks/techniques that you use to deal with these types of issues, so please feel free to chime in below!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example-talk.html</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0800</pubDate>
      
      <guid>/talk/example-talk.html</guid>
      <description>&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://sourcethemes.com/academic/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Person Re-Identification System For Mobile Devices</title>
      <link>/publication/person-re-identification.html</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/publication/person-re-identification.html</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mobile visual clothing search</title>
      <link>/publication/clothing-search.html</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>/publication/clothing-search.html</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
